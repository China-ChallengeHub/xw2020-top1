{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 14:24:40 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.92       Driver Version: 410.92       CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:0D.0 Off |                  Off |\r\n",
      "| N/A   64C    P0   113W / 250W |   7892MiB / 16130MiB |     50%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     31139      C   ..._04/anaconda3/envs/tf2_torch/bin/python  7881MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "online\n",
    "\n",
    "if random.choice([0,1,1,1,]):\n",
    "[0.859, 0.86, 0.861, 0.843, 0.855] 0.8556000000000001\n",
    "\n",
    "[0.8789124971441612, 0.8831880936061866, 0.8818342151675468, 0.8683454177281323, 0.8759553203997628] 0.877647108809158\n",
    "\n",
    "online 0.7916984126984127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        '0': 'A_1', '1': 'B_2', '2': 'A_3', '3': 'A_4', '4': 'B_3', '5': 'C_5', '6': 'C_2', '7': 'A_5', '8': 'B_1', \n",
    "        '9': 'C_1', '10': 'A_2', '11': 'C_3', '12': 'B_5', '13': 'B_4', '14': 'C_4', \n",
    "        '15': 'D_6', '16': 'E_7', '17': 'F_8', '18': 'G_9', '19': 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[str(int(y))], mapping[str(int(y_pred))]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_test_final.csv  sensor_train_final.csv  submit_example.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path  = '../data/final_data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay', 'thetaz', 'xy',\n",
      "       'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "    df['thetax']=np.arctan(df.acc_xg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetay']=np.arctan(df.acc_yg/\n",
    "                           np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetaz']=np.arctan(df.acc_zg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "    df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "    df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'acc_xg',\n",
       " 'acc_yg',\n",
       " 'acc_zg',\n",
       " 'acc',\n",
       " 'accg',\n",
       " 'thetax',\n",
       " 'thetay',\n",
       " 'thetaz',\n",
       " 'xy',\n",
       " 'xy_g',\n",
       " 'g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NUM=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 16000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train['fragment_id'])),len(set(test['fragment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 59/7292 [00:00<00:12, 583.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:12<00:00, 595.17it/s]\n",
      "100%|██████████| 7500/7500 [00:12<00:00, 590.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    \n",
    "    \n",
    "    \n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 59/7292 [00:00<00:12, 585.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:12<00:00, 596.72it/s]\n",
      "100%|██████████| 7500/7500 [00:12<00:00, 589.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n"
     ]
    }
   ],
   "source": [
    "# 一个完成了的generator\n",
    "def data_generator(data,label,shuffle,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    \"\"\"\n",
    "    \n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    length=len(data)\n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "        count+=1\n",
    "#         print(count,inds)\n",
    "        yield data[inds],label[inds]\n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x[:10],y[:10],True,3):\n",
    "    print(a.shape,b.shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBNRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "#                activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def ConvRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "               activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def squeeze_excitation_layer(x, out_dim,ratio=8):\n",
    "    '''\n",
    "    SE module performs inter-channel weighting.\n",
    "    '''\n",
    "    squeeze = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
    "    excitation = Activation('relu')(excitation)\n",
    "    excitation = Dense(units=out_dim)(excitation)\n",
    "    excitation = Activation('sigmoid')(excitation)\n",
    "    excitation = Reshape((1,1,out_dim))(excitation)\n",
    "    scale = multiply([x,excitation])\n",
    "    return scale\n",
    "\n",
    "# def SE_Residual(X):\n",
    "#     A = \n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "#     X =  Add()([X,A])\n",
    "    \n",
    "\n",
    "def lenet5(input):\n",
    "    A = ConvBNRelu(input,64,kernal_size=(3,3))\n",
    "#     B = ConvBNRelu(input,16,kernal_size=(5,1))\n",
    "#     C = ConvBNRelu(input,16,kernal_size=(7,1))\n",
    "#     ABC = layers.Concatenate()([A,B,C])\n",
    "    X = ConvBNRelu(A,128)\n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    X = AveragePooling2D()(X)\n",
    "    \n",
    "    X = ConvBNRelu(X,256)\n",
    "    X = Dropout(0.3)(X)\n",
    "#     X = squeeze_excitation_layer(X,256)\n",
    "    X = ConvBNRelu(X,512)   \n",
    "    X = Dropout(0.5)(X)\n",
    "#     X = squeeze_excitation_layer(X,512)\n",
    "#     X = GlobalMaxPooling2D()(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "#     X = BatchNormalization()(X)\n",
    "    return X\n",
    "import tensorflow as tf\n",
    "def Net(sample_num):\n",
    "    input1 = Input(shape=(sample_num, FEATURE_NUM, 1))\n",
    "    part = tf.split(input1,axis=2, num_or_size_splits = [6, 2, 6])\n",
    "#     res = tf.split(c, axis = 3, num_or_size_splits = [2, 2, 4])\n",
    "    \n",
    "    \n",
    "    X1 = Concatenate(axis=-2)([part[0],part[1]])\n",
    "    X1 = lenet5(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dense(128, activation='relu')(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "\n",
    "    X2 = Concatenate(axis=-2)([part[0],part[2]])\n",
    "    X2 = lenet5(X2)    \n",
    "    X2 = BatchNormalization()(X2)\n",
    "#     X = Dense(512, activation='relu')(X)\n",
    "#     X = BatchNormalization()(X)\n",
    "    X2 = Dense(128, activation='relu')(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    \n",
    "    X = Concatenate(axis=-1)([X1,X2])\n",
    "    \n",
    "#     X = Dense(256)(X)    \n",
    "    \n",
    "#     output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "#     X = Dense(64)(X)\n",
    "#     output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='20class')(X) #小类\n",
    "    \n",
    "    \n",
    "    return Model([input1], [output3])\n",
    "\n",
    "# model = Net(60)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 30, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_3 (TensorFlow [(None, 30, 6, 1), ( 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 30, 8, 1)     0           tf_op_layer_split_3[0][0]        \n",
      "                                                                 tf_op_layer_split_3[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 30, 12, 1)    0           tf_op_layer_split_3[0][0]        \n",
      "                                                                 tf_op_layer_split_3[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 30, 8, 64)    576         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 30, 12, 64)   576         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 30, 8, 64)    256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 30, 12, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 30, 8, 64)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 30, 12, 64)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 30, 8, 128)   73728       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 30, 12, 128)  73728       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 30, 8, 128)   512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 30, 12, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 30, 8, 128)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 30, 12, 128)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 30, 8, 128)   0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 30, 12, 128)  0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 15, 4, 128)   0           dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 15, 6, 128)   0           dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 15, 4, 256)   294912      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 15, 6, 256)   294912      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 15, 4, 256)   1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 15, 6, 256)   1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 15, 4, 256)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 15, 6, 256)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 15, 4, 256)   0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 15, 6, 256)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 15, 4, 512)   1179648     dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 15, 6, 512)   1179648     dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 15, 4, 512)   2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 15, 6, 512)   2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 15, 4, 512)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 15, 6, 512)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 15, 4, 512)   0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 15, 6, 512)   0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 512)          0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 512)          0           dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 512)          2048        global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          65664       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          65664       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 128)          512         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 128)          512         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 128)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 128)          0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           dropout_27[0][0]                 \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           16448       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "20class (Dense)                 (None, 20)           1300        dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,259,604\n",
      "Trainable params: 3,253,204\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 9s 762us/sample - loss: 207.4384 - acc: 0.0822 - val_loss: 14.6177 - val_acc: 0.0520\n",
      "Epoch 2/1000\n",
      "12000/12000 [==============================] - 7s 543us/sample - loss: 3.1672 - acc: 0.1200 - val_loss: 12.3697 - val_acc: 0.1147\n",
      "Epoch 3/1000\n",
      "12000/12000 [==============================] - 7s 549us/sample - loss: 3.0842 - acc: 0.1289 - val_loss: 147.6431 - val_acc: 0.1527\n",
      "Epoch 4/1000\n",
      "12000/12000 [==============================] - 6s 541us/sample - loss: 10209.3199 - acc: 0.1101 - val_loss: 3654896889.1733 - val_acc: 0.0393\n",
      "Epoch 5/1000\n",
      " 8288/12000 [===================>..........] - ETA: 1s - loss: 1663.7646 - acc: 0.0819WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1028b448288c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     callbacks=[plateau3, early_stopping, checkpoint])\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    790\u001b[0m   \"\"\"\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \"\"\"\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "fold_num=3\n",
    "\n",
    "proba_t = np.zeros((15000, 20))\n",
    "proba_oof = np.zeros((16000,20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "seeds=[42]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "\n",
    "\n",
    "        model = Net(30)\n",
    "        model.summary()\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "\n",
    "        plateau3 = ReduceLROnPlateau(monitor=\"acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.1,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'bl{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx],y_binary[xx],batch_size=32,\n",
    "                  epochs=1000,shuffle=True,\n",
    "                  validation_data=(train[yy],y_binary[yy]),\n",
    "                    callbacks=[plateau3, early_stopping, checkpoint])\n",
    "        history.append(train_res)\n",
    "\n",
    "        model.load_weights(f'bl{fold}.h5')\n",
    "        proba_t += model.predict(test, verbose=0, batch_size=1024)[2] / (fold_num*len(seeds))\n",
    "        proba_oof[yy] += model.predict(train[yy],verbose=0,batch_size=1024) [2]\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
