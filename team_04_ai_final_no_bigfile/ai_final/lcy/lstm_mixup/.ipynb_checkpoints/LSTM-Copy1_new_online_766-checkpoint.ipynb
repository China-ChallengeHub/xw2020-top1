{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention+序列复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:09.673656Z",
     "start_time": "2020-08-21T10:09:09.596593Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:10.336182Z",
     "start_time": "2020-08-21T10:09:10.245109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "# def acc_combo(y, y_pred):\n",
    "#     # 数值ID与行为编码的对应关系\n",
    "#     mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "#         4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "#         8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "#         12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "#         16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "#     # 将行为ID转为编码\n",
    "#     code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "#     if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "#         return 1.0\n",
    "#     elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "#         return 1.0/7\n",
    "#     elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "#         return 1.0/3\n",
    "#     else:\n",
    "#         return 0.0\n",
    "\n",
    "\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:10.534005Z",
     "start_time": "2020-08-21T10:09:10.339396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_test_final.csv  sensor_train_final.csv  submit_example.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:15:50.466408Z",
     "start_time": "2020-08-21T10:15:49.737809Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../../data/final_data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "# data_test['fragment_id'] += 100000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:15:50.793007Z",
     "start_time": "2020-08-21T10:15:50.469259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "#     df['thetax']=np.arctan(df.acc_xg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetay']=np.arctan(df.acc_yg/\n",
    "#                            np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetaz']=np.arctan(df.acc_zg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "#     df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "#     df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:12.936039Z",
     "start_time": "2020-08-21T10:51:09.661611Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "# group1 = [\"acc_x\",\"acc_y\",\"acc_z\",\"acc\",\"acc_xg\",\"acc_yg\",\"acc_zg\",\"accg\"]\n",
    "\n",
    "\n",
    "test['fragment_id'] += 15000\n",
    "data = pd.concat([train, test], sort=False)\n",
    "ss_tool = StandardScaler()\n",
    "data[group1] = ss_tool.fit_transform(data[group1])\n",
    "\n",
    "\n",
    "train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "test['fragment_id'] -= 15000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:13.570289Z",
     "start_time": "2020-08-21T10:51:13.509246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x', 'acc_y', 'acc_z', 'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'g']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:15.510838Z",
     "start_time": "2020-08-21T10:51:15.461803Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=len(group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:15.390959Z",
     "start_time": "2020-08-21T10:09:15.328381Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))\n",
    "\n",
    "# from scipy.fftpack import fft\n",
    "# from scipy.signal import resample\n",
    "# def get_fft_values(y_values, N, f_s):\n",
    "#     f_values = np.linspace(0.0, f_s/2.0, N//2)\n",
    "#     fft_values_ = fft(y_values)\n",
    "#     plt.plot(fft_values_)\n",
    "#     plt.show()\n",
    "#     print(fft_values_.shape)\n",
    "#     fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "#     print(fft_values.shape)\n",
    "#     return f_values, fft_values\n",
    "\n",
    "# tmp = train[train.fragment_id == 0][:sample_num]\n",
    "\n",
    "# get_fft_values(tmp[\"acc\"].values,60,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:45<00:00, 329.71it/s]\n",
      "100%|██████████| 16000/16000 [00:50<00:00, 319.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def df2array(df,num,x):\n",
    "    for i in tqdm(range(num)):\n",
    "        tmp = df[df.fragment_id == i][:sample_num]\n",
    "        length=len(tmp)\n",
    "        if length<50:\n",
    "            new_tmp=tmp.copy()\n",
    "            while len(tmp)<60:\n",
    "                new_tmp['time_point']=tmp['time_point'].max()+new_tmp['time_point']\n",
    "                tmp=pd.concat([tmp,new_tmp],ignore_index=True)\n",
    "        x[i,:,:,0] = resample(tmp[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "    return x\n",
    "x=df2array(train,15000,x)\n",
    "t=df2array(test,16000,t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "CPU times: user 27.4 ms, sys: 2.01 ms, total: 29.4 ms\n",
      "Wall time: 27.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 一个完成了的generator\n",
    "def data_generator(data,label,class17label,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    class17label: series\n",
    "    \"\"\"\n",
    "    class17label=np.asarray(class17label)\n",
    "    length=len(data)\n",
    "    seq_length=len(data[0])\n",
    "    half_seq_length=int(seq_length/2)\n",
    "    \n",
    "    # index2label\n",
    "    index2label=dict(zip(range(length),class17label))\n",
    "    \n",
    "    label2index={}\n",
    "#     print(class17label)\n",
    "    for i in range(length):\n",
    "#         print(class17label[i],label2index.get(class17label[i],[]))\n",
    "        label2index[class17label[i]]=label2index.get(class17label[i],[])\n",
    "        label2index[class17label[i]].append(i)\n",
    "\n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        if random.choice([0,1,1,1]):\n",
    "            # minxup\n",
    "            #one specific index -> label -> all the index belong to this\n",
    "            choice_index=[random.choice(label2index[index2label[x]]) for x in inds]   # get the random choice seq(waiting for concat)\n",
    "            # 1st 前1/2 seq_length 点原始  后1/2 seq_length 点随机\n",
    "            res_x_orig=data[inds,:half_seq_length]   #原始\n",
    "            res_x=data[choice_index,half_seq_length:]   #需要加入的\n",
    "\n",
    "    #         print(inds)\n",
    "    #         print(data.shape,res_x_orig.shape,res_x.shape,np.concatenate((res_x_orig,res_x),axis=1).shape)\n",
    "#             yield np.concatenate((res_x_orig,res_x),axis=1),\\\n",
    "#                     [label[0][inds],label[1][inds],label[2][inds]]\n",
    "            yield np.concatenate((res_x_orig,res_x),axis=1),label[inds]\n",
    "        else:\n",
    "        \n",
    "            yield data[inds],label[inds]\n",
    "            \n",
    "    \n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x,y,y,32):\n",
    "    print(a.shape,b.shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM,LayerNormalization,Dropout,GlobalAveragePooling1D,Input,Concatenate,BatchNormalization,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from cos_dense_attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:56:36.797851Z",
     "start_time": "2020-08-21T10:56:29.285067Z"
    },
    "code_folding": [
     21,
     36
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_6 (TensorFlow [(None, 60, 3), (Non 0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_6[0][2]        \n",
      "                                                                 tf_op_layer_split_6[0][3]        \n",
      "                                                                 tf_op_layer_split_6[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_6[0][0]        \n",
      "                                                                 tf_op_layer_split_6[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_6[0][1]        \n",
      "                                                                 tf_op_layer_split_6[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_108 (LSTM)                 (None, 60, 32)       5376        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_111 (LSTM)                 (None, 60, 16)       1280        tf_op_layer_split_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_114 (LSTM)                 (None, 60, 16)       1280        tf_op_layer_split_6[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_117 (LSTM)                 (None, 60, 16)       1280        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_120 (LSTM)                 (None, 60, 32)       4736        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_123 (LSTM)                 (None, 60, 32)       4736        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_108 (LayerN (None, 60, 32)       64          lstm_108[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_111 (LayerN (None, 60, 16)       32          lstm_111[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_114 (LayerN (None, 60, 16)       32          lstm_114[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_117 (LayerN (None, 60, 16)       32          lstm_117[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_120 (LayerN (None, 60, 32)       64          lstm_120[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_123 (LayerN (None, 60, 32)       64          lstm_123[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 60, 32)       0           layer_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 60, 16)       0           layer_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 60, 16)       0           layer_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 60, 16)       0           layer_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)           (None, 60, 32)       0           layer_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)           (None, 60, 32)       0           layer_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_109 (LSTM)                 (None, 60, 32)       8320        dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_112 (LSTM)                 (None, 60, 16)       2112        dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_115 (LSTM)                 (None, 60, 16)       2112        dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_118 (LSTM)                 (None, 60, 16)       2112        dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_121 (LSTM)                 (None, 60, 32)       8320        dropout_120[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_124 (LSTM)                 (None, 60, 32)       8320        dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_109 (LayerN (None, 60, 32)       64          lstm_109[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_112 (LayerN (None, 60, 16)       32          lstm_112[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_115 (LayerN (None, 60, 16)       32          lstm_115[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_118 (LayerN (None, 60, 16)       32          lstm_118[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_121 (LayerN (None, 60, 32)       64          lstm_121[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_124 (LayerN (None, 60, 32)       64          lstm_124[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 60, 32)       0           layer_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 60, 16)       0           layer_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 60, 16)       0           layer_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)           (None, 60, 16)       0           layer_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)           (None, 60, 32)       0           layer_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)           (None, 60, 32)       0           layer_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_110 (LSTM)                 (None, 60, 32)       8320        dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_113 (LSTM)                 (None, 60, 16)       2112        dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_116 (LSTM)                 (None, 60, 16)       2112        dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_119 (LSTM)                 (None, 60, 16)       2112        dropout_118[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_122 (LSTM)                 (None, 60, 32)       8320        dropout_121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_125 (LSTM)                 (None, 60, 32)       8320        dropout_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_36 (Attention)        (None, 60, 32)       0           lstm_110[0][0]                   \n",
      "                                                                 lstm_110[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_37 (Attention)        (None, 60, 16)       0           lstm_113[0][0]                   \n",
      "                                                                 lstm_113[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_38 (Attention)        (None, 60, 16)       0           lstm_116[0][0]                   \n",
      "                                                                 lstm_116[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_39 (Attention)        (None, 60, 16)       0           lstm_119[0][0]                   \n",
      "                                                                 lstm_119[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_40 (Attention)        (None, 60, 32)       0           lstm_122[0][0]                   \n",
      "                                                                 lstm_122[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_41 (Attention)        (None, 60, 32)       0           lstm_125[0][0]                   \n",
      "                                                                 lstm_125[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_110 (LayerN (None, 60, 32)       64          attention_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_113 (LayerN (None, 60, 16)       32          attention_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_116 (LayerN (None, 60, 16)       32          attention_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_119 (LayerN (None, 60, 16)       32          attention_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_122 (LayerN (None, 60, 32)       64          attention_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_125 (LayerN (None, 60, 32)       64          attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 60, 32)       0           layer_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 60, 16)       0           layer_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 60, 16)       0           layer_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)           (None, 60, 16)       0           layer_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)           (None, 60, 32)       0           layer_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)           (None, 60, 32)       0           layer_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_36 (Gl (None, 32)           0           dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_37 (Gl (None, 16)           0           dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_38 (Gl (None, 16)           0           dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_39 (Gl (None, 16)           0           dropout_119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_40 (Gl (None, 32)           0           dropout_122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_41 (Gl (None, 32)           0           dropout_125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 144)          0           global_average_pooling1d_36[0][0]\n",
      "                                                                 global_average_pooling1d_37[0][0]\n",
      "                                                                 global_average_pooling1d_38[0][0]\n",
      "                                                                 global_average_pooling1d_39[0][0]\n",
      "                                                                 global_average_pooling1d_40[0][0]\n",
      "                                                                 global_average_pooling1d_41[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 144)          576         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 256)          37120       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256)          1024        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           16448       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64)           4160        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def LSTM_A(input,INPUT_SIZE = 8,CELL_SIZE = 64):\n",
    "    TIME_STEPS = 60\n",
    "    OUTPUT_SIZE = 19\n",
    "    \n",
    "    activateion_fun = 'tanh'\n",
    "#     inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "#     x = Attention()([x,x])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True)(x)\n",
    "    x = Attention()([x,x])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalAveragePooling1D()(x)   \n",
    "    return x\n",
    "\n",
    "import tensorflow as tf\n",
    "def LSTM_Model():\n",
    "    \n",
    "    TIME_STEPS = 60\n",
    "    INPUT_SIZE = len(group1)\n",
    "    OUTPUT_SIZE = 19\n",
    "    activateion_fun = 'tanh'\n",
    "    inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    part = tf.split(inputs,axis=2, num_or_size_splits = [3,3,1,1,1])\n",
    "    A = LSTM_A(inputs,CELL_SIZE = 32)\n",
    "    A1 = LSTM_A(part[0],3,CELL_SIZE = 16)\n",
    "    A2 = LSTM_A(part[1],3,CELL_SIZE = 16)\n",
    "    A3 = LSTM_A(Concatenate()([part[2],part[3],part[4]]),3,CELL_SIZE = 16)\n",
    "    A4 = LSTM_A(Concatenate()([part[0],part[2]]),4,CELL_SIZE = 32)\n",
    "    A5 = LSTM_A(Concatenate()([part[1],part[3]]),4,CELL_SIZE = 32)\n",
    "    \n",
    "#     A4 = LSTM_A(part[3],6,CELL_SIZE = 16)    \n",
    "#     B = LSTM_B(inputs,INPUT_SIZE=9)\n",
    "#     C = LSTM_C(inputs,CELL_SIZE=46)\n",
    "    x = Concatenate()([A,A1,A2,A3,A4,A5])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    X = BatchNormalization()(x)\n",
    "    output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "    print(X.shape)\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "    X = Dense(64)(X)\n",
    "    output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(64)(X)\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='19class')(X) #小类\n",
    "    print(output3.shape)\n",
    "    return Model([inputs], output3)\n",
    "\n",
    "LSTM_Model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T10:56:35.269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 60, 3), (Non 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 60, 3)        0           tf_op_layer_split_1[0][2]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "                                                                 tf_op_layer_split_1[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_1[0][0]        \n",
      "                                                                 tf_op_layer_split_1[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_1[0][1]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 60, 32)       5376        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_1[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 60, 16)       1280        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, 60, 32)       4736        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 60, 32)       4736        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 60, 32)       64          lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 60, 16)       32          lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 60, 16)       32          lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 60, 16)       32          lstm_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 60, 32)       64          lstm_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 60, 32)       64          lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 60, 32)       0           layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 16)       0           layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 60, 16)       0           layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 60, 16)       0           layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 60, 32)       0           layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 60, 32)       0           layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 60, 32)       8320        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 60, 16)       2112        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  (None, 60, 16)       2112        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 60, 16)       2112        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 60, 32)       8320        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, 60, 32)       8320        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 60, 32)       64          lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 60, 16)       32          lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 60, 16)       32          lstm_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 60, 16)       32          lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 60, 32)       64          lstm_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 60, 32)       64          lstm_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 60, 32)       0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 60, 16)       0           layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 60, 16)       0           layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 60, 16)       0           layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 60, 32)       0           layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 60, 32)       0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 60, 32)       8320        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 16)       2112        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                  (None, 60, 16)       2112        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 60, 16)       2112        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 60, 32)       8320        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 60, 32)       8320        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 60, 32)       0           lstm_20[0][0]                    \n",
      "                                                                 lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 60, 16)       0           lstm_23[0][0]                    \n",
      "                                                                 lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 60, 16)       0           lstm_26[0][0]                    \n",
      "                                                                 lstm_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 60, 16)       0           lstm_29[0][0]                    \n",
      "                                                                 lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 60, 32)       0           lstm_32[0][0]                    \n",
      "                                                                 lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 60, 32)       0           lstm_35[0][0]                    \n",
      "                                                                 lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 60, 32)       64          attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 60, 16)       32          attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 60, 16)       32          attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 60, 16)       32          attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 60, 32)       64          attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 60, 32)       64          attention_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 60, 32)       0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 16)       0           layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 60, 16)       0           layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 60, 16)       0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 60, 32)       0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 60, 32)       0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 16)           0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 16)           0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 16)           0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 32)           0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 32)           0           dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 144)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 144)          576         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          37120       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16448       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n",
      "12000/12000 [==============================] - 40s 3ms/sample - loss: 1.8406 - acc: 0.4694 - val_loss: 1.6610 - val_acc: 0.5320\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 1.3763 - acc: 0.6069 - val_loss: 1.4318 - val_acc: 0.6290\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 1.2422 - acc: 0.6570 - val_loss: 1.2288 - val_acc: 0.6780\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 1.1549 - acc: 0.6907 - val_loss: 1.1943 - val_acc: 0.6827\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 1.0868 - acc: 0.7172 - val_loss: 1.1275 - val_acc: 0.6997\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 1.0385 - acc: 0.7359 - val_loss: 1.0840 - val_acc: 0.7227\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 1.0060 - acc: 0.7467 - val_loss: 1.0671 - val_acc: 0.7287\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.9699 - acc: 0.7603 - val_loss: 1.0754 - val_acc: 0.7253\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9476 - acc: 0.7669 - val_loss: 1.0648 - val_acc: 0.7343\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.9197 - acc: 0.7784 - val_loss: 1.0340 - val_acc: 0.7480\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.8949 - acc: 0.7843 - val_loss: 1.0321 - val_acc: 0.7400\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.8668 - acc: 0.7992 - val_loss: 0.9969 - val_acc: 0.7660\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.8517 - acc: 0.8068 - val_loss: 1.0206 - val_acc: 0.7540\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.8315 - acc: 0.8150 - val_loss: 1.0162 - val_acc: 0.7517\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.8148 - acc: 0.8198 - val_loss: 1.0092 - val_acc: 0.7610\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.7935 - acc: 0.8346 - val_loss: 0.9832 - val_acc: 0.7693\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.7777 - acc: 0.8405 - val_loss: 1.0159 - val_acc: 0.7600\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.7709 - acc: 0.8407 - val_loss: 1.0050 - val_acc: 0.7617\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.7590 - acc: 0.8459 - val_loss: 1.0190 - val_acc: 0.7660\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.7342 - acc: 0.8582 - val_loss: 1.0083 - val_acc: 0.7637\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.7303 - acc: 0.8586 - val_loss: 0.9874 - val_acc: 0.7763\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.7246 - acc: 0.8597 - val_loss: 0.9782 - val_acc: 0.7790\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.7061 - acc: 0.8718 - val_loss: 0.9935 - val_acc: 0.7743\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.6943 - acc: 0.8792 - val_loss: 1.0095 - val_acc: 0.7660\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6812 - acc: 0.8819 - val_loss: 1.0147 - val_acc: 0.7783\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6719 - acc: 0.8841 - val_loss: 0.9747 - val_acc: 0.7857\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6601 - acc: 0.8935 - val_loss: 0.9864 - val_acc: 0.7790\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.6607 - acc: 0.8914 - val_loss: 0.9924 - val_acc: 0.7810\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.6496 - acc: 0.8954 - val_loss: 0.9725 - val_acc: 0.7837\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.6399 - acc: 0.9003 - val_loss: 0.9779 - val_acc: 0.7850\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.6257 - acc: 0.9056 - val_loss: 0.9925 - val_acc: 0.7853\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.6220 - acc: 0.9067 - val_loss: 0.9909 - val_acc: 0.7870\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.6097 - acc: 0.9151 - val_loss: 0.9731 - val_acc: 0.7950\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.6078 - acc: 0.9153 - val_loss: 0.9762 - val_acc: 0.7917\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.5973 - acc: 0.9177 - val_loss: 0.9887 - val_acc: 0.7863\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.6035 - acc: 0.9141 - val_loss: 0.9788 - val_acc: 0.7913\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5921 - acc: 0.9230 - val_loss: 1.0056 - val_acc: 0.7853\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.5932 - acc: 0.9227 - val_loss: 0.9952 - val_acc: 0.7767\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5788 - acc: 0.9291 - val_loss: 0.9856 - val_acc: 0.7953\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5691 - acc: 0.9311 - val_loss: 0.9900 - val_acc: 0.7897\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5703 - acc: 0.9306 - val_loss: 1.0113 - val_acc: 0.7910\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5725 - acc: 0.9313 - val_loss: 1.0173 - val_acc: 0.7857\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5613 - acc: 0.9362 - val_loss: 0.9943 - val_acc: 0.7907\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.5544 - acc: 0.9384 - val_loss: 0.9949 - val_acc: 0.7920\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.5471 - acc: 0.9412 - val_loss: 0.9895 - val_acc: 0.7917\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.5380 - acc: 0.9442 - val_loss: 0.9998 - val_acc: 0.7887\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5383 - acc: 0.9454 - val_loss: 0.9878 - val_acc: 0.7947\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5319 - acc: 0.9492 - val_loss: 0.9866 - val_acc: 0.7900\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.5308 - acc: 0.9470 - val_loss: 0.9994 - val_acc: 0.7967\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.5280 - acc: 0.9483 - val_loss: 1.0105 - val_acc: 0.7913\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5257 - acc: 0.9516 - val_loss: 0.9973 - val_acc: 0.7947\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5214 - acc: 0.9525 - val_loss: 1.0403 - val_acc: 0.7837\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5223 - acc: 0.9520 - val_loss: 0.9958 - val_acc: 0.7897\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.5157 - acc: 0.9553 - val_loss: 1.0079 - val_acc: 0.7857\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5150 - acc: 0.9543 - val_loss: 1.0103 - val_acc: 0.7900\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5151 - acc: 0.9549 - val_loss: 1.0032 - val_acc: 0.7963\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5066 - acc: 0.9596 - val_loss: 1.0055 - val_acc: 0.7893\n",
      "Epoch 58/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5048 - acc: 0.9604 - val_loss: 1.0081 - val_acc: 0.7950\n",
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.4963 - acc: 0.9625 - val_loss: 0.9934 - val_acc: 0.7980\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4868 - acc: 0.9675 - val_loss: 1.0091 - val_acc: 0.7877\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4917 - acc: 0.9657 - val_loss: 1.0200 - val_acc: 0.7940\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4946 - acc: 0.9628 - val_loss: 0.9981 - val_acc: 0.7997\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4867 - acc: 0.9666 - val_loss: 1.0052 - val_acc: 0.7933\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.4841 - acc: 0.9691 - val_loss: 0.9928 - val_acc: 0.7960\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4796 - acc: 0.9691 - val_loss: 1.0156 - val_acc: 0.7927\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4823 - acc: 0.9671 - val_loss: 1.0187 - val_acc: 0.7933\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4814 - acc: 0.9691 - val_loss: 1.0212 - val_acc: 0.7993\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4821 - acc: 0.9678 - val_loss: 1.0014 - val_acc: 0.8020\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4766 - acc: 0.9705 - val_loss: 1.0212 - val_acc: 0.7830\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.4764 - acc: 0.9697 - val_loss: 1.0077 - val_acc: 0.7873\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.4773 - acc: 0.9717 - val_loss: 0.9945 - val_acc: 0.8023\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.4702 - acc: 0.9728 - val_loss: 0.9964 - val_acc: 0.7967\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4715 - acc: 0.9723 - val_loss: 0.9974 - val_acc: 0.8010\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.4686 - acc: 0.9729 - val_loss: 1.0268 - val_acc: 0.7900\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.4696 - acc: 0.9743 - val_loss: 1.0108 - val_acc: 0.7917\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4734 - acc: 0.9712 - val_loss: 1.0154 - val_acc: 0.7960\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.4795 - acc: 0.9696 - val_loss: 0.9978 - val_acc: 0.7967\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4608 - acc: 0.9774 - val_loss: 0.9982 - val_acc: 0.7977\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4526 - acc: 0.9793 - val_loss: 1.0064 - val_acc: 0.7990\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4583 - acc: 0.9771 - val_loss: 1.0108 - val_acc: 0.7960\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.4566 - acc: 0.9786 - val_loss: 1.0010 - val_acc: 0.7930\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.4531 - acc: 0.9789 - val_loss: 1.0042 - val_acc: 0.8040\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4542 - acc: 0.9797 - val_loss: 1.0227 - val_acc: 0.7957\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4527 - acc: 0.9788 - val_loss: 1.0187 - val_acc: 0.7897\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4552 - acc: 0.9781 - val_loss: 1.0040 - val_acc: 0.7973\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4487 - acc: 0.9813 - val_loss: 1.0015 - val_acc: 0.7917\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.4448 - acc: 0.9831 - val_loss: 0.9761 - val_acc: 0.8057\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.4485 - acc: 0.9801 - val_loss: 0.9969 - val_acc: 0.8003\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4466 - acc: 0.9804 - val_loss: 0.9932 - val_acc: 0.7953\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4447 - acc: 0.9823 - val_loss: 1.0174 - val_acc: 0.7937\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4402 - acc: 0.9830 - val_loss: 0.9884 - val_acc: 0.8013\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.4351 - acc: 0.9851 - val_loss: 0.9920 - val_acc: 0.7953\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4423 - acc: 0.9817 - val_loss: 0.9991 - val_acc: 0.7993\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.4383 - acc: 0.9859 - val_loss: 1.0023 - val_acc: 0.7990\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.4377 - acc: 0.9845 - val_loss: 0.9987 - val_acc: 0.7960\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4361 - acc: 0.9847 - val_loss: 1.0007 - val_acc: 0.8017\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.4424 - acc: 0.9822 - val_loss: 1.0052 - val_acc: 0.7977\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4420 - acc: 0.9832 - val_loss: 1.0027 - val_acc: 0.8027\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.4395 - acc: 0.9837 - val_loss: 1.0161 - val_acc: 0.8000\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4330 - acc: 0.9877 - val_loss: 1.0025 - val_acc: 0.7963\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4346 - acc: 0.9851 - val_loss: 0.9955 - val_acc: 0.7923\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.4349 - acc: 0.9847 - val_loss: 0.9991 - val_acc: 0.8030\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.4351 - acc: 0.9849 - val_loss: 1.0006 - val_acc: 0.8013\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4296 - acc: 0.9868 - val_loss: 0.9940 - val_acc: 0.8023\n",
      "Epoch 105/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.9862\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.4314 - acc: 0.9861 - val_loss: 0.9890 - val_acc: 0.8003\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.4137 - acc: 0.9927 - val_loss: 0.9778 - val_acc: 0.8057\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4069 - acc: 0.9940 - val_loss: 0.9758 - val_acc: 0.8000\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.4044 - acc: 0.9940 - val_loss: 0.9752 - val_acc: 0.8057\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.4041 - acc: 0.9952 - val_loss: 0.9766 - val_acc: 0.8067\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.4033 - acc: 0.9952 - val_loss: 0.9794 - val_acc: 0.8010\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4042 - acc: 0.9950 - val_loss: 0.9850 - val_acc: 0.7980\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4044 - acc: 0.9939 - val_loss: 0.9798 - val_acc: 0.8047\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.4017 - acc: 0.9959 - val_loss: 0.9778 - val_acc: 0.8040\n",
      "Epoch 114/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4019 - acc: 0.9952 - val_loss: 0.9773 - val_acc: 0.8040\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4015 - acc: 0.9950 - val_loss: 0.9833 - val_acc: 0.8013\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.4018 - acc: 0.9960 - val_loss: 0.9709 - val_acc: 0.8067\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.4018 - acc: 0.9949 - val_loss: 0.9811 - val_acc: 0.8020\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4027 - acc: 0.9950 - val_loss: 0.9854 - val_acc: 0.8043\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.4021 - acc: 0.9950 - val_loss: 0.9799 - val_acc: 0.8053\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4013 - acc: 0.9943 - val_loss: 0.9830 - val_acc: 0.8067\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3988 - acc: 0.9963 - val_loss: 0.9758 - val_acc: 0.8067\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3992 - acc: 0.9953 - val_loss: 0.9804 - val_acc: 0.8090\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.3983 - acc: 0.9967 - val_loss: 0.9768 - val_acc: 0.8023\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4015 - acc: 0.9957 - val_loss: 0.9778 - val_acc: 0.8050\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3991 - acc: 0.9957 - val_loss: 0.9761 - val_acc: 0.8023\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.3993 - acc: 0.9960 - val_loss: 0.9777 - val_acc: 0.8087\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.3999 - acc: 0.9958 - val_loss: 0.9768 - val_acc: 0.8053\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4010 - acc: 0.9938 - val_loss: 0.9797 - val_acc: 0.8030\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4001 - acc: 0.9953 - val_loss: 0.9780 - val_acc: 0.8003\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.4002 - acc: 0.9956 - val_loss: 0.9821 - val_acc: 0.8003\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3979 - acc: 0.9962 - val_loss: 0.9760 - val_acc: 0.8050\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3965 - acc: 0.9961 - val_loss: 0.9781 - val_acc: 0.8070\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3965 - acc: 0.9962 - val_loss: 0.9866 - val_acc: 0.8040\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3981 - acc: 0.9955 - val_loss: 0.9941 - val_acc: 0.8020\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3980 - acc: 0.9957 - val_loss: 0.9969 - val_acc: 0.8023\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3966 - acc: 0.9960 - val_loss: 0.9904 - val_acc: 0.7997\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.3932 - acc: 0.9980 - val_loss: 0.9838 - val_acc: 0.8047\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3957 - acc: 0.9966 - val_loss: 0.9902 - val_acc: 0.8013\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3962 - acc: 0.9968 - val_loss: 0.9825 - val_acc: 0.8050\n",
      "Epoch 140/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.9964\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.3961 - acc: 0.9965 - val_loss: 0.9789 - val_acc: 0.8063\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3929 - acc: 0.9970 - val_loss: 0.9674 - val_acc: 0.8057\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3890 - acc: 0.9981 - val_loss: 0.9721 - val_acc: 0.8027\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3874 - acc: 0.9983 - val_loss: 0.9684 - val_acc: 0.8057\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.3885 - acc: 0.9986 - val_loss: 0.9716 - val_acc: 0.8027\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3870 - acc: 0.9983 - val_loss: 0.9710 - val_acc: 0.8073\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.3883 - acc: 0.9977 - val_loss: 0.9695 - val_acc: 0.8097\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.3869 - acc: 0.9983 - val_loss: 0.9709 - val_acc: 0.8063\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3871 - acc: 0.9982 - val_loss: 0.9690 - val_acc: 0.8073\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3864 - acc: 0.9986 - val_loss: 0.9710 - val_acc: 0.8067\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3866 - acc: 0.9988 - val_loss: 0.9683 - val_acc: 0.8053\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3889 - acc: 0.9973 - val_loss: 0.9753 - val_acc: 0.8030\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3874 - acc: 0.9977 - val_loss: 0.9668 - val_acc: 0.8087\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3868 - acc: 0.9987 - val_loss: 0.9767 - val_acc: 0.8067\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.3860 - acc: 0.9977 - val_loss: 0.9702 - val_acc: 0.8010\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3872 - acc: 0.9984 - val_loss: 0.9743 - val_acc: 0.8047\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3865 - acc: 0.9987 - val_loss: 0.9749 - val_acc: 0.8020\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.3857 - acc: 0.9982 - val_loss: 0.9713 - val_acc: 0.8080\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.3860 - acc: 0.9983 - val_loss: 0.9712 - val_acc: 0.8103\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3869 - acc: 0.9983 - val_loss: 0.9671 - val_acc: 0.8053\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3874 - acc: 0.9973 - val_loss: 0.9646 - val_acc: 0.8070\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.3871 - acc: 0.9982 - val_loss: 0.9744 - val_acc: 0.8053\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3857 - acc: 0.9977 - val_loss: 0.9768 - val_acc: 0.8060\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.3870 - acc: 0.9983 - val_loss: 0.9720 - val_acc: 0.8027\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3867 - acc: 0.9981 - val_loss: 0.9737 - val_acc: 0.8050\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3853 - acc: 0.9983 - val_loss: 0.9748 - val_acc: 0.8050\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3866 - acc: 0.9980 - val_loss: 0.9760 - val_acc: 0.8070\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3857 - acc: 0.9985 - val_loss: 0.9773 - val_acc: 0.8060\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.3876 - acc: 0.9981 - val_loss: 0.9702 - val_acc: 0.8080\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3842 - acc: 0.9988 - val_loss: 0.9733 - val_acc: 0.8030\n",
      "Epoch 170/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3854 - acc: 0.9984 - val_loss: 0.9754 - val_acc: 0.8043\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3854 - acc: 0.9976 - val_loss: 0.9708 - val_acc: 0.8067\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.3848 - acc: 0.9987 - val_loss: 0.9714 - val_acc: 0.8047\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3848 - acc: 0.9987 - val_loss: 0.9754 - val_acc: 0.8043\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3841 - acc: 0.9986 - val_loss: 0.9686 - val_acc: 0.8033\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.3836 - acc: 0.9987 - val_loss: 0.9717 - val_acc: 0.8023\n",
      "Epoch 176/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.9983\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3850 - acc: 0.9983 - val_loss: 0.9713 - val_acc: 0.8080\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3830 - acc: 0.9984 - val_loss: 0.9698 - val_acc: 0.8073\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.3819 - acc: 0.9993 - val_loss: 0.9703 - val_acc: 0.8073\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.3817 - acc: 0.9989 - val_loss: 0.9685 - val_acc: 0.8070\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.3805 - acc: 0.9993 - val_loss: 0.9704 - val_acc: 0.8033\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.3815 - acc: 0.9987 - val_loss: 0.9708 - val_acc: 0.8020\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3805 - acc: 0.9991 - val_loss: 0.9733 - val_acc: 0.8043\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.3810 - acc: 0.9987 - val_loss: 0.9710 - val_acc: 0.8093\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.3810 - acc: 0.9990 - val_loss: 0.9710 - val_acc: 0.8043\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3796 - acc: 0.9993 - val_loss: 0.9698 - val_acc: 0.8060\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.3803 - acc: 0.9991 - val_loss: 0.9693 - val_acc: 0.8050\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.3794 - acc: 0.9992 - val_loss: 0.9688 - val_acc: 0.8073\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.3799 - acc: 0.9992 - val_loss: 0.9681 - val_acc: 0.8067\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.3802 - acc: 0.9991 - val_loss: 0.9673 - val_acc: 0.8097\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.3796 - acc: 0.9996 - val_loss: 0.9679 - val_acc: 0.8093\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.3802 - acc: 0.9990 - val_loss: 0.9685 - val_acc: 0.8040\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.3799 - acc: 0.9992 - val_loss: 0.9720 - val_acc: 0.8070\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.3804 - acc: 0.9990 - val_loss: 0.9693 - val_acc: 0.8077\n",
      "Epoch 194/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.9989\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.3793 - acc: 0.9987 - val_loss: 0.9640 - val_acc: 0.8047\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3787 - acc: 0.9992 - val_loss: 0.9657 - val_acc: 0.8037\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.3784 - acc: 0.9994 - val_loss: 0.9647 - val_acc: 0.8070\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.3792 - acc: 0.9987 - val_loss: 0.9658 - val_acc: 0.8070\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.3783 - acc: 0.9993 - val_loss: 0.9675 - val_acc: 0.8057\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.3790 - acc: 0.9992 - val_loss: 0.9661 - val_acc: 0.8070\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.3790 - acc: 0.9992 - val_loss: 0.9655 - val_acc: 0.8053\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3788 - acc: 0.9994 - val_loss: 0.9650 - val_acc: 0.8067\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.3783 - acc: 0.9993 - val_loss: 0.9637 - val_acc: 0.8090\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.3788 - acc: 0.9991 - val_loss: 0.9664 - val_acc: 0.8060\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.3787 - acc: 0.9989 - val_loss: 0.9670 - val_acc: 0.8070\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3775 - acc: 0.9993 - val_loss: 0.9663 - val_acc: 0.8063\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.3785 - acc: 0.9988 - val_loss: 0.9639 - val_acc: 0.8070\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.3779 - acc: 0.9992 - val_loss: 0.9648 - val_acc: 0.8053\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.3790 - acc: 0.9992 - val_loss: 0.9634 - val_acc: 0.8047\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.3776 - acc: 0.9992 - val_loss: 0.9689 - val_acc: 0.8067\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.3773 - acc: 0.9993 - val_loss: 0.9659 - val_acc: 0.8060\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.3773 - acc: 0.9994 - val_loss: 0.9645 - val_acc: 0.8060\n",
      "Epoch 212/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.9991\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.3779 - acc: 0.9991 - val_loss: 0.9620 - val_acc: 0.8060\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.3770 - acc: 0.9998 - val_loss: 0.9637 - val_acc: 0.8060\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 6s 520us/sample - loss: 0.3770 - acc: 0.9996 - val_loss: 0.9639 - val_acc: 0.8050\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.3775 - acc: 0.9991 - val_loss: 0.9640 - val_acc: 0.8060\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.3778 - acc: 0.9993 - val_loss: 0.9649 - val_acc: 0.8093\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 6s 491us/sample - loss: 0.3770 - acc: 0.9991 - val_loss: 0.9632 - val_acc: 0.8067\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.3772 - acc: 0.9992 - val_loss: 0.9630 - val_acc: 0.8073\n",
      "Epoch 00218: early stopping\n",
      "0.81\n",
      "0.82814\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_2 (TensorFlow [(None, 60, 3), (Non 0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 60, 3)        0           tf_op_layer_split_2[0][2]        \n",
      "                                                                 tf_op_layer_split_2[0][3]        \n",
      "                                                                 tf_op_layer_split_2[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_2[0][0]        \n",
      "                                                                 tf_op_layer_split_2[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_2[0][1]        \n",
      "                                                                 tf_op_layer_split_2[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, 60, 32)       5376        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  (None, 60, 16)       1280        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                  (None, 60, 32)       4736        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                  (None, 60, 32)       4736        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 60, 32)       64          lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 60, 16)       32          lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, 60, 16)       32          lstm_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, 60, 16)       32          lstm_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, 60, 32)       64          lstm_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_51 (LayerNo (None, 60, 32)       64          lstm_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 60, 32)       0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 60, 16)       0           layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 60, 16)       0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 60, 16)       0           layer_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 60, 32)       0           layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 60, 32)       0           layer_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (None, 60, 32)       8320        dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, 60, 16)       2112        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 60, 16)       2112        dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 60, 16)       2112        dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_49 (LSTM)                  (None, 60, 32)       8320        dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_52 (LSTM)                  (None, 60, 32)       8320        dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 60, 32)       64          lstm_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, 60, 16)       32          lstm_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, 60, 16)       32          lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, 60, 16)       32          lstm_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, 60, 32)       64          lstm_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_52 (LayerNo (None, 60, 32)       64          lstm_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 60, 32)       0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 60, 16)       0           layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 60, 16)       0           layer_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 60, 16)       0           layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 60, 32)       0           layer_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 60, 32)       0           layer_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (None, 60, 32)       8320        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, 60, 16)       2112        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 60, 16)       2112        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 60, 16)       2112        dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_50 (LSTM)                  (None, 60, 32)       8320        dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_53 (LSTM)                  (None, 60, 32)       8320        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 60, 32)       0           lstm_38[0][0]                    \n",
      "                                                                 lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 60, 16)       0           lstm_41[0][0]                    \n",
      "                                                                 lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 60, 16)       0           lstm_44[0][0]                    \n",
      "                                                                 lstm_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 60, 16)       0           lstm_47[0][0]                    \n",
      "                                                                 lstm_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 60, 32)       0           lstm_50[0][0]                    \n",
      "                                                                 lstm_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 60, 32)       0           lstm_53[0][0]                    \n",
      "                                                                 lstm_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 60, 32)       64          attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, 60, 16)       32          attention_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, 60, 16)       32          attention_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, 60, 16)       32          attention_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, 60, 32)       64          attention_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_53 (LayerNo (None, 60, 32)       64          attention_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 60, 32)       0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 60, 16)       0           layer_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 60, 16)       0           layer_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 60, 16)       0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 60, 32)       0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 60, 32)       0           layer_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 16)           0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 16)           0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 16)           0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 32)           0           dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 144)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 144)          576         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          37120       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           16448       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n",
      "12000/12000 [==============================] - 54s 5ms/sample - loss: 1.8036 - acc: 0.4816 - val_loss: 1.6382 - val_acc: 0.5303\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 6s 526us/sample - loss: 1.3482 - acc: 0.6150 - val_loss: 1.3654 - val_acc: 0.6300\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 1.2307 - acc: 0.6663 - val_loss: 1.2419 - val_acc: 0.6683\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 1.1499 - acc: 0.6917 - val_loss: 1.1606 - val_acc: 0.6943\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 1.0801 - acc: 0.7178 - val_loss: 1.1097 - val_acc: 0.7163\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 1.0302 - acc: 0.7384 - val_loss: 1.0943 - val_acc: 0.7247\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.9913 - acc: 0.7546 - val_loss: 1.0851 - val_acc: 0.7280\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.9629 - acc: 0.7663 - val_loss: 1.0516 - val_acc: 0.7410\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.9223 - acc: 0.7804 - val_loss: 1.0423 - val_acc: 0.7393\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 6s 505us/sample - loss: 0.9003 - acc: 0.7926 - val_loss: 1.0414 - val_acc: 0.7437\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.8771 - acc: 0.8007 - val_loss: 1.0362 - val_acc: 0.7553\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 6s 497us/sample - loss: 0.8579 - acc: 0.8083 - val_loss: 1.0047 - val_acc: 0.7583\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.8394 - acc: 0.8135 - val_loss: 1.0375 - val_acc: 0.7450\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.8121 - acc: 0.8267 - val_loss: 1.0521 - val_acc: 0.7470\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 6s 526us/sample - loss: 0.7946 - acc: 0.8322 - val_loss: 0.9934 - val_acc: 0.7690\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.7800 - acc: 0.8397 - val_loss: 1.0301 - val_acc: 0.7467\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.7692 - acc: 0.8428 - val_loss: 1.0125 - val_acc: 0.7683\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.7463 - acc: 0.8546 - val_loss: 1.0076 - val_acc: 0.7700\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.7381 - acc: 0.8580 - val_loss: 1.0314 - val_acc: 0.7647\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.7261 - acc: 0.8643 - val_loss: 1.0255 - val_acc: 0.7597\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.7231 - acc: 0.8623 - val_loss: 1.0133 - val_acc: 0.7677\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.6996 - acc: 0.8739 - val_loss: 1.0220 - val_acc: 0.7670\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.6870 - acc: 0.8798 - val_loss: 1.0173 - val_acc: 0.7710\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 6s 509us/sample - loss: 0.6810 - acc: 0.8816 - val_loss: 1.0037 - val_acc: 0.7717\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.6711 - acc: 0.8849 - val_loss: 1.0117 - val_acc: 0.7723\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.6603 - acc: 0.8906 - val_loss: 1.0048 - val_acc: 0.7780\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.6504 - acc: 0.8938 - val_loss: 0.9968 - val_acc: 0.7737\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.6367 - acc: 0.9015 - val_loss: 0.9962 - val_acc: 0.7783\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6353 - acc: 0.9028 - val_loss: 1.0264 - val_acc: 0.7737\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 6s 532us/sample - loss: 0.6295 - acc: 0.9066 - val_loss: 1.0021 - val_acc: 0.7873\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.6143 - acc: 0.9115 - val_loss: 0.9937 - val_acc: 0.7847\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.6053 - acc: 0.9184 - val_loss: 1.0159 - val_acc: 0.7790\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.5940 - acc: 0.9210 - val_loss: 1.0033 - val_acc: 0.7800\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5947 - acc: 0.9203 - val_loss: 0.9928 - val_acc: 0.7833\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 6s 500us/sample - loss: 0.5977 - acc: 0.9184 - val_loss: 1.0035 - val_acc: 0.7890\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5882 - acc: 0.9225 - val_loss: 1.0126 - val_acc: 0.7827\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.5708 - acc: 0.9311 - val_loss: 1.0022 - val_acc: 0.7870\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5732 - acc: 0.9288 - val_loss: 1.0123 - val_acc: 0.7863\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.5769 - acc: 0.9284 - val_loss: 0.9833 - val_acc: 0.7860\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 6s 510us/sample - loss: 0.5636 - acc: 0.9337 - val_loss: 1.0065 - val_acc: 0.7923\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.5599 - acc: 0.9358 - val_loss: 1.0074 - val_acc: 0.7873\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5543 - acc: 0.9367 - val_loss: 0.9959 - val_acc: 0.7920\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.5444 - acc: 0.9427 - val_loss: 0.9940 - val_acc: 0.7930\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5398 - acc: 0.9441 - val_loss: 1.0117 - val_acc: 0.7887\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.5347 - acc: 0.9474 - val_loss: 1.0037 - val_acc: 0.7890\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 6s 495us/sample - loss: 0.5348 - acc: 0.9477 - val_loss: 1.0143 - val_acc: 0.7943\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.5290 - acc: 0.9488 - val_loss: 1.0140 - val_acc: 0.7913\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5294 - acc: 0.9482 - val_loss: 1.0105 - val_acc: 0.7843\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5253 - acc: 0.9518 - val_loss: 1.0206 - val_acc: 0.7890\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5152 - acc: 0.9546 - val_loss: 1.0178 - val_acc: 0.7910\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.5164 - acc: 0.9541 - val_loss: 1.0230 - val_acc: 0.7940\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5212 - acc: 0.9508 - val_loss: 1.0074 - val_acc: 0.7963\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.5076 - acc: 0.9565 - val_loss: 1.0212 - val_acc: 0.7917\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.5073 - acc: 0.9567 - val_loss: 1.0009 - val_acc: 0.7977\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5007 - acc: 0.9617 - val_loss: 0.9994 - val_acc: 0.8010\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.5031 - acc: 0.9582 - val_loss: 1.0035 - val_acc: 0.7927\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4966 - acc: 0.9622 - val_loss: 1.0236 - val_acc: 0.7873\n",
      "Epoch 58/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4927 - acc: 0.9630 - val_loss: 1.0052 - val_acc: 0.7943\n",
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4898 - acc: 0.9657 - val_loss: 0.9868 - val_acc: 0.8023\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.4933 - acc: 0.9645 - val_loss: 1.0138 - val_acc: 0.7913\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.4875 - acc: 0.9672 - val_loss: 1.0040 - val_acc: 0.7917\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4897 - acc: 0.9646 - val_loss: 1.0122 - val_acc: 0.8013\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4822 - acc: 0.9686 - val_loss: 0.9994 - val_acc: 0.7987\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.4800 - acc: 0.9692 - val_loss: 1.0053 - val_acc: 0.7897\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.4740 - acc: 0.9721 - val_loss: 1.0184 - val_acc: 0.7957\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4778 - acc: 0.9702 - val_loss: 1.0176 - val_acc: 0.7877\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.4776 - acc: 0.9706 - val_loss: 1.0051 - val_acc: 0.7957\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.4715 - acc: 0.9720 - val_loss: 1.0181 - val_acc: 0.7897\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.4753 - acc: 0.9699 - val_loss: 0.9981 - val_acc: 0.8003\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.4674 - acc: 0.9745 - val_loss: 1.0262 - val_acc: 0.7907\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.4667 - acc: 0.9737 - val_loss: 1.0062 - val_acc: 0.7967\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4672 - acc: 0.9732 - val_loss: 1.0069 - val_acc: 0.7943\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4641 - acc: 0.9751 - val_loss: 1.0039 - val_acc: 0.7990\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.4687 - acc: 0.9725 - val_loss: 1.0122 - val_acc: 0.7990\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.4628 - acc: 0.9768 - val_loss: 1.0004 - val_acc: 0.7977\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.4553 - acc: 0.9772 - val_loss: 1.0340 - val_acc: 0.7903\n",
      "Epoch 77/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.9732\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4606 - acc: 0.9734 - val_loss: 1.0258 - val_acc: 0.7913\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4418 - acc: 0.9826 - val_loss: 1.0002 - val_acc: 0.8010\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 6s 494us/sample - loss: 0.4315 - acc: 0.9872 - val_loss: 0.9901 - val_acc: 0.8047\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4290 - acc: 0.9877 - val_loss: 0.9890 - val_acc: 0.8040\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.4283 - acc: 0.9889 - val_loss: 0.9872 - val_acc: 0.8077\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.4236 - acc: 0.9901 - val_loss: 0.9838 - val_acc: 0.8027\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4238 - acc: 0.9905 - val_loss: 0.9814 - val_acc: 0.8087\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4253 - acc: 0.9889 - val_loss: 0.9878 - val_acc: 0.8027\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.4229 - acc: 0.9896 - val_loss: 0.9863 - val_acc: 0.8033\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.4234 - acc: 0.9898 - val_loss: 0.9887 - val_acc: 0.8007\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.4203 - acc: 0.9909 - val_loss: 0.9911 - val_acc: 0.8007\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.4212 - acc: 0.9907 - val_loss: 0.9798 - val_acc: 0.8070\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.4194 - acc: 0.9901 - val_loss: 0.9967 - val_acc: 0.8017\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4205 - acc: 0.9902 - val_loss: 0.9896 - val_acc: 0.8037\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 6s 493us/sample - loss: 0.4198 - acc: 0.9913 - val_loss: 0.9937 - val_acc: 0.8037\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4197 - acc: 0.9905 - val_loss: 0.9932 - val_acc: 0.8047\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4195 - acc: 0.9901 - val_loss: 0.9889 - val_acc: 0.8010\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4181 - acc: 0.9914 - val_loss: 0.9991 - val_acc: 0.8007\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.4162 - acc: 0.9924 - val_loss: 0.9851 - val_acc: 0.8020\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.4173 - acc: 0.9913 - val_loss: 0.9979 - val_acc: 0.7970\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.4180 - acc: 0.9916 - val_loss: 0.9856 - val_acc: 0.8037\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4159 - acc: 0.9909 - val_loss: 0.9939 - val_acc: 0.8043\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.4192 - acc: 0.9914 - val_loss: 0.9958 - val_acc: 0.8047\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4164 - acc: 0.9920 - val_loss: 1.0017 - val_acc: 0.8013\n",
      "Epoch 101/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.9907\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.4189 - acc: 0.9906 - val_loss: 0.9945 - val_acc: 0.8023\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.4113 - acc: 0.9927 - val_loss: 0.9815 - val_acc: 0.8043\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4072 - acc: 0.9946 - val_loss: 0.9837 - val_acc: 0.8010\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4061 - acc: 0.9940 - val_loss: 0.9820 - val_acc: 0.8037\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.4046 - acc: 0.9948 - val_loss: 0.9825 - val_acc: 0.8043\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.4045 - acc: 0.9946 - val_loss: 0.9841 - val_acc: 0.8050\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4042 - acc: 0.9958 - val_loss: 0.9808 - val_acc: 0.8053\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4043 - acc: 0.9942 - val_loss: 0.9886 - val_acc: 0.8023\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.4045 - acc: 0.9958 - val_loss: 0.9865 - val_acc: 0.8040\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.4056 - acc: 0.9941 - val_loss: 0.9883 - val_acc: 0.8050\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4056 - acc: 0.9937 - val_loss: 0.9919 - val_acc: 0.8020\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.4038 - acc: 0.9952 - val_loss: 0.9909 - val_acc: 0.8003\n",
      "Epoch 113/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4044 - acc: 0.9948 - val_loss: 0.9879 - val_acc: 0.8020\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4046 - acc: 0.9937 - val_loss: 0.9855 - val_acc: 0.7993\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4031 - acc: 0.9954 - val_loss: 0.9832 - val_acc: 0.8043\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4030 - acc: 0.9948 - val_loss: 0.9844 - val_acc: 0.8033\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.4026 - acc: 0.9948 - val_loss: 0.9834 - val_acc: 0.8040\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4014 - acc: 0.9962 - val_loss: 0.9843 - val_acc: 0.8083\n",
      "Epoch 119/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.9955\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4015 - acc: 0.9955 - val_loss: 0.9837 - val_acc: 0.8060\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3986 - acc: 0.9971 - val_loss: 0.9831 - val_acc: 0.8083\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3980 - acc: 0.9967 - val_loss: 0.9808 - val_acc: 0.8090\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3980 - acc: 0.9971 - val_loss: 0.9848 - val_acc: 0.8063\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3968 - acc: 0.9967 - val_loss: 0.9833 - val_acc: 0.8077\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3981 - acc: 0.9967 - val_loss: 0.9828 - val_acc: 0.8073\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3974 - acc: 0.9974 - val_loss: 0.9843 - val_acc: 0.8097\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3980 - acc: 0.9962 - val_loss: 0.9848 - val_acc: 0.8093\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.3979 - acc: 0.9961 - val_loss: 0.9857 - val_acc: 0.8050\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3978 - acc: 0.9969 - val_loss: 0.9867 - val_acc: 0.8057\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3958 - acc: 0.9973 - val_loss: 0.9864 - val_acc: 0.8073\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3970 - acc: 0.9971 - val_loss: 0.9833 - val_acc: 0.8063\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3969 - acc: 0.9975 - val_loss: 0.9825 - val_acc: 0.8053\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.3981 - acc: 0.9966 - val_loss: 0.9841 - val_acc: 0.8097\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3961 - acc: 0.9967 - val_loss: 0.9844 - val_acc: 0.8063\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3966 - acc: 0.9966 - val_loss: 0.9862 - val_acc: 0.8070\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3973 - acc: 0.9968 - val_loss: 0.9854 - val_acc: 0.8080\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3966 - acc: 0.9969 - val_loss: 0.9828 - val_acc: 0.8083\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3953 - acc: 0.9973 - val_loss: 0.9854 - val_acc: 0.8070\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3962 - acc: 0.9967 - val_loss: 0.9848 - val_acc: 0.8063\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3954 - acc: 0.9965 - val_loss: 0.9850 - val_acc: 0.8083\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.3958 - acc: 0.9961 - val_loss: 0.9878 - val_acc: 0.8057\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3975 - acc: 0.9964 - val_loss: 0.9848 - val_acc: 0.8050\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3959 - acc: 0.9967 - val_loss: 0.9857 - val_acc: 0.8070\n",
      "Epoch 143/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.9973\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3954 - acc: 0.9973 - val_loss: 0.9850 - val_acc: 0.8027\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.3941 - acc: 0.9973 - val_loss: 0.9815 - val_acc: 0.8073\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3937 - acc: 0.9976 - val_loss: 0.9796 - val_acc: 0.8083\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.3929 - acc: 0.9980 - val_loss: 0.9823 - val_acc: 0.8067\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3942 - acc: 0.9975 - val_loss: 0.9838 - val_acc: 0.8053\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3924 - acc: 0.9976 - val_loss: 0.9816 - val_acc: 0.8077\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.3929 - acc: 0.9977 - val_loss: 0.9827 - val_acc: 0.8093\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3935 - acc: 0.9971 - val_loss: 0.9827 - val_acc: 0.8080\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3933 - acc: 0.9975 - val_loss: 0.9813 - val_acc: 0.8077\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3924 - acc: 0.9983 - val_loss: 0.9814 - val_acc: 0.8070\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.3925 - acc: 0.9980 - val_loss: 0.9817 - val_acc: 0.8090\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3941 - acc: 0.9973 - val_loss: 0.9815 - val_acc: 0.8077\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3925 - acc: 0.9974 - val_loss: 0.9815 - val_acc: 0.8063\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.3940 - acc: 0.9976 - val_loss: 0.9818 - val_acc: 0.8067\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.3926 - acc: 0.9980 - val_loss: 0.9809 - val_acc: 0.8070\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3920 - acc: 0.9976 - val_loss: 0.9806 - val_acc: 0.8083\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3930 - acc: 0.9973 - val_loss: 0.9815 - val_acc: 0.8083\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.3931 - acc: 0.9976 - val_loss: 0.9807 - val_acc: 0.8060\n",
      "Epoch 161/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.9976\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3930 - acc: 0.9976 - val_loss: 0.9821 - val_acc: 0.8047\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3920 - acc: 0.9981 - val_loss: 0.9825 - val_acc: 0.8077\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.3909 - acc: 0.9980 - val_loss: 0.9806 - val_acc: 0.8077\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.3910 - acc: 0.9979 - val_loss: 0.9801 - val_acc: 0.8080\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3923 - acc: 0.9974 - val_loss: 0.9824 - val_acc: 0.8077\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3914 - acc: 0.9981 - val_loss: 0.9819 - val_acc: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3913 - acc: 0.9980 - val_loss: 0.9803 - val_acc: 0.8067\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3911 - acc: 0.9983 - val_loss: 0.9794 - val_acc: 0.8087\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3917 - acc: 0.9977 - val_loss: 0.9811 - val_acc: 0.8083\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3911 - acc: 0.9974 - val_loss: 0.9811 - val_acc: 0.8093\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3919 - acc: 0.9971 - val_loss: 0.9799 - val_acc: 0.8100\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3910 - acc: 0.9978 - val_loss: 0.9807 - val_acc: 0.8090\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.3911 - acc: 0.9984 - val_loss: 0.9821 - val_acc: 0.8077\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.3911 - acc: 0.9983 - val_loss: 0.9802 - val_acc: 0.8090\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3898 - acc: 0.9988 - val_loss: 0.9810 - val_acc: 0.8080\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3907 - acc: 0.9979 - val_loss: 0.9805 - val_acc: 0.8083\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.3917 - acc: 0.9973 - val_loss: 0.9811 - val_acc: 0.8073\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3916 - acc: 0.9977 - val_loss: 0.9822 - val_acc: 0.8067\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3903 - acc: 0.9977 - val_loss: 0.9805 - val_acc: 0.8090\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3912 - acc: 0.9978 - val_loss: 0.9806 - val_acc: 0.8090\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.3914 - acc: 0.9982 - val_loss: 0.9820 - val_acc: 0.8090\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.3904 - acc: 0.9977 - val_loss: 0.9797 - val_acc: 0.8113\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3906 - acc: 0.9983 - val_loss: 0.9804 - val_acc: 0.8097\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3910 - acc: 0.9981 - val_loss: 0.9822 - val_acc: 0.8080\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3907 - acc: 0.9983 - val_loss: 0.9813 - val_acc: 0.8097\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3905 - acc: 0.9980 - val_loss: 0.9824 - val_acc: 0.8087\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.3907 - acc: 0.9978 - val_loss: 0.9810 - val_acc: 0.8100\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3898 - acc: 0.9987 - val_loss: 0.9803 - val_acc: 0.8097\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3901 - acc: 0.9980 - val_loss: 0.9815 - val_acc: 0.8097\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3901 - acc: 0.9981 - val_loss: 0.9803 - val_acc: 0.8103\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.3900 - acc: 0.9981 - val_loss: 0.9797 - val_acc: 0.8100\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.3906 - acc: 0.9977 - val_loss: 0.9813 - val_acc: 0.8083\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3908 - acc: 0.9977 - val_loss: 0.9824 - val_acc: 0.8083\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.3903 - acc: 0.9983 - val_loss: 0.9802 - val_acc: 0.8083\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.3910 - acc: 0.9978 - val_loss: 0.9807 - val_acc: 0.8080\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3895 - acc: 0.9981 - val_loss: 0.9812 - val_acc: 0.8080\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3910 - acc: 0.9975 - val_loss: 0.9814 - val_acc: 0.8083\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3894 - acc: 0.9984 - val_loss: 0.9807 - val_acc: 0.8080\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3899 - acc: 0.9982 - val_loss: 0.9808 - val_acc: 0.8097\n",
      "Epoch 200/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.9976\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3903 - acc: 0.9977 - val_loss: 0.9832 - val_acc: 0.8090\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3897 - acc: 0.9979 - val_loss: 0.9815 - val_acc: 0.8097\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.3902 - acc: 0.9980 - val_loss: 0.9822 - val_acc: 0.8083\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3898 - acc: 0.9983 - val_loss: 0.9816 - val_acc: 0.8097\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3897 - acc: 0.9980 - val_loss: 0.9814 - val_acc: 0.8093\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.3896 - acc: 0.9985 - val_loss: 0.9812 - val_acc: 0.8093\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3893 - acc: 0.9983 - val_loss: 0.9820 - val_acc: 0.8080\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3890 - acc: 0.9986 - val_loss: 0.9821 - val_acc: 0.8083\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3908 - acc: 0.9982 - val_loss: 0.9817 - val_acc: 0.8090\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.3892 - acc: 0.9982 - val_loss: 0.9829 - val_acc: 0.8107\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3891 - acc: 0.9981 - val_loss: 0.9819 - val_acc: 0.8113\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3896 - acc: 0.9980 - val_loss: 0.9818 - val_acc: 0.8090\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.3890 - acc: 0.9980 - val_loss: 0.9822 - val_acc: 0.8093\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.3888 - acc: 0.9985 - val_loss: 0.9817 - val_acc: 0.8097\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3897 - acc: 0.9985 - val_loss: 0.9818 - val_acc: 0.8087\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3888 - acc: 0.9985 - val_loss: 0.9815 - val_acc: 0.8103\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3901 - acc: 0.9977 - val_loss: 0.9811 - val_acc: 0.8117\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3898 - acc: 0.9976 - val_loss: 0.9813 - val_acc: 0.8110\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3896 - acc: 0.9980 - val_loss: 0.9814 - val_acc: 0.8093\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.3898 - acc: 0.9984 - val_loss: 0.9811 - val_acc: 0.8077\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.3899 - acc: 0.9979 - val_loss: 0.9807 - val_acc: 0.8090\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3893 - acc: 0.9987 - val_loss: 0.9800 - val_acc: 0.8093\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3891 - acc: 0.9978 - val_loss: 0.9805 - val_acc: 0.8097\n",
      "Epoch 223/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.3901 - acc: 0.9983 - val_loss: 0.9815 - val_acc: 0.8100\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3898 - acc: 0.9984 - val_loss: 0.9818 - val_acc: 0.8087\n",
      "Epoch 225/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3895 - acc: 0.9976 - val_loss: 0.9803 - val_acc: 0.8093\n",
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.3893 - acc: 0.9980 - val_loss: 0.9805 - val_acc: 0.8107\n",
      "Epoch 227/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.3894 - acc: 0.9986 - val_loss: 0.9811 - val_acc: 0.8100\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3884 - acc: 0.9984 - val_loss: 0.9816 - val_acc: 0.8103\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3879 - acc: 0.9987 - val_loss: 0.9825 - val_acc: 0.8107\n",
      "Epoch 230/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.3888 - acc: 0.9981 - val_loss: 0.9825 - val_acc: 0.8093\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3894 - acc: 0.9986 - val_loss: 0.9822 - val_acc: 0.8097\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3894 - acc: 0.9977 - val_loss: 0.9822 - val_acc: 0.8090\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3890 - acc: 0.9986 - val_loss: 0.9821 - val_acc: 0.8083\n",
      "Epoch 234/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.9983\n",
      "Epoch 00234: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3893 - acc: 0.9983 - val_loss: 0.9824 - val_acc: 0.8087\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3888 - acc: 0.9985 - val_loss: 0.9823 - val_acc: 0.8077\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3888 - acc: 0.9983 - val_loss: 0.9818 - val_acc: 0.8077\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3884 - acc: 0.9981 - val_loss: 0.9818 - val_acc: 0.8077\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3884 - acc: 0.9986 - val_loss: 0.9813 - val_acc: 0.8083\n",
      "Epoch 239/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3874 - acc: 0.9987 - val_loss: 0.9811 - val_acc: 0.8090\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3885 - acc: 0.9987 - val_loss: 0.9811 - val_acc: 0.8093\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3890 - acc: 0.9985 - val_loss: 0.9810 - val_acc: 0.8093\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3886 - acc: 0.9984 - val_loss: 0.9812 - val_acc: 0.8110\n",
      "Epoch 243/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3884 - acc: 0.9983 - val_loss: 0.9820 - val_acc: 0.8100\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.3895 - acc: 0.9981 - val_loss: 0.9821 - val_acc: 0.8090\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3887 - acc: 0.9982 - val_loss: 0.9817 - val_acc: 0.8100\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3887 - acc: 0.9982 - val_loss: 0.9815 - val_acc: 0.8083\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.3880 - acc: 0.9983 - val_loss: 0.9817 - val_acc: 0.8097\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3884 - acc: 0.9987 - val_loss: 0.9814 - val_acc: 0.8103\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3892 - acc: 0.9982 - val_loss: 0.9813 - val_acc: 0.8090\n",
      "Epoch 250/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3888 - acc: 0.9979 - val_loss: 0.9814 - val_acc: 0.8103\n",
      "Epoch 251/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3888 - acc: 0.9983 - val_loss: 0.9819 - val_acc: 0.8103\n",
      "Epoch 252/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.9980\n",
      "Epoch 00252: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3889 - acc: 0.9981 - val_loss: 0.9820 - val_acc: 0.8093\n",
      "Epoch 253/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3888 - acc: 0.9983 - val_loss: 0.9821 - val_acc: 0.8100\n",
      "Epoch 254/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.3879 - acc: 0.9985 - val_loss: 0.9819 - val_acc: 0.8103\n",
      "Epoch 255/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3891 - acc: 0.9971 - val_loss: 0.9815 - val_acc: 0.8100\n",
      "Epoch 256/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3893 - acc: 0.9980 - val_loss: 0.9820 - val_acc: 0.8103\n",
      "Epoch 257/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3891 - acc: 0.9983 - val_loss: 0.9821 - val_acc: 0.8097\n",
      "Epoch 258/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.3887 - acc: 0.9983 - val_loss: 0.9817 - val_acc: 0.8093\n",
      "Epoch 259/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3877 - acc: 0.9982 - val_loss: 0.9817 - val_acc: 0.8090\n",
      "Epoch 260/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3885 - acc: 0.9985 - val_loss: 0.9815 - val_acc: 0.8103\n",
      "Epoch 261/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3879 - acc: 0.9983 - val_loss: 0.9815 - val_acc: 0.8103\n",
      "Epoch 262/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3886 - acc: 0.9982 - val_loss: 0.9817 - val_acc: 0.8100\n",
      "Epoch 263/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3893 - acc: 0.9978 - val_loss: 0.9818 - val_acc: 0.8097\n",
      "Epoch 264/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.3879 - acc: 0.9985 - val_loss: 0.9819 - val_acc: 0.8093\n",
      "Epoch 265/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.3886 - acc: 0.9981 - val_loss: 0.9817 - val_acc: 0.8093\n",
      "Epoch 266/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3885 - acc: 0.9988 - val_loss: 0.9818 - val_acc: 0.8087\n",
      "Epoch 267/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3892 - acc: 0.9983 - val_loss: 0.9815 - val_acc: 0.8087\n",
      "Epoch 268/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3883 - acc: 0.9984 - val_loss: 0.9814 - val_acc: 0.8097\n",
      "Epoch 269/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.3883 - acc: 0.9981 - val_loss: 0.9810 - val_acc: 0.8107\n",
      "Epoch 270/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.9983\n",
      "Epoch 00270: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3887 - acc: 0.9983 - val_loss: 0.9810 - val_acc: 0.8113\n",
      "Epoch 271/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3890 - acc: 0.9980 - val_loss: 0.9810 - val_acc: 0.8103\n",
      "Epoch 272/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.3886 - acc: 0.9987 - val_loss: 0.9809 - val_acc: 0.8103\n",
      "Epoch 273/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3877 - acc: 0.9984 - val_loss: 0.9807 - val_acc: 0.8103\n",
      "Epoch 274/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3883 - acc: 0.9987 - val_loss: 0.9808 - val_acc: 0.8103\n",
      "Epoch 275/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.3886 - acc: 0.9983 - val_loss: 0.9810 - val_acc: 0.8103\n",
      "Epoch 276/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3890 - acc: 0.9983 - val_loss: 0.9811 - val_acc: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00276: early stopping\n",
      "0.812\n",
      "0.83243\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_3 (TensorFlow [(None, 60, 3), (Non 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_3[0][2]        \n",
      "                                                                 tf_op_layer_split_3[0][3]        \n",
      "                                                                 tf_op_layer_split_3[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_3[0][0]        \n",
      "                                                                 tf_op_layer_split_3[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_3[0][1]        \n",
      "                                                                 tf_op_layer_split_3[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                  (None, 60, 32)       5376        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_57 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_60 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_3[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_63 (LSTM)                  (None, 60, 16)       1280        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_66 (LSTM)                  (None, 60, 32)       4736        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_69 (LSTM)                  (None, 60, 32)       4736        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_54 (LayerNo (None, 60, 32)       64          lstm_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (None, 60, 16)       32          lstm_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_60 (LayerNo (None, 60, 16)       32          lstm_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_63 (LayerNo (None, 60, 16)       32          lstm_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_66 (LayerNo (None, 60, 32)       64          lstm_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_69 (LayerNo (None, 60, 32)       64          lstm_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 60, 32)       0           layer_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 60, 16)       0           layer_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 60, 16)       0           layer_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 60, 16)       0           layer_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 60, 32)       0           layer_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 60, 32)       0           layer_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                  (None, 60, 32)       8320        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_58 (LSTM)                  (None, 60, 16)       2112        dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  (None, 60, 16)       2112        dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_64 (LSTM)                  (None, 60, 16)       2112        dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_67 (LSTM)                  (None, 60, 32)       8320        dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_70 (LSTM)                  (None, 60, 32)       8320        dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_55 (LayerNo (None, 60, 32)       64          lstm_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (None, 60, 16)       32          lstm_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_61 (LayerNo (None, 60, 16)       32          lstm_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_64 (LayerNo (None, 60, 16)       32          lstm_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_67 (LayerNo (None, 60, 32)       64          lstm_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_70 (LayerNo (None, 60, 32)       64          lstm_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 60, 32)       0           layer_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 60, 16)       0           layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 60, 16)       0           layer_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 60, 16)       0           layer_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 60, 32)       0           layer_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 60, 32)       0           layer_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_56 (LSTM)                  (None, 60, 32)       8320        dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_59 (LSTM)                  (None, 60, 16)       2112        dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_62 (LSTM)                  (None, 60, 16)       2112        dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_65 (LSTM)                  (None, 60, 16)       2112        dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_68 (LSTM)                  (None, 60, 32)       8320        dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (None, 60, 32)       8320        dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 60, 32)       0           lstm_56[0][0]                    \n",
      "                                                                 lstm_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 60, 16)       0           lstm_59[0][0]                    \n",
      "                                                                 lstm_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 60, 16)       0           lstm_62[0][0]                    \n",
      "                                                                 lstm_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 60, 16)       0           lstm_65[0][0]                    \n",
      "                                                                 lstm_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 60, 32)       0           lstm_68[0][0]                    \n",
      "                                                                 lstm_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 60, 32)       0           lstm_71[0][0]                    \n",
      "                                                                 lstm_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (None, 60, 32)       64          attention_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (None, 60, 16)       32          attention_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, 60, 16)       32          attention_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_65 (LayerNo (None, 60, 16)       32          attention_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_68 (LayerNo (None, 60, 32)       64          attention_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, 60, 32)       64          attention_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 60, 32)       0           layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 60, 16)       0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 60, 16)       0           layer_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 60, 16)       0           layer_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 60, 32)       0           layer_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 60, 32)       0           layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 16)           0           dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 16)           0           dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 16)           0           dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 144)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 144)          576         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          37120       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256)          1024        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           16448       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n",
      "12000/12000 [==============================] - 37s 3ms/sample - loss: 1.8232 - acc: 0.4736 - val_loss: 1.7045 - val_acc: 0.5113\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 1.3595 - acc: 0.6116 - val_loss: 1.4151 - val_acc: 0.6060\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 1.2307 - acc: 0.6581 - val_loss: 1.2834 - val_acc: 0.6490\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 1.1431 - acc: 0.6947 - val_loss: 1.1968 - val_acc: 0.6783\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 1.0755 - acc: 0.7237 - val_loss: 1.1345 - val_acc: 0.6913\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 1.0304 - acc: 0.7364 - val_loss: 1.1581 - val_acc: 0.6910\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9907 - acc: 0.7543 - val_loss: 1.1097 - val_acc: 0.7023\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.9548 - acc: 0.7668 - val_loss: 1.0755 - val_acc: 0.7243\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.9294 - acc: 0.7774 - val_loss: 1.0757 - val_acc: 0.7250\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.9063 - acc: 0.7888 - val_loss: 1.0989 - val_acc: 0.7183\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8763 - acc: 0.7946 - val_loss: 1.0443 - val_acc: 0.7373\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.8543 - acc: 0.8076 - val_loss: 1.0512 - val_acc: 0.7460\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.8372 - acc: 0.8139 - val_loss: 1.0521 - val_acc: 0.7360\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.8190 - acc: 0.8230 - val_loss: 1.0575 - val_acc: 0.7377\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.7992 - acc: 0.8307 - val_loss: 1.0414 - val_acc: 0.7530\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.7840 - acc: 0.8365 - val_loss: 1.0204 - val_acc: 0.7563\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.7591 - acc: 0.8471 - val_loss: 1.0484 - val_acc: 0.7503\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.7574 - acc: 0.8477 - val_loss: 1.0286 - val_acc: 0.7593\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.7358 - acc: 0.8557 - val_loss: 1.0491 - val_acc: 0.7530\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.7228 - acc: 0.8635 - val_loss: 1.0072 - val_acc: 0.7640\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.7143 - acc: 0.8651 - val_loss: 1.0356 - val_acc: 0.7600\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.6963 - acc: 0.8746 - val_loss: 1.0358 - val_acc: 0.7637\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.6883 - acc: 0.8777 - val_loss: 1.0385 - val_acc: 0.7537\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6825 - acc: 0.8817 - val_loss: 1.0203 - val_acc: 0.7660\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6777 - acc: 0.8817 - val_loss: 1.0196 - val_acc: 0.7687\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.6678 - acc: 0.8840 - val_loss: 1.0340 - val_acc: 0.7633\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.6574 - acc: 0.8912 - val_loss: 1.0284 - val_acc: 0.7573\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.6412 - acc: 0.8992 - val_loss: 1.0388 - val_acc: 0.7620\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.6331 - acc: 0.9000 - val_loss: 1.0300 - val_acc: 0.7713\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.6278 - acc: 0.9046 - val_loss: 1.0533 - val_acc: 0.7683\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.6187 - acc: 0.9080 - val_loss: 1.0247 - val_acc: 0.7683\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.6066 - acc: 0.9153 - val_loss: 1.0313 - val_acc: 0.7720\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.6038 - acc: 0.9148 - val_loss: 1.0386 - val_acc: 0.7693\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6007 - acc: 0.9183 - val_loss: 1.0345 - val_acc: 0.7717\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.5902 - acc: 0.9216 - val_loss: 1.0273 - val_acc: 0.7733\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.5854 - acc: 0.9242 - val_loss: 1.0453 - val_acc: 0.7693\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.5796 - acc: 0.9259 - val_loss: 1.0423 - val_acc: 0.7697\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.5698 - acc: 0.9309 - val_loss: 1.0336 - val_acc: 0.7737\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.5683 - acc: 0.9318 - val_loss: 1.0370 - val_acc: 0.7667\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5652 - acc: 0.9332 - val_loss: 1.0420 - val_acc: 0.7753\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5652 - acc: 0.9310 - val_loss: 1.0464 - val_acc: 0.7687\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5546 - acc: 0.9376 - val_loss: 1.0667 - val_acc: 0.7627\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.5503 - acc: 0.9395 - val_loss: 1.0384 - val_acc: 0.7797\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5444 - acc: 0.9421 - val_loss: 1.0358 - val_acc: 0.7763\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5365 - acc: 0.9449 - val_loss: 1.0456 - val_acc: 0.7800\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5311 - acc: 0.9481 - val_loss: 1.0444 - val_acc: 0.7777\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.5301 - acc: 0.9486 - val_loss: 1.0600 - val_acc: 0.7817\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5270 - acc: 0.9494 - val_loss: 1.0354 - val_acc: 0.7760\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.5300 - acc: 0.9482 - val_loss: 1.0519 - val_acc: 0.7727\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5240 - acc: 0.9500 - val_loss: 1.0557 - val_acc: 0.7807\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5172 - acc: 0.9532 - val_loss: 1.0541 - val_acc: 0.7693\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.5088 - acc: 0.9556 - val_loss: 1.0366 - val_acc: 0.7780\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.5079 - acc: 0.9556 - val_loss: 1.0485 - val_acc: 0.7737\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5159 - acc: 0.9519 - val_loss: 1.0543 - val_acc: 0.7750\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.5108 - acc: 0.9549 - val_loss: 1.0672 - val_acc: 0.7837\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.5049 - acc: 0.9603 - val_loss: 1.0207 - val_acc: 0.7833\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.5041 - acc: 0.9582 - val_loss: 1.0471 - val_acc: 0.7807\n",
      "Epoch 58/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5003 - acc: 0.9607 - val_loss: 1.0704 - val_acc: 0.7833\n",
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.4916 - acc: 0.9646 - val_loss: 1.0682 - val_acc: 0.7767\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4999 - acc: 0.9611 - val_loss: 1.0706 - val_acc: 0.7693\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4960 - acc: 0.9607 - val_loss: 1.0489 - val_acc: 0.7780\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.4874 - acc: 0.9652 - val_loss: 1.0493 - val_acc: 0.7810\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4843 - acc: 0.9657 - val_loss: 1.0502 - val_acc: 0.7777\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4840 - acc: 0.9680 - val_loss: 1.0619 - val_acc: 0.7803\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4827 - acc: 0.9669 - val_loss: 1.0510 - val_acc: 0.7780\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.4830 - acc: 0.9654 - val_loss: 1.0837 - val_acc: 0.7780\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4803 - acc: 0.9685 - val_loss: 1.0444 - val_acc: 0.7837\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4764 - acc: 0.9719 - val_loss: 1.0477 - val_acc: 0.7863\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.4708 - acc: 0.9722 - val_loss: 1.0591 - val_acc: 0.7817\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4701 - acc: 0.9729 - val_loss: 1.0662 - val_acc: 0.7843\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4738 - acc: 0.9701 - val_loss: 1.0689 - val_acc: 0.7837\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.4685 - acc: 0.9722 - val_loss: 1.0588 - val_acc: 0.7837\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4686 - acc: 0.9718 - val_loss: 1.0831 - val_acc: 0.7783\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4679 - acc: 0.9761 - val_loss: 1.0661 - val_acc: 0.7787\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4670 - acc: 0.9726 - val_loss: 1.0524 - val_acc: 0.7850\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.4569 - acc: 0.9787 - val_loss: 1.0497 - val_acc: 0.7763\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.4563 - acc: 0.9780 - val_loss: 1.0480 - val_acc: 0.7873\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4540 - acc: 0.9783 - val_loss: 1.0651 - val_acc: 0.7810\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.4633 - acc: 0.9741 - val_loss: 1.0514 - val_acc: 0.7803\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.4532 - acc: 0.9778 - val_loss: 1.0549 - val_acc: 0.7817\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.4536 - acc: 0.9790 - val_loss: 1.0288 - val_acc: 0.7913\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.4483 - acc: 0.9812 - val_loss: 1.0598 - val_acc: 0.7817\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.4539 - acc: 0.9782 - val_loss: 1.0545 - val_acc: 0.7820\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4556 - acc: 0.9783 - val_loss: 1.0528 - val_acc: 0.7843\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4475 - acc: 0.9820 - val_loss: 1.0506 - val_acc: 0.7830\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.4484 - acc: 0.9818 - val_loss: 1.0548 - val_acc: 0.7850\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.4457 - acc: 0.9807 - val_loss: 1.0622 - val_acc: 0.7780\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4432 - acc: 0.9812 - val_loss: 1.0372 - val_acc: 0.7910\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.4439 - acc: 0.9818 - val_loss: 1.0753 - val_acc: 0.7817\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.4452 - acc: 0.9819 - val_loss: 1.0560 - val_acc: 0.7823\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.4479 - acc: 0.9812 - val_loss: 1.0395 - val_acc: 0.7933\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.4422 - acc: 0.9812 - val_loss: 1.0456 - val_acc: 0.7927\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.4365 - acc: 0.9843 - val_loss: 1.0501 - val_acc: 0.7843\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4365 - acc: 0.9855 - val_loss: 1.0400 - val_acc: 0.7883\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4331 - acc: 0.9871 - val_loss: 1.0468 - val_acc: 0.7910\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4337 - acc: 0.9858 - val_loss: 1.0553 - val_acc: 0.7827\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.4332 - acc: 0.9864 - val_loss: 1.0498 - val_acc: 0.7900\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4359 - acc: 0.9847 - val_loss: 1.0579 - val_acc: 0.7847\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4369 - acc: 0.9842 - val_loss: 1.0698 - val_acc: 0.7813\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.4365 - acc: 0.9844 - val_loss: 1.0434 - val_acc: 0.7867\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4316 - acc: 0.9856 - val_loss: 1.0661 - val_acc: 0.7790\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4328 - acc: 0.9840 - val_loss: 1.0551 - val_acc: 0.7833\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.4339 - acc: 0.9855 - val_loss: 1.0610 - val_acc: 0.7797\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.4271 - acc: 0.9879 - val_loss: 1.0483 - val_acc: 0.7870\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4251 - acc: 0.9887 - val_loss: 1.0421 - val_acc: 0.7930\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4294 - acc: 0.9856 - val_loss: 1.0481 - val_acc: 0.7903\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.4337 - acc: 0.9837 - val_loss: 1.0750 - val_acc: 0.7840\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.4349 - acc: 0.9823 - val_loss: 1.0606 - val_acc: 0.7793\n",
      "Epoch 109/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.9872\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4270 - acc: 0.9873 - val_loss: 1.0417 - val_acc: 0.7913\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4112 - acc: 0.9915 - val_loss: 1.0297 - val_acc: 0.7890\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.4020 - acc: 0.9954 - val_loss: 1.0172 - val_acc: 0.7977\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4019 - acc: 0.9946 - val_loss: 1.0220 - val_acc: 0.7993\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3999 - acc: 0.9958 - val_loss: 1.0238 - val_acc: 0.7937\n",
      "Epoch 114/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.4005 - acc: 0.9951 - val_loss: 1.0264 - val_acc: 0.7970\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3992 - acc: 0.9948 - val_loss: 1.0257 - val_acc: 0.7973\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3974 - acc: 0.9963 - val_loss: 1.0204 - val_acc: 0.7953\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.3980 - acc: 0.9963 - val_loss: 1.0303 - val_acc: 0.7947\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3978 - acc: 0.9955 - val_loss: 1.0184 - val_acc: 0.7933\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3976 - acc: 0.9963 - val_loss: 1.0250 - val_acc: 0.7940\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3958 - acc: 0.9965 - val_loss: 1.0343 - val_acc: 0.7907\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.3985 - acc: 0.9953 - val_loss: 1.0281 - val_acc: 0.7937\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3974 - acc: 0.9955 - val_loss: 1.0216 - val_acc: 0.7950\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3978 - acc: 0.9962 - val_loss: 1.0273 - val_acc: 0.7937\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3972 - acc: 0.9955 - val_loss: 1.0244 - val_acc: 0.7973\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.3977 - acc: 0.9950 - val_loss: 1.0206 - val_acc: 0.7933\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3990 - acc: 0.9944 - val_loss: 1.0236 - val_acc: 0.7947\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3963 - acc: 0.9962 - val_loss: 1.0232 - val_acc: 0.7947\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.3968 - acc: 0.9950 - val_loss: 1.0247 - val_acc: 0.7940\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3971 - acc: 0.9962 - val_loss: 1.0204 - val_acc: 0.7937\n",
      "Epoch 130/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.9969\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3947 - acc: 0.9970 - val_loss: 1.0250 - val_acc: 0.7930\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.3908 - acc: 0.9970 - val_loss: 1.0152 - val_acc: 0.7983\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3892 - acc: 0.9976 - val_loss: 1.0169 - val_acc: 0.7997\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3901 - acc: 0.9972 - val_loss: 1.0131 - val_acc: 0.7987\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3894 - acc: 0.9977 - val_loss: 1.0196 - val_acc: 0.7963\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.3883 - acc: 0.9979 - val_loss: 1.0163 - val_acc: 0.8000\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3877 - acc: 0.9981 - val_loss: 1.0188 - val_acc: 0.7947\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.3863 - acc: 0.9984 - val_loss: 1.0206 - val_acc: 0.7923\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3876 - acc: 0.9977 - val_loss: 1.0180 - val_acc: 0.8007\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.3889 - acc: 0.9969 - val_loss: 1.0148 - val_acc: 0.7947\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3881 - acc: 0.9980 - val_loss: 1.0178 - val_acc: 0.7943\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.3870 - acc: 0.9973 - val_loss: 1.0173 - val_acc: 0.7927\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.3867 - acc: 0.9981 - val_loss: 1.0133 - val_acc: 0.7983\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3868 - acc: 0.9979 - val_loss: 1.0287 - val_acc: 0.7933\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3879 - acc: 0.9969 - val_loss: 1.0179 - val_acc: 0.7940\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.3873 - acc: 0.9977 - val_loss: 1.0217 - val_acc: 0.7963\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3880 - acc: 0.9976 - val_loss: 1.0217 - val_acc: 0.8000\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3865 - acc: 0.9983 - val_loss: 1.0187 - val_acc: 0.7987\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3878 - acc: 0.9977 - val_loss: 1.0211 - val_acc: 0.7970\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.3870 - acc: 0.9978 - val_loss: 1.0230 - val_acc: 0.7980\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3870 - acc: 0.9977 - val_loss: 1.0190 - val_acc: 0.7957\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3886 - acc: 0.9969 - val_loss: 1.0234 - val_acc: 0.7923\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3863 - acc: 0.9976 - val_loss: 1.0178 - val_acc: 0.7930\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3865 - acc: 0.9980 - val_loss: 1.0200 - val_acc: 0.7990\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3878 - acc: 0.9973 - val_loss: 1.0179 - val_acc: 0.7940\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.3869 - acc: 0.9980 - val_loss: 1.0226 - val_acc: 0.7923\n",
      "Epoch 156/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.9975\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.3853 - acc: 0.9976 - val_loss: 1.0143 - val_acc: 0.7973\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3835 - acc: 0.9983 - val_loss: 1.0161 - val_acc: 0.8000\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.3823 - acc: 0.9983 - val_loss: 1.0104 - val_acc: 0.8020\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3830 - acc: 0.9987 - val_loss: 1.0116 - val_acc: 0.8020\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3827 - acc: 0.9984 - val_loss: 1.0144 - val_acc: 0.7957\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3828 - acc: 0.9987 - val_loss: 1.0153 - val_acc: 0.7953\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.3824 - acc: 0.9986 - val_loss: 1.0155 - val_acc: 0.7950\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.3828 - acc: 0.9986 - val_loss: 1.0159 - val_acc: 0.7947\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3822 - acc: 0.9985 - val_loss: 1.0148 - val_acc: 0.7967\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3827 - acc: 0.9986 - val_loss: 1.0174 - val_acc: 0.7947\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.3824 - acc: 0.9991 - val_loss: 1.0156 - val_acc: 0.7947\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3828 - acc: 0.9980 - val_loss: 1.0195 - val_acc: 0.7967\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3825 - acc: 0.9983 - val_loss: 1.0104 - val_acc: 0.8013\n",
      "Epoch 169/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3826 - acc: 0.9985 - val_loss: 1.0144 - val_acc: 0.7993\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.3821 - acc: 0.9987 - val_loss: 1.0132 - val_acc: 0.7967\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3808 - acc: 0.9988 - val_loss: 1.0134 - val_acc: 0.7960\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3818 - acc: 0.9988 - val_loss: 1.0151 - val_acc: 0.7970\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.3816 - acc: 0.9987 - val_loss: 1.0186 - val_acc: 0.7967\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3819 - acc: 0.9987 - val_loss: 1.0149 - val_acc: 0.8010\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3814 - acc: 0.9985 - val_loss: 1.0140 - val_acc: 0.7970\n",
      "Epoch 176/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.9986\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3817 - acc: 0.9985 - val_loss: 1.0137 - val_acc: 0.7993\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.3804 - acc: 0.9987 - val_loss: 1.0139 - val_acc: 0.7983\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3799 - acc: 0.9989 - val_loss: 1.0124 - val_acc: 0.7980\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3806 - acc: 0.9992 - val_loss: 1.0097 - val_acc: 0.8013\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.3794 - acc: 0.9989 - val_loss: 1.0120 - val_acc: 0.7987\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3804 - acc: 0.9987 - val_loss: 1.0122 - val_acc: 0.7970\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3792 - acc: 0.9990 - val_loss: 1.0120 - val_acc: 0.7993\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.3800 - acc: 0.9987 - val_loss: 1.0125 - val_acc: 0.7973\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3805 - acc: 0.9992 - val_loss: 1.0140 - val_acc: 0.7957\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3801 - acc: 0.9990 - val_loss: 1.0157 - val_acc: 0.7953\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3798 - acc: 0.9989 - val_loss: 1.0125 - val_acc: 0.7970\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3786 - acc: 0.9994 - val_loss: 1.0130 - val_acc: 0.7957\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3800 - acc: 0.9989 - val_loss: 1.0135 - val_acc: 0.7967\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3802 - acc: 0.9988 - val_loss: 1.0118 - val_acc: 0.7987\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.3796 - acc: 0.9991 - val_loss: 1.0120 - val_acc: 0.7997\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.3806 - acc: 0.9987 - val_loss: 1.0125 - val_acc: 0.7967\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3789 - acc: 0.9991 - val_loss: 1.0127 - val_acc: 0.7990\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3801 - acc: 0.9987 - val_loss: 1.0138 - val_acc: 0.7950\n",
      "Epoch 194/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.9988\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.3790 - acc: 0.9987 - val_loss: 1.0153 - val_acc: 0.7987\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3794 - acc: 0.9989 - val_loss: 1.0105 - val_acc: 0.7993\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3788 - acc: 0.9989 - val_loss: 1.0113 - val_acc: 0.7983\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.3786 - acc: 0.9988 - val_loss: 1.0108 - val_acc: 0.7987\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3781 - acc: 0.9992 - val_loss: 1.0121 - val_acc: 0.7980\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3786 - acc: 0.9987 - val_loss: 1.0123 - val_acc: 0.7970\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3785 - acc: 0.9988 - val_loss: 1.0116 - val_acc: 0.7977\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.3782 - acc: 0.9992 - val_loss: 1.0127 - val_acc: 0.7973\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3781 - acc: 0.9992 - val_loss: 1.0131 - val_acc: 0.7953\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3784 - acc: 0.9992 - val_loss: 1.0124 - val_acc: 0.7973\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.3781 - acc: 0.9992 - val_loss: 1.0129 - val_acc: 0.7977\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.3782 - acc: 0.9992 - val_loss: 1.0117 - val_acc: 0.7990\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3792 - acc: 0.9990 - val_loss: 1.0102 - val_acc: 0.8010\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3791 - acc: 0.9986 - val_loss: 1.0116 - val_acc: 0.8003\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.3785 - acc: 0.9992 - val_loss: 1.0120 - val_acc: 0.7970\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3780 - acc: 0.9986 - val_loss: 1.0122 - val_acc: 0.7983\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3791 - acc: 0.9987 - val_loss: 1.0118 - val_acc: 0.7980\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.3780 - acc: 0.9992 - val_loss: 1.0121 - val_acc: 0.7990\n",
      "Epoch 212/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.9991\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.3785 - acc: 0.9991 - val_loss: 1.0119 - val_acc: 0.7987\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3780 - acc: 0.9992 - val_loss: 1.0125 - val_acc: 0.7990\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3780 - acc: 0.9992 - val_loss: 1.0123 - val_acc: 0.7987\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.3780 - acc: 0.9991 - val_loss: 1.0131 - val_acc: 0.7983\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3779 - acc: 0.9987 - val_loss: 1.0130 - val_acc: 0.7963\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3783 - acc: 0.9991 - val_loss: 1.0128 - val_acc: 0.7977\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.3780 - acc: 0.9993 - val_loss: 1.0127 - val_acc: 0.7977\n",
      "Epoch 00218: early stopping\n",
      "0.802\n",
      "0.822\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_4 (TensorFlow [(None, 60, 3), (Non 0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_4[0][2]        \n",
      "                                                                 tf_op_layer_split_4[0][3]        \n",
      "                                                                 tf_op_layer_split_4[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_4[0][0]        \n",
      "                                                                 tf_op_layer_split_4[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_4[0][1]        \n",
      "                                                                 tf_op_layer_split_4[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_72 (LSTM)                  (None, 60, 32)       5376        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_75 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_78 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_4[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                  (None, 60, 16)       1280        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_84 (LSTM)                  (None, 60, 32)       4736        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_87 (LSTM)                  (None, 60, 32)       4736        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_72 (LayerNo (None, 60, 32)       64          lstm_72[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, 60, 16)       32          lstm_75[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, 60, 16)       32          lstm_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, 60, 16)       32          lstm_81[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, 60, 32)       64          lstm_84[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 60, 32)       64          lstm_87[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 60, 32)       0           layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 60, 16)       0           layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 60, 16)       0           layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 60, 16)       0           layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 60, 32)       0           layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 60, 32)       0           layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_73 (LSTM)                  (None, 60, 32)       8320        dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_76 (LSTM)                  (None, 60, 16)       2112        dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_79 (LSTM)                  (None, 60, 16)       2112        dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                  (None, 60, 16)       2112        dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_85 (LSTM)                  (None, 60, 32)       8320        dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_88 (LSTM)                  (None, 60, 32)       8320        dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, 60, 32)       64          lstm_73[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, 60, 16)       32          lstm_76[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, 60, 16)       32          lstm_79[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, 60, 16)       32          lstm_82[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 60, 32)       64          lstm_85[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 60, 32)       64          lstm_88[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 60, 32)       0           layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 60, 16)       0           layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 60, 16)       0           layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 60, 16)       0           layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 60, 32)       0           layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 60, 32)       0           layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_74 (LSTM)                  (None, 60, 32)       8320        dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_77 (LSTM)                  (None, 60, 16)       2112        dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_80 (LSTM)                  (None, 60, 16)       2112        dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_83 (LSTM)                  (None, 60, 16)       2112        dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_86 (LSTM)                  (None, 60, 32)       8320        dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_89 (LSTM)                  (None, 60, 32)       8320        dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 60, 32)       0           lstm_74[0][0]                    \n",
      "                                                                 lstm_74[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_25 (Attention)        (None, 60, 16)       0           lstm_77[0][0]                    \n",
      "                                                                 lstm_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_26 (Attention)        (None, 60, 16)       0           lstm_80[0][0]                    \n",
      "                                                                 lstm_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_27 (Attention)        (None, 60, 16)       0           lstm_83[0][0]                    \n",
      "                                                                 lstm_83[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_28 (Attention)        (None, 60, 32)       0           lstm_86[0][0]                    \n",
      "                                                                 lstm_86[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_29 (Attention)        (None, 60, 32)       0           lstm_89[0][0]                    \n",
      "                                                                 lstm_89[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, 60, 32)       64          attention_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, 60, 16)       32          attention_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, 60, 16)       32          attention_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, 60, 16)       32          attention_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 60, 32)       64          attention_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 60, 32)       64          attention_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 60, 32)       0           layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 60, 16)       0           layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 60, 16)       0           layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 60, 16)       0           layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 60, 32)       0           layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 60, 32)       0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 16)           0           dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 16)           0           dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 16)           0           dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 32)           0           dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 32)           0           dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 144)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "                                                                 global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 144)          576         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          37120       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           16448       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           4160        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 38s 3ms/sample - loss: 1.7923 - acc: 0.4826 - val_loss: 1.7249 - val_acc: 0.5077\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 1.3582 - acc: 0.6097 - val_loss: 1.4414 - val_acc: 0.6030\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 1.2358 - acc: 0.6578 - val_loss: 1.2629 - val_acc: 0.6730\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 1.1494 - acc: 0.6861 - val_loss: 1.1967 - val_acc: 0.6870\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 1.0853 - acc: 0.7163 - val_loss: 1.1020 - val_acc: 0.7217\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 1.0275 - acc: 0.7407 - val_loss: 1.0904 - val_acc: 0.7267\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.9855 - acc: 0.7558 - val_loss: 1.0518 - val_acc: 0.7373\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.9703 - acc: 0.7607 - val_loss: 1.0623 - val_acc: 0.7323\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.9432 - acc: 0.7690 - val_loss: 1.0417 - val_acc: 0.7443\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.9075 - acc: 0.7814 - val_loss: 1.0523 - val_acc: 0.7417\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8769 - acc: 0.7977 - val_loss: 1.0268 - val_acc: 0.7567\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.8588 - acc: 0.8057 - val_loss: 1.0278 - val_acc: 0.7550\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.8390 - acc: 0.8152 - val_loss: 1.0132 - val_acc: 0.7687\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8231 - acc: 0.8202 - val_loss: 0.9956 - val_acc: 0.7683\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.8127 - acc: 0.8231 - val_loss: 0.9970 - val_acc: 0.7727\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.7913 - acc: 0.8325 - val_loss: 0.9941 - val_acc: 0.7707\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.7744 - acc: 0.8347 - val_loss: 0.9876 - val_acc: 0.7780\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.7615 - acc: 0.8435 - val_loss: 1.0511 - val_acc: 0.7570\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.7471 - acc: 0.8501 - val_loss: 0.9937 - val_acc: 0.7750\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.7302 - acc: 0.8611 - val_loss: 0.9962 - val_acc: 0.7743\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.7214 - acc: 0.8627 - val_loss: 0.9745 - val_acc: 0.7810\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.7071 - acc: 0.8686 - val_loss: 1.0096 - val_acc: 0.7727\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.6911 - acc: 0.8741 - val_loss: 0.9807 - val_acc: 0.7850\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.6812 - acc: 0.8797 - val_loss: 0.9943 - val_acc: 0.7773\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6746 - acc: 0.8805 - val_loss: 0.9748 - val_acc: 0.7880\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.6656 - acc: 0.8870 - val_loss: 0.9837 - val_acc: 0.7810\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.6622 - acc: 0.8913 - val_loss: 0.9714 - val_acc: 0.7887\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6421 - acc: 0.9003 - val_loss: 0.9715 - val_acc: 0.7907\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.6343 - acc: 0.9004 - val_loss: 0.9824 - val_acc: 0.7857\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.6223 - acc: 0.9072 - val_loss: 0.9857 - val_acc: 0.7900\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.6169 - acc: 0.9111 - val_loss: 1.0041 - val_acc: 0.7873\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.6208 - acc: 0.9069 - val_loss: 1.0014 - val_acc: 0.7840\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.6174 - acc: 0.9078 - val_loss: 1.0089 - val_acc: 0.7800\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.6079 - acc: 0.9141 - val_loss: 0.9973 - val_acc: 0.7893\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.5978 - acc: 0.9182 - val_loss: 0.9900 - val_acc: 0.7927\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5866 - acc: 0.9233 - val_loss: 0.9677 - val_acc: 0.8023\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5815 - acc: 0.9263 - val_loss: 0.9722 - val_acc: 0.7913\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5707 - acc: 0.9295 - val_loss: 0.9888 - val_acc: 0.7890\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.5688 - acc: 0.9306 - val_loss: 1.0164 - val_acc: 0.7823\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.5713 - acc: 0.9320 - val_loss: 0.9951 - val_acc: 0.7960\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.5675 - acc: 0.9302 - val_loss: 0.9876 - val_acc: 0.7937\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5605 - acc: 0.9339 - val_loss: 0.9916 - val_acc: 0.7980\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.5509 - acc: 0.9398 - val_loss: 0.9950 - val_acc: 0.7920\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.5461 - acc: 0.9425 - val_loss: 0.9880 - val_acc: 0.7970\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5413 - acc: 0.9430 - val_loss: 0.9818 - val_acc: 0.8007\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5334 - acc: 0.9463 - val_loss: 1.0006 - val_acc: 0.7897\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.5375 - acc: 0.9455 - val_loss: 0.9905 - val_acc: 0.7967\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.5260 - acc: 0.9522 - val_loss: 0.9878 - val_acc: 0.7997\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.5272 - acc: 0.9482 - val_loss: 0.9741 - val_acc: 0.7993\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5265 - acc: 0.9499 - val_loss: 1.0307 - val_acc: 0.7823\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.5229 - acc: 0.9498 - val_loss: 0.9687 - val_acc: 0.8000\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5165 - acc: 0.9540 - val_loss: 1.0134 - val_acc: 0.7930\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5133 - acc: 0.9561 - val_loss: 0.9805 - val_acc: 0.7960\n",
      "Epoch 54/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.9555Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.5045 - acc: 0.9588 - val_loss: 0.9751 - val_acc: 0.8027\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.4986 - acc: 0.9617 - val_loss: 0.9944 - val_acc: 0.8020\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5032 - acc: 0.9601 - val_loss: 0.9871 - val_acc: 0.7967\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4993 - acc: 0.9632 - val_loss: 0.9842 - val_acc: 0.7980\n",
      "Epoch 59/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.4864 - acc: 0.9664 - val_loss: 0.9933 - val_acc: 0.8050\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.4943 - acc: 0.9620 - val_loss: 1.0022 - val_acc: 0.8000\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4903 - acc: 0.9650 - val_loss: 0.9952 - val_acc: 0.8030\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4906 - acc: 0.9651 - val_loss: 0.9858 - val_acc: 0.8023\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.4809 - acc: 0.9678 - val_loss: 0.9964 - val_acc: 0.8030\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4857 - acc: 0.9657 - val_loss: 1.0024 - val_acc: 0.7973\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.4864 - acc: 0.9652 - val_loss: 0.9904 - val_acc: 0.8037\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.4816 - acc: 0.9669 - val_loss: 0.9848 - val_acc: 0.8077\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.4717 - acc: 0.9712 - val_loss: 0.9899 - val_acc: 0.7993\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4795 - acc: 0.9685 - val_loss: 0.9896 - val_acc: 0.8050\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4722 - acc: 0.9720 - val_loss: 0.9933 - val_acc: 0.8023\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.4656 - acc: 0.9760 - val_loss: 1.0089 - val_acc: 0.8060\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.4655 - acc: 0.9747 - val_loss: 0.9989 - val_acc: 0.8023\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4675 - acc: 0.9734 - val_loss: 0.9932 - val_acc: 0.7997\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.4678 - acc: 0.9731 - val_loss: 0.9951 - val_acc: 0.7963\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4644 - acc: 0.9749 - val_loss: 0.9934 - val_acc: 0.8010\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.4617 - acc: 0.9762 - val_loss: 0.9681 - val_acc: 0.8120\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4607 - acc: 0.9762 - val_loss: 0.9886 - val_acc: 0.8063\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4604 - acc: 0.9763 - val_loss: 0.9821 - val_acc: 0.8097\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4523 - acc: 0.9782 - val_loss: 0.9877 - val_acc: 0.8063\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4509 - acc: 0.9803 - val_loss: 1.0038 - val_acc: 0.8010\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 375us/sample - loss: 0.4543 - acc: 0.9775 - val_loss: 0.9916 - val_acc: 0.8033\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.4559 - acc: 0.9784 - val_loss: 0.9995 - val_acc: 0.8013\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4475 - acc: 0.9815 - val_loss: 0.9934 - val_acc: 0.8007\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4509 - acc: 0.9784 - val_loss: 0.9950 - val_acc: 0.8027\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.4444 - acc: 0.9814 - val_loss: 0.9900 - val_acc: 0.8030\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4485 - acc: 0.9809 - val_loss: 0.9952 - val_acc: 0.8047\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4488 - acc: 0.9803 - val_loss: 0.9831 - val_acc: 0.8023\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.4488 - acc: 0.9793 - val_loss: 0.9825 - val_acc: 0.8017\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4440 - acc: 0.9820 - val_loss: 0.9881 - val_acc: 0.8077\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4414 - acc: 0.9836 - val_loss: 0.9768 - val_acc: 0.8033\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4422 - acc: 0.9837 - val_loss: 0.9782 - val_acc: 0.8073\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4390 - acc: 0.9836 - val_loss: 0.9643 - val_acc: 0.8120\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4345 - acc: 0.9863 - val_loss: 0.9706 - val_acc: 0.8110\n",
      "Epoch 93/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4358 - acc: 0.9848\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4360 - acc: 0.9846 - val_loss: 0.9748 - val_acc: 0.8023\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.4182 - acc: 0.9918 - val_loss: 0.9522 - val_acc: 0.8117\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4120 - acc: 0.9943 - val_loss: 0.9576 - val_acc: 0.8093\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4108 - acc: 0.9935 - val_loss: 0.9517 - val_acc: 0.8167\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4089 - acc: 0.9945 - val_loss: 0.9610 - val_acc: 0.8100\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4079 - acc: 0.9948 - val_loss: 0.9687 - val_acc: 0.8110\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4087 - acc: 0.9935 - val_loss: 0.9642 - val_acc: 0.8113\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4097 - acc: 0.9930 - val_loss: 0.9573 - val_acc: 0.8090\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.4070 - acc: 0.9939 - val_loss: 0.9564 - val_acc: 0.8100\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4080 - acc: 0.9935 - val_loss: 0.9593 - val_acc: 0.8110\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.4076 - acc: 0.9940 - val_loss: 0.9638 - val_acc: 0.8063\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.4077 - acc: 0.9942 - val_loss: 0.9741 - val_acc: 0.8090\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4129 - acc: 0.9912 - val_loss: 0.9619 - val_acc: 0.8063\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4063 - acc: 0.9942 - val_loss: 0.9682 - val_acc: 0.8120\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4066 - acc: 0.9942 - val_loss: 0.9654 - val_acc: 0.8073\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.4066 - acc: 0.9941 - val_loss: 0.9693 - val_acc: 0.8057\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.4082 - acc: 0.9939 - val_loss: 0.9617 - val_acc: 0.8073\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4059 - acc: 0.9946 - val_loss: 0.9651 - val_acc: 0.8057\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4039 - acc: 0.9952 - val_loss: 0.9676 - val_acc: 0.8070\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4067 - acc: 0.9936 - val_loss: 0.9635 - val_acc: 0.8067\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4043 - acc: 0.9946 - val_loss: 0.9667 - val_acc: 0.8087\n",
      "Epoch 114/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.9951\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4031 - acc: 0.9951 - val_loss: 0.9630 - val_acc: 0.8107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.3997 - acc: 0.9958 - val_loss: 0.9616 - val_acc: 0.8130\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3963 - acc: 0.9964 - val_loss: 0.9639 - val_acc: 0.8083\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3954 - acc: 0.9975 - val_loss: 0.9625 - val_acc: 0.8120\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.3946 - acc: 0.9975 - val_loss: 0.9608 - val_acc: 0.8113\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.3954 - acc: 0.9963 - val_loss: 0.9598 - val_acc: 0.8100\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3970 - acc: 0.9966 - val_loss: 0.9573 - val_acc: 0.8100\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3961 - acc: 0.9964 - val_loss: 0.9542 - val_acc: 0.8133\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.3950 - acc: 0.9973 - val_loss: 0.9593 - val_acc: 0.8090\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3947 - acc: 0.9971 - val_loss: 0.9657 - val_acc: 0.8083\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3944 - acc: 0.9973 - val_loss: 0.9508 - val_acc: 0.8113\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.3948 - acc: 0.9967 - val_loss: 0.9618 - val_acc: 0.8103\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3947 - acc: 0.9973 - val_loss: 0.9632 - val_acc: 0.8097\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3933 - acc: 0.9977 - val_loss: 0.9611 - val_acc: 0.8093\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3942 - acc: 0.9968 - val_loss: 0.9665 - val_acc: 0.8110\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3926 - acc: 0.9980 - val_loss: 0.9630 - val_acc: 0.8067\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3928 - acc: 0.9980 - val_loss: 0.9613 - val_acc: 0.8087\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3958 - acc: 0.9967 - val_loss: 0.9577 - val_acc: 0.8070\n",
      "Epoch 132/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.9975\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.3923 - acc: 0.9974 - val_loss: 0.9543 - val_acc: 0.8117\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.3901 - acc: 0.9982 - val_loss: 0.9602 - val_acc: 0.8100\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3896 - acc: 0.9987 - val_loss: 0.9635 - val_acc: 0.8050\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3898 - acc: 0.9977 - val_loss: 0.9562 - val_acc: 0.8090\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.3888 - acc: 0.9989 - val_loss: 0.9600 - val_acc: 0.8090\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3894 - acc: 0.9987 - val_loss: 0.9592 - val_acc: 0.8087\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.3886 - acc: 0.9980 - val_loss: 0.9572 - val_acc: 0.8103\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3899 - acc: 0.9982 - val_loss: 0.9612 - val_acc: 0.8063\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.3887 - acc: 0.9981 - val_loss: 0.9636 - val_acc: 0.8073\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3909 - acc: 0.9974 - val_loss: 0.9634 - val_acc: 0.8053\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3884 - acc: 0.9988 - val_loss: 0.9615 - val_acc: 0.8070\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.3875 - acc: 0.9985 - val_loss: 0.9588 - val_acc: 0.8087\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.3882 - acc: 0.9983 - val_loss: 0.9585 - val_acc: 0.8093\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3882 - acc: 0.9985 - val_loss: 0.9610 - val_acc: 0.8087\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.3883 - acc: 0.9984 - val_loss: 0.9592 - val_acc: 0.8087\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.3880 - acc: 0.9986 - val_loss: 0.9585 - val_acc: 0.8043\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3877 - acc: 0.9983 - val_loss: 0.9580 - val_acc: 0.8070\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3888 - acc: 0.9981 - val_loss: 0.9603 - val_acc: 0.8093\n",
      "Epoch 150/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.9988\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.3889 - acc: 0.9987 - val_loss: 0.9554 - val_acc: 0.8077\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3865 - acc: 0.9985 - val_loss: 0.9564 - val_acc: 0.8067\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3859 - acc: 0.9994 - val_loss: 0.9583 - val_acc: 0.8070\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.3861 - acc: 0.9992 - val_loss: 0.9578 - val_acc: 0.8073\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.3871 - acc: 0.9979 - val_loss: 0.9568 - val_acc: 0.8073\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3864 - acc: 0.9987 - val_loss: 0.9565 - val_acc: 0.8063\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3860 - acc: 0.9988 - val_loss: 0.9585 - val_acc: 0.8080\n",
      "Epoch 00156: early stopping\n",
      "0.817\n",
      "0.83525\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_5 (TensorFlow [(None, 60, 3), (Non 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_5[0][2]        \n",
      "                                                                 tf_op_layer_split_5[0][3]        \n",
      "                                                                 tf_op_layer_split_5[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_5[0][0]        \n",
      "                                                                 tf_op_layer_split_5[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_5[0][1]        \n",
      "                                                                 tf_op_layer_split_5[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_90 (LSTM)                  (None, 60, 32)       5376        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_93 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_96 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_5[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_99 (LSTM)                  (None, 60, 16)       1280        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_102 (LSTM)                 (None, 60, 32)       4736        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_105 (LSTM)                 (None, 60, 32)       4736        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, 60, 32)       64          lstm_90[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_93 (LayerNo (None, 60, 16)       32          lstm_93[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_96 (LayerNo (None, 60, 16)       32          lstm_96[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_99 (LayerNo (None, 60, 16)       32          lstm_99[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_102 (LayerN (None, 60, 32)       64          lstm_102[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_105 (LayerN (None, 60, 32)       64          lstm_105[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 60, 32)       0           layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 60, 16)       0           layer_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 60, 16)       0           layer_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 60, 16)       0           layer_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 60, 32)       0           layer_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 60, 32)       0           layer_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_91 (LSTM)                  (None, 60, 32)       8320        dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_94 (LSTM)                  (None, 60, 16)       2112        dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_97 (LSTM)                  (None, 60, 16)       2112        dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_100 (LSTM)                 (None, 60, 16)       2112        dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_103 (LSTM)                 (None, 60, 32)       8320        dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_106 (LSTM)                 (None, 60, 32)       8320        dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_91 (LayerNo (None, 60, 32)       64          lstm_91[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_94 (LayerNo (None, 60, 16)       32          lstm_94[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_97 (LayerNo (None, 60, 16)       32          lstm_97[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_100 (LayerN (None, 60, 16)       32          lstm_100[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_103 (LayerN (None, 60, 32)       64          lstm_103[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_106 (LayerN (None, 60, 32)       64          lstm_106[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 60, 32)       0           layer_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 60, 16)       0           layer_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 60, 16)       0           layer_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 60, 16)       0           layer_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 60, 32)       0           layer_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 60, 32)       0           layer_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_92 (LSTM)                  (None, 60, 32)       8320        dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_95 (LSTM)                  (None, 60, 16)       2112        dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_98 (LSTM)                  (None, 60, 16)       2112        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_101 (LSTM)                 (None, 60, 16)       2112        dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_104 (LSTM)                 (None, 60, 32)       8320        dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_107 (LSTM)                 (None, 60, 32)       8320        dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_30 (Attention)        (None, 60, 32)       0           lstm_92[0][0]                    \n",
      "                                                                 lstm_92[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_31 (Attention)        (None, 60, 16)       0           lstm_95[0][0]                    \n",
      "                                                                 lstm_95[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_32 (Attention)        (None, 60, 16)       0           lstm_98[0][0]                    \n",
      "                                                                 lstm_98[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_33 (Attention)        (None, 60, 16)       0           lstm_101[0][0]                   \n",
      "                                                                 lstm_101[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_34 (Attention)        (None, 60, 32)       0           lstm_104[0][0]                   \n",
      "                                                                 lstm_104[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_35 (Attention)        (None, 60, 32)       0           lstm_107[0][0]                   \n",
      "                                                                 lstm_107[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_92 (LayerNo (None, 60, 32)       64          attention_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, 60, 16)       32          attention_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_98 (LayerNo (None, 60, 16)       32          attention_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_101 (LayerN (None, 60, 16)       32          attention_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_104 (LayerN (None, 60, 32)       64          attention_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_107 (LayerN (None, 60, 32)       64          attention_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 60, 32)       0           layer_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 60, 16)       0           layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 60, 16)       0           layer_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 60, 16)       0           layer_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 60, 32)       0           layer_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 60, 32)       0           layer_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 32)           0           dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 16)           0           dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 16)           0           dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 16)           0           dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 32)           0           dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 32)           0           dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 144)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "                                                                 global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 144)          576         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          37120       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256)          1024        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           16448       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           4160        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n",
      "12000/12000 [==============================] - 39s 3ms/sample - loss: 1.8239 - acc: 0.4794 - val_loss: 1.6544 - val_acc: 0.5547\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 1.3571 - acc: 0.6148 - val_loss: 1.4091 - val_acc: 0.6160\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 1.2168 - acc: 0.6642 - val_loss: 1.2483 - val_acc: 0.6747\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 1.1436 - acc: 0.6934 - val_loss: 1.2052 - val_acc: 0.6877\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.0910 - acc: 0.7105 - val_loss: 1.1245 - val_acc: 0.7027\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 1.0425 - acc: 0.7352 - val_loss: 1.0934 - val_acc: 0.7113\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 1.0030 - acc: 0.7507 - val_loss: 1.0884 - val_acc: 0.7170\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.9679 - acc: 0.7621 - val_loss: 1.1049 - val_acc: 0.7130\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.9410 - acc: 0.7707 - val_loss: 1.0599 - val_acc: 0.7350\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.9035 - acc: 0.7867 - val_loss: 1.0298 - val_acc: 0.7450\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.8894 - acc: 0.7915 - val_loss: 1.0422 - val_acc: 0.7413\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.8672 - acc: 0.7993 - val_loss: 1.0393 - val_acc: 0.7440\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.8469 - acc: 0.8062 - val_loss: 1.0101 - val_acc: 0.7573\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.8276 - acc: 0.8173 - val_loss: 1.0213 - val_acc: 0.7563\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.8174 - acc: 0.8202 - val_loss: 1.0096 - val_acc: 0.7537\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.7948 - acc: 0.8287 - val_loss: 1.0037 - val_acc: 0.7560\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.7859 - acc: 0.8361 - val_loss: 1.0388 - val_acc: 0.7477\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.7666 - acc: 0.8403 - val_loss: 1.0028 - val_acc: 0.7613\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.7438 - acc: 0.8508 - val_loss: 1.0084 - val_acc: 0.7657\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.7364 - acc: 0.8585 - val_loss: 1.0026 - val_acc: 0.7667\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.7237 - acc: 0.8637 - val_loss: 0.9883 - val_acc: 0.7710\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7108 - acc: 0.8674 - val_loss: 0.9959 - val_acc: 0.7737\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.7028 - acc: 0.8700 - val_loss: 1.0158 - val_acc: 0.7593\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.6905 - acc: 0.8766 - val_loss: 1.0000 - val_acc: 0.7753\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.6812 - acc: 0.8814 - val_loss: 0.9949 - val_acc: 0.7780\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.6652 - acc: 0.8882 - val_loss: 0.9909 - val_acc: 0.7747\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6560 - acc: 0.8963 - val_loss: 1.0214 - val_acc: 0.7733\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.6512 - acc: 0.8938 - val_loss: 0.9770 - val_acc: 0.7867\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.6387 - acc: 0.8988 - val_loss: 1.0097 - val_acc: 0.7763\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.6393 - acc: 0.9009 - val_loss: 1.0066 - val_acc: 0.7743\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.6358 - acc: 0.9018 - val_loss: 1.0023 - val_acc: 0.7797\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.6255 - acc: 0.9080 - val_loss: 0.9684 - val_acc: 0.7893\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6115 - acc: 0.9133 - val_loss: 0.9937 - val_acc: 0.7767\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5988 - acc: 0.9162 - val_loss: 0.9904 - val_acc: 0.7890\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.5991 - acc: 0.9205 - val_loss: 1.0174 - val_acc: 0.7797\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5920 - acc: 0.9208 - val_loss: 1.0314 - val_acc: 0.7737\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.5830 - acc: 0.9272 - val_loss: 0.9912 - val_acc: 0.7910\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5722 - acc: 0.9301 - val_loss: 1.0053 - val_acc: 0.7793\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5726 - acc: 0.9308 - val_loss: 1.0008 - val_acc: 0.7863\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5593 - acc: 0.9352 - val_loss: 1.0094 - val_acc: 0.7810\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5607 - acc: 0.9353 - val_loss: 1.0000 - val_acc: 0.7803\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5527 - acc: 0.9394 - val_loss: 0.9866 - val_acc: 0.7890\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5475 - acc: 0.9396 - val_loss: 1.0086 - val_acc: 0.7890\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.5559 - acc: 0.9365 - val_loss: 0.9972 - val_acc: 0.7893\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5417 - acc: 0.9436 - val_loss: 0.9979 - val_acc: 0.7893\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.5467 - acc: 0.9405 - val_loss: 1.0151 - val_acc: 0.7853\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5358 - acc: 0.9476 - val_loss: 0.9883 - val_acc: 0.7883\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5268 - acc: 0.9494 - val_loss: 0.9893 - val_acc: 0.7893\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5292 - acc: 0.9498 - val_loss: 1.0137 - val_acc: 0.7890\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.5227 - acc: 0.9523 - val_loss: 1.0133 - val_acc: 0.7933\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.5123 - acc: 0.9568 - val_loss: 1.0027 - val_acc: 0.7940\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5121 - acc: 0.9575 - val_loss: 0.9924 - val_acc: 0.7963\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5114 - acc: 0.9563 - val_loss: 1.0041 - val_acc: 0.7913\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.5061 - acc: 0.9597 - val_loss: 1.0083 - val_acc: 0.7843\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.5123 - acc: 0.9566 - val_loss: 1.0037 - val_acc: 0.7863\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5058 - acc: 0.9593 - val_loss: 0.9897 - val_acc: 0.7967\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5009 - acc: 0.9607 - val_loss: 1.0003 - val_acc: 0.7943\n",
      "Epoch 58/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4985 - acc: 0.9618 - val_loss: 1.0132 - val_acc: 0.7937\n",
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4955 - acc: 0.9627 - val_loss: 1.0141 - val_acc: 0.7890\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4923 - acc: 0.9646 - val_loss: 1.0225 - val_acc: 0.7843\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.4948 - acc: 0.9644 - val_loss: 1.0036 - val_acc: 0.7890\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4965 - acc: 0.9636 - val_loss: 0.9883 - val_acc: 0.7960\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.4801 - acc: 0.9702 - val_loss: 1.0044 - val_acc: 0.7923\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.4790 - acc: 0.9690 - val_loss: 1.0240 - val_acc: 0.7887\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4786 - acc: 0.9708 - val_loss: 1.0080 - val_acc: 0.7977\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.4752 - acc: 0.9692 - val_loss: 0.9996 - val_acc: 0.8040\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.4807 - acc: 0.9682 - val_loss: 1.0031 - val_acc: 0.8020\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.4778 - acc: 0.9714 - val_loss: 1.0127 - val_acc: 0.7903\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4774 - acc: 0.9719 - val_loss: 1.0059 - val_acc: 0.7983\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4724 - acc: 0.9731 - val_loss: 0.9977 - val_acc: 0.7980\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.4690 - acc: 0.9726 - val_loss: 1.0191 - val_acc: 0.7950\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4613 - acc: 0.9778 - val_loss: 1.0096 - val_acc: 0.7970\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4566 - acc: 0.9791 - val_loss: 1.0092 - val_acc: 0.7983\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.4583 - acc: 0.9786 - val_loss: 1.0303 - val_acc: 0.7890\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.4633 - acc: 0.9747 - val_loss: 1.0116 - val_acc: 0.7990\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4618 - acc: 0.9766 - val_loss: 1.0441 - val_acc: 0.7880\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4626 - acc: 0.9758 - val_loss: 1.0208 - val_acc: 0.7947\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.4565 - acc: 0.9787 - val_loss: 1.0203 - val_acc: 0.7957\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4527 - acc: 0.9775 - val_loss: 1.0167 - val_acc: 0.7940\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4533 - acc: 0.9785 - val_loss: 1.0272 - val_acc: 0.7927\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.4528 - acc: 0.9807 - val_loss: 1.0205 - val_acc: 0.7963\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4530 - acc: 0.9793 - val_loss: 1.0229 - val_acc: 0.7877\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4493 - acc: 0.9803 - val_loss: 1.0165 - val_acc: 0.7963\n",
      "Epoch 84/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.9825\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4448 - acc: 0.9822 - val_loss: 1.0278 - val_acc: 0.7940\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.4285 - acc: 0.9880 - val_loss: 0.9910 - val_acc: 0.8040\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4202 - acc: 0.9918 - val_loss: 0.9856 - val_acc: 0.8020\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4223 - acc: 0.9897 - val_loss: 0.9966 - val_acc: 0.8023\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.4198 - acc: 0.9907 - val_loss: 0.9925 - val_acc: 0.8013\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4179 - acc: 0.9915 - val_loss: 0.9962 - val_acc: 0.8020\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4182 - acc: 0.9915 - val_loss: 0.9951 - val_acc: 0.8030\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.4168 - acc: 0.9912 - val_loss: 0.9947 - val_acc: 0.8017\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.4167 - acc: 0.9918 - val_loss: 0.9934 - val_acc: 0.8030\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.4144 - acc: 0.9923 - val_loss: 0.9975 - val_acc: 0.8043\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.4159 - acc: 0.9919 - val_loss: 0.9912 - val_acc: 0.8050\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.4153 - acc: 0.9917 - val_loss: 0.9914 - val_acc: 0.8070\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4129 - acc: 0.9933 - val_loss: 0.9930 - val_acc: 0.8027\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4156 - acc: 0.9918 - val_loss: 1.0110 - val_acc: 0.7977\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4149 - acc: 0.9924 - val_loss: 0.9949 - val_acc: 0.8027\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.4153 - acc: 0.9906 - val_loss: 1.0026 - val_acc: 0.8010\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4125 - acc: 0.9931 - val_loss: 1.0023 - val_acc: 0.8020\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4141 - acc: 0.9922 - val_loss: 1.0032 - val_acc: 0.7967\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.4120 - acc: 0.9923 - val_loss: 1.0009 - val_acc: 0.8007\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4142 - acc: 0.9921 - val_loss: 0.9996 - val_acc: 0.8023\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.4124 - acc: 0.9933 - val_loss: 0.9941 - val_acc: 0.8057\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4099 - acc: 0.9933 - val_loss: 0.9990 - val_acc: 0.7990\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.4094 - acc: 0.9944 - val_loss: 1.0065 - val_acc: 0.8040\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4115 - acc: 0.9923 - val_loss: 1.0015 - val_acc: 0.7983\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.4116 - acc: 0.9923 - val_loss: 1.0037 - val_acc: 0.8023\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.4095 - acc: 0.9935 - val_loss: 1.0054 - val_acc: 0.7997\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4105 - acc: 0.9930 - val_loss: 1.0120 - val_acc: 0.7983\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4110 - acc: 0.9935 - val_loss: 1.0009 - val_acc: 0.8017\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4103 - acc: 0.9923 - val_loss: 1.0025 - val_acc: 0.8027\n",
      "Epoch 113/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.9934\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.4090 - acc: 0.9933 - val_loss: 1.0054 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4016 - acc: 0.9957 - val_loss: 0.9947 - val_acc: 0.8060\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3997 - acc: 0.9964 - val_loss: 0.9932 - val_acc: 0.8033\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.3985 - acc: 0.9966 - val_loss: 0.9950 - val_acc: 0.8060\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.3979 - acc: 0.9967 - val_loss: 0.9952 - val_acc: 0.8070\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3967 - acc: 0.9966 - val_loss: 0.9978 - val_acc: 0.8053\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.3967 - acc: 0.9972 - val_loss: 0.9916 - val_acc: 0.8063\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.3966 - acc: 0.9970 - val_loss: 0.9940 - val_acc: 0.8040\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3967 - acc: 0.9970 - val_loss: 0.9944 - val_acc: 0.8047\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3975 - acc: 0.9962 - val_loss: 0.9927 - val_acc: 0.8013\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.3962 - acc: 0.9976 - val_loss: 0.9950 - val_acc: 0.8030\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3958 - acc: 0.9970 - val_loss: 0.9924 - val_acc: 0.8040\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3960 - acc: 0.9968 - val_loss: 0.9965 - val_acc: 0.8053\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.3950 - acc: 0.9972 - val_loss: 0.9980 - val_acc: 0.8083\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.3963 - acc: 0.9970 - val_loss: 0.9979 - val_acc: 0.8017\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3965 - acc: 0.9968 - val_loss: 0.9948 - val_acc: 0.8033\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3959 - acc: 0.9966 - val_loss: 0.9957 - val_acc: 0.8043\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.3948 - acc: 0.9967 - val_loss: 0.9964 - val_acc: 0.8053\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3949 - acc: 0.9970 - val_loss: 0.9969 - val_acc: 0.8057\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3948 - acc: 0.9977 - val_loss: 0.9907 - val_acc: 0.8073\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.3934 - acc: 0.9977 - val_loss: 1.0027 - val_acc: 0.8040\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.3949 - acc: 0.9970 - val_loss: 1.0006 - val_acc: 0.8087\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3941 - acc: 0.9973 - val_loss: 0.9941 - val_acc: 0.8053\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.3951 - acc: 0.9966 - val_loss: 0.9990 - val_acc: 0.8063\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3939 - acc: 0.9978 - val_loss: 0.9964 - val_acc: 0.8100\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3954 - acc: 0.9975 - val_loss: 0.9991 - val_acc: 0.8063\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3936 - acc: 0.9968 - val_loss: 0.9967 - val_acc: 0.8077\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.3957 - acc: 0.9969 - val_loss: 0.9947 - val_acc: 0.8060\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3949 - acc: 0.9964 - val_loss: 0.9924 - val_acc: 0.8043\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3924 - acc: 0.9967 - val_loss: 0.9884 - val_acc: 0.8097\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.3931 - acc: 0.9975 - val_loss: 0.9973 - val_acc: 0.8053\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.3925 - acc: 0.9980 - val_loss: 1.0012 - val_acc: 0.8087\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3920 - acc: 0.9982 - val_loss: 0.9967 - val_acc: 0.8083\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3932 - acc: 0.9977 - val_loss: 0.9964 - val_acc: 0.8073\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.3914 - acc: 0.9976 - val_loss: 0.9988 - val_acc: 0.8047\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.3919 - acc: 0.9980 - val_loss: 0.9969 - val_acc: 0.8057\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3933 - acc: 0.9968 - val_loss: 0.9986 - val_acc: 0.8063\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3925 - acc: 0.9969 - val_loss: 0.9981 - val_acc: 0.8070\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.3924 - acc: 0.9975 - val_loss: 1.0010 - val_acc: 0.8080\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3933 - acc: 0.9970 - val_loss: 1.0024 - val_acc: 0.8030\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3924 - acc: 0.9973 - val_loss: 1.0051 - val_acc: 0.8033\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.3919 - acc: 0.9975 - val_loss: 0.9907 - val_acc: 0.8113\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.3905 - acc: 0.9987 - val_loss: 1.0008 - val_acc: 0.8030\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.3917 - acc: 0.9977 - val_loss: 0.9981 - val_acc: 0.8050\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.3926 - acc: 0.9964 - val_loss: 1.0019 - val_acc: 0.8037\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.3914 - acc: 0.9977 - val_loss: 0.9999 - val_acc: 0.8037\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.3909 - acc: 0.9969 - val_loss: 0.9942 - val_acc: 0.8033\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.3925 - acc: 0.9971 - val_loss: 0.9990 - val_acc: 0.8077\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.3906 - acc: 0.9975 - val_loss: 1.0025 - val_acc: 0.8050\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.3909 - acc: 0.9976 - val_loss: 1.0025 - val_acc: 0.8033\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.3932 - acc: 0.9978 - val_loss: 0.9975 - val_acc: 0.8043\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3907 - acc: 0.9977 - val_loss: 1.0010 - val_acc: 0.8030\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 4s 368us/sample - loss: 0.3895 - acc: 0.9983 - val_loss: 0.9983 - val_acc: 0.8067\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3909 - acc: 0.9977 - val_loss: 0.9974 - val_acc: 0.8083\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.3908 - acc: 0.9974 - val_loss: 0.9969 - val_acc: 0.8070\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.3911 - acc: 0.9976 - val_loss: 0.9950 - val_acc: 0.8027\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.3899 - acc: 0.9975 - val_loss: 0.9999 - val_acc: 0.8033\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3902 - acc: 0.9977 - val_loss: 0.9982 - val_acc: 0.8073\n",
      "Epoch 171/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.3889 - acc: 0.9977 - val_loss: 1.0010 - val_acc: 0.8060\n",
      "Epoch 172/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.9973\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3905 - acc: 0.9973 - val_loss: 0.9954 - val_acc: 0.8077\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.3866 - acc: 0.9980 - val_loss: 0.9921 - val_acc: 0.8100\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.3843 - acc: 0.9992 - val_loss: 0.9872 - val_acc: 0.8127\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.3853 - acc: 0.9983 - val_loss: 0.9870 - val_acc: 0.8080\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3843 - acc: 0.9986 - val_loss: 0.9893 - val_acc: 0.8080\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.3853 - acc: 0.9980 - val_loss: 0.9852 - val_acc: 0.8120\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.3838 - acc: 0.9983 - val_loss: 0.9880 - val_acc: 0.8087\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.3844 - acc: 0.9985 - val_loss: 0.9905 - val_acc: 0.8117\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.3852 - acc: 0.9986 - val_loss: 0.9896 - val_acc: 0.8107\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.3836 - acc: 0.9987 - val_loss: 0.9866 - val_acc: 0.8113\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3842 - acc: 0.9982 - val_loss: 0.9859 - val_acc: 0.8110\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.3844 - acc: 0.9986 - val_loss: 0.9897 - val_acc: 0.8097\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.3849 - acc: 0.9987 - val_loss: 0.9927 - val_acc: 0.8063\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.3847 - acc: 0.9988 - val_loss: 0.9886 - val_acc: 0.8097\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3836 - acc: 0.9988 - val_loss: 0.9959 - val_acc: 0.8080\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.3832 - acc: 0.9990 - val_loss: 0.9900 - val_acc: 0.8103\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.3848 - acc: 0.9985 - val_loss: 0.9924 - val_acc: 0.8087\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.3841 - acc: 0.9985 - val_loss: 0.9912 - val_acc: 0.8120\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.3836 - acc: 0.9987 - val_loss: 0.9903 - val_acc: 0.8130\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3827 - acc: 0.9991 - val_loss: 0.9906 - val_acc: 0.8090\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3847 - acc: 0.9984 - val_loss: 0.9888 - val_acc: 0.8117\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.3834 - acc: 0.9990 - val_loss: 0.9912 - val_acc: 0.8097\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.3832 - acc: 0.9988 - val_loss: 0.9930 - val_acc: 0.8127\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.3819 - acc: 0.9990 - val_loss: 0.9893 - val_acc: 0.8120\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.3829 - acc: 0.9988 - val_loss: 0.9908 - val_acc: 0.8093\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.3836 - acc: 0.9987 - val_loss: 0.9880 - val_acc: 0.8140\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.3820 - acc: 0.9994 - val_loss: 0.9888 - val_acc: 0.8087\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3822 - acc: 0.9991 - val_loss: 0.9906 - val_acc: 0.8067\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.3825 - acc: 0.9987 - val_loss: 0.9882 - val_acc: 0.8087\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.3841 - acc: 0.9987 - val_loss: 0.9874 - val_acc: 0.8083\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.3825 - acc: 0.9989 - val_loss: 0.9901 - val_acc: 0.8090\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3824 - acc: 0.9992 - val_loss: 0.9865 - val_acc: 0.8077\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.3822 - acc: 0.9987 - val_loss: 0.9877 - val_acc: 0.8103\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.3832 - acc: 0.9987 - val_loss: 0.9935 - val_acc: 0.8077\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.3836 - acc: 0.9982 - val_loss: 0.9931 - val_acc: 0.8077\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.3837 - acc: 0.9987 - val_loss: 0.9915 - val_acc: 0.8097\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.3836 - acc: 0.9984 - val_loss: 0.9925 - val_acc: 0.8050\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.3833 - acc: 0.9986 - val_loss: 0.9900 - val_acc: 0.8077\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.3823 - acc: 0.9988 - val_loss: 0.9821 - val_acc: 0.8113\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.3822 - acc: 0.9986 - val_loss: 0.9917 - val_acc: 0.8117\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.3821 - acc: 0.9985 - val_loss: 0.9933 - val_acc: 0.8060\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.3823 - acc: 0.9983 - val_loss: 0.9931 - val_acc: 0.8110\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.3813 - acc: 0.9987 - val_loss: 0.9916 - val_acc: 0.8073\n",
      "Epoch 215/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.9990\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.3820 - acc: 0.9988 - val_loss: 0.9916 - val_acc: 0.8087\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.3811 - acc: 0.9989 - val_loss: 0.9913 - val_acc: 0.8110\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.3805 - acc: 0.9987 - val_loss: 0.9908 - val_acc: 0.8110\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.3805 - acc: 0.9990 - val_loss: 0.9890 - val_acc: 0.8107\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.3801 - acc: 0.9991 - val_loss: 0.9877 - val_acc: 0.8123\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.3802 - acc: 0.9992 - val_loss: 0.9870 - val_acc: 0.8093\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.3806 - acc: 0.9990 - val_loss: 0.9864 - val_acc: 0.8077\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3802 - acc: 0.9988 - val_loss: 0.9877 - val_acc: 0.8113\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3803 - acc: 0.9987 - val_loss: 0.9892 - val_acc: 0.8090\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.3800 - acc: 0.9991 - val_loss: 0.9872 - val_acc: 0.8107\n",
      "Epoch 225/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.3797 - acc: 0.9992 - val_loss: 0.9848 - val_acc: 0.8127\n",
      "Epoch 226/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.3804 - acc: 0.9989 - val_loss: 0.9848 - val_acc: 0.8097\n",
      "Epoch 227/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.3798 - acc: 0.9992 - val_loss: 0.9857 - val_acc: 0.8117\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.3796 - acc: 0.9993 - val_loss: 0.9872 - val_acc: 0.8100\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.3789 - acc: 0.9995 - val_loss: 0.9879 - val_acc: 0.8097\n",
      "Epoch 230/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.3795 - acc: 0.9990 - val_loss: 0.9845 - val_acc: 0.8097\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.3802 - acc: 0.9992 - val_loss: 0.9847 - val_acc: 0.8103\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.3806 - acc: 0.9990 - val_loss: 0.9845 - val_acc: 0.8117\n",
      "Epoch 233/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.9992\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.3800 - acc: 0.9992 - val_loss: 0.9883 - val_acc: 0.8110\n",
      "Epoch 234/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.3787 - acc: 0.9992 - val_loss: 0.9857 - val_acc: 0.8100\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3789 - acc: 0.9992 - val_loss: 0.9859 - val_acc: 0.8110\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.3784 - acc: 0.9991 - val_loss: 0.9863 - val_acc: 0.8107\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.3779 - acc: 0.9997 - val_loss: 0.9857 - val_acc: 0.8110\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.3788 - acc: 0.9989 - val_loss: 0.9853 - val_acc: 0.8120\n",
      "Epoch 239/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.3790 - acc: 0.9991 - val_loss: 0.9859 - val_acc: 0.8127\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.3784 - acc: 0.9992 - val_loss: 0.9864 - val_acc: 0.8077\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.3782 - acc: 0.9992 - val_loss: 0.9873 - val_acc: 0.8103\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.3783 - acc: 0.9992 - val_loss: 0.9852 - val_acc: 0.8097\n",
      "Epoch 243/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.3784 - acc: 0.9990 - val_loss: 0.9863 - val_acc: 0.8110\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.3772 - acc: 0.9995 - val_loss: 0.9849 - val_acc: 0.8093\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.3788 - acc: 0.9994 - val_loss: 0.9857 - val_acc: 0.8107\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.3789 - acc: 0.9986 - val_loss: 0.9856 - val_acc: 0.8107\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.3789 - acc: 0.9992 - val_loss: 0.9851 - val_acc: 0.8100\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.3783 - acc: 0.9994 - val_loss: 0.9854 - val_acc: 0.8090\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.3793 - acc: 0.9994 - val_loss: 0.9872 - val_acc: 0.8107\n",
      "Epoch 250/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3787 - acc: 0.9994 - val_loss: 0.9875 - val_acc: 0.8110\n",
      "Epoch 251/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.9990\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.3781 - acc: 0.9990 - val_loss: 0.9877 - val_acc: 0.8100\n",
      "Epoch 252/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.3784 - acc: 0.9991 - val_loss: 0.9868 - val_acc: 0.8117\n",
      "Epoch 253/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.3786 - acc: 0.9994 - val_loss: 0.9868 - val_acc: 0.8100\n",
      "Epoch 254/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.3783 - acc: 0.9988 - val_loss: 0.9873 - val_acc: 0.8097\n",
      "Epoch 255/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.3781 - acc: 0.9995 - val_loss: 0.9873 - val_acc: 0.8103\n",
      "Epoch 256/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.3777 - acc: 0.9996 - val_loss: 0.9873 - val_acc: 0.8107\n",
      "Epoch 257/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.3776 - acc: 0.9994 - val_loss: 0.9882 - val_acc: 0.8117\n",
      "Epoch 00257: early stopping\n",
      "0.814\n",
      "0.83263\n"
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "proba_oof = np.zeros((15000, 20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "\n",
    "mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "\n",
    "# # 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# # 每一个大类输出 \n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "seeds = [42,39,17][:1]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "        print(train.shape)\n",
    "        mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "        new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "        model = LSTM_Model()\n",
    "        model.summary()\n",
    "#         optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "#         lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "#         wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "        model.compile(loss=custom_loss,\n",
    "#                       ,loss_weights=[3,7,21],\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.5,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'LSTM{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx][:,:,:,0], y_binary[xx],\n",
    "                  epochs=400,\n",
    "                  batch_size=256,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(train[yy][:,:,:,0], y_binary[yy]),\n",
    "                  callbacks=[plateau, early_stopping, checkpoint],\n",
    "#                              class_weight=[classweights1,classweights2,classweights3]\n",
    "                             )\n",
    "        history.append(train_res)\n",
    "        \n",
    "        model.load_weights(f'LSTM{fold}.h5')\n",
    "        proba_t[:,:20] += model.predict(test[:,:,:,0], verbose=0, batch_size=1024) / fold_num /len(seeds)\n",
    "        proba_oof[yy,:20] += model.predict(train[yy][:,:,:,0],verbose=0,batch_size=1024)/len(seeds)\n",
    "\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy,:20], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.81, 0.812, 0.802, 0.817, 0.814],\n",
       " 0.8110000000000002,\n",
       " [0.8281428571428581,\n",
       "  0.8324285714285731,\n",
       "  0.8220000000000006,\n",
       "  0.8352539682539688,\n",
       "  0.8326349206349217],\n",
       " 0.8300920634920643)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_score,np.mean(oof_score),oof_comm,np.mean(oof_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:26:28.195445Z",
     "start_time": "2020-08-21T09:26:28.099862Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8281428571428581 0.81\n",
      "1 0.8324285714285731 0.812\n",
      "2 0.8220000000000006 0.802\n",
      "3 0.8352539682539688 0.817\n",
      "4 0.8326349206349217 0.814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LSTM0.83009_dict.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "joblib.dump(oof_dict,\"LSTM%.5f_dict.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:27:59.699599Z",
     "start_time": "2020-08-21T09:27:58.538866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81093\n",
      "0.83009\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   fragment_id  16000 non-null  int64\n",
      " 1   behavior_id  16000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 250.1 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t[:,:20], axis=1)\n",
    "oof_y = np.argmax(proba_oof[:,:20], axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../../data/final_data/'\n",
    "sub = pd.read_csv(data_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "sub.to_csv('LSTM%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_test_final.csv  sensor_train_final.csv  submit_example.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
