{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:09.673656Z",
     "start_time": "2020-08-21T10:09:09.596593Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:10.336182Z",
     "start_time": "2020-08-21T10:09:10.245109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "# def acc_combo(y, y_pred):\n",
    "#     # 数值ID与行为编码的对应关系\n",
    "#     mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "#         4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "#         8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "#         12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "#         16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "#     # 将行为ID转为编码\n",
    "#     code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "#     if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "#         return 1.0\n",
    "#     elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "#         return 1.0/7\n",
    "#     elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "#         return 1.0/3\n",
    "#     else:\n",
    "#         return 0.0\n",
    "\n",
    "\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:10.534005Z",
     "start_time": "2020-08-21T10:09:10.339396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_test_final.csv  sensor_train_final.csv  submit_example.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:15:50.466408Z",
     "start_time": "2020-08-21T10:15:49.737809Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../../data/final_data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "# data_test['fragment_id'] += 100000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:15:50.793007Z",
     "start_time": "2020-08-21T10:15:50.469259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'xy', 'accg', 'xy_g',\n",
      "       'thetax', 'thetay', 'thetaz', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'xy', 'accg', 'xy_g', 'thetax', 'thetay',\n",
      "       'thetaz', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "    \n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "    df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5 \n",
    "    \n",
    "    \n",
    "    df['thetax']=np.arctan(df.acc_xg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetay']=np.arctan(df.acc_yg/\n",
    "                           np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetaz']=np.arctan(df.acc_zg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:52:20.806602Z",
     "start_time": "2020-08-21T10:52:20.227674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragment_id</th>\n",
       "      <th>time_point</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>acc_xg</th>\n",
       "      <th>acc_yg</th>\n",
       "      <th>acc_zg</th>\n",
       "      <th>behavior_id</th>\n",
       "      <th>acc</th>\n",
       "      <th>xy</th>\n",
       "      <th>accg</th>\n",
       "      <th>xy_g</th>\n",
       "      <th>thetax</th>\n",
       "      <th>thetay</th>\n",
       "      <th>thetaz</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "      <td>391794.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7476.755147</td>\n",
       "      <td>1379.682838</td>\n",
       "      <td>-0.009284</td>\n",
       "      <td>-0.171336</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>0.684638</td>\n",
       "      <td>3.413824</td>\n",
       "      <td>7.014268</td>\n",
       "      <td>9.875366</td>\n",
       "      <td>0.745027</td>\n",
       "      <td>0.476640</td>\n",
       "      <td>9.780786</td>\n",
       "      <td>4.806181</td>\n",
       "      <td>4.556521</td>\n",
       "      <td>23.021800</td>\n",
       "      <td>52.636211</td>\n",
       "      <td>9.773486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4330.938746</td>\n",
       "      <td>1049.440947</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.683727</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>2.820369</td>\n",
       "      <td>3.135597</td>\n",
       "      <td>4.136285</td>\n",
       "      <td>5.879332</td>\n",
       "      <td>1.007100</td>\n",
       "      <td>0.664953</td>\n",
       "      <td>0.744531</td>\n",
       "      <td>2.609592</td>\n",
       "      <td>18.801977</td>\n",
       "      <td>23.130714</td>\n",
       "      <td>31.541102</td>\n",
       "      <td>0.286256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.120000</td>\n",
       "      <td>-8.040000</td>\n",
       "      <td>-23.160000</td>\n",
       "      <td>-11.630000</td>\n",
       "      <td>-9.250000</td>\n",
       "      <td>-13.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.772597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-89.486065</td>\n",
       "      <td>-64.964977</td>\n",
       "      <td>-82.734104</td>\n",
       "      <td>2.204745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3693.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.670000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>6.460000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.560073</td>\n",
       "      <td>2.476954</td>\n",
       "      <td>-3.886956</td>\n",
       "      <td>3.569773</td>\n",
       "      <td>41.918009</td>\n",
       "      <td>9.667559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7482.000000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>8.390000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.343657</td>\n",
       "      <td>0.214709</td>\n",
       "      <td>9.749872</td>\n",
       "      <td>4.773395</td>\n",
       "      <td>0.175579</td>\n",
       "      <td>17.495758</td>\n",
       "      <td>59.791002</td>\n",
       "      <td>9.774428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11228.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>6.130000</td>\n",
       "      <td>9.420000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.868389</td>\n",
       "      <td>0.582580</td>\n",
       "      <td>9.943449</td>\n",
       "      <td>6.953799</td>\n",
       "      <td>7.192069</td>\n",
       "      <td>39.926225</td>\n",
       "      <td>75.273684</td>\n",
       "      <td>9.854207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14999.000000</td>\n",
       "      <td>4999.000000</td>\n",
       "      <td>7.610000</td>\n",
       "      <td>8.580000</td>\n",
       "      <td>11.180000</td>\n",
       "      <td>18.540000</td>\n",
       "      <td>13.060000</td>\n",
       "      <td>48.570000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.263863</td>\n",
       "      <td>16.149158</td>\n",
       "      <td>48.572835</td>\n",
       "      <td>18.865622</td>\n",
       "      <td>89.821879</td>\n",
       "      <td>89.917486</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>60.917291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fragment_id     time_point          acc_x          acc_y  \\\n",
       "count  391794.000000  391794.000000  391794.000000  391794.000000   \n",
       "mean     7476.755147    1379.682838      -0.009284      -0.171336   \n",
       "std      4330.938746    1049.440947       0.415241       0.683727   \n",
       "min         0.000000       0.000000     -16.120000      -8.040000   \n",
       "25%      3693.000000     552.000000      -0.100000      -0.190000   \n",
       "50%      7482.000000    1145.000000      -0.000000       0.000000   \n",
       "75%     11228.000000    1973.000000       0.100000       0.070000   \n",
       "max     14999.000000    4999.000000       7.610000       8.580000   \n",
       "\n",
       "               acc_z         acc_xg         acc_yg         acc_zg  \\\n",
       "count  391794.000000  391794.000000  391794.000000  391794.000000   \n",
       "mean        0.014685       0.684638       3.413824       7.014268   \n",
       "std         0.948553       2.820369       3.135597       4.136285   \n",
       "min       -23.160000     -11.630000      -9.250000     -13.330000   \n",
       "25%        -0.200000      -0.670000       0.610000       6.460000   \n",
       "50%         0.000000       0.030000       3.060000       8.390000   \n",
       "75%         0.190000       1.210000       6.130000       9.420000   \n",
       "max        11.180000      18.540000      13.060000      48.570000   \n",
       "\n",
       "         behavior_id            acc             xy           accg  \\\n",
       "count  391794.000000  391794.000000  391794.000000  391794.000000   \n",
       "mean        9.875366       0.745027       0.476640       9.780786   \n",
       "std         5.879332       1.007100       0.664953       0.744531   \n",
       "min         0.000000       0.000000       0.000000       1.772597   \n",
       "25%         5.000000       0.141421       0.100000       9.560073   \n",
       "50%        10.000000       0.343657       0.214709       9.749872   \n",
       "75%        15.000000       0.868389       0.582580       9.943449   \n",
       "max        19.000000      23.263863      16.149158      48.572835   \n",
       "\n",
       "                xy_g         thetax         thetay         thetaz  \\\n",
       "count  391794.000000  391794.000000  391794.000000  391794.000000   \n",
       "mean        4.806181       4.556521      23.021800      52.636211   \n",
       "std         2.609592      18.801977      23.130714      31.541102   \n",
       "min         0.000000     -89.486065     -64.964977     -82.734104   \n",
       "25%         2.476954      -3.886956       3.569773      41.918009   \n",
       "50%         4.773395       0.175579      17.495758      59.791002   \n",
       "75%         6.953799       7.192069      39.926225      75.273684   \n",
       "max        18.865622      89.821879      89.917486      90.000000   \n",
       "\n",
       "                   g  \n",
       "count  391794.000000  \n",
       "mean        9.773486  \n",
       "std         0.286256  \n",
       "min         2.204745  \n",
       "25%         9.667559  \n",
       "50%         9.774428  \n",
       "75%         9.854207  \n",
       "max        60.917291  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:12.936039Z",
     "start_time": "2020-08-21T10:51:09.661611Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "# group1 = [\"acc_x\",\"acc_y\",\"acc_z\",\"acc\",\"acc_xg\",\"acc_yg\",\"acc_zg\",\"accg\"]\n",
    "\n",
    "\n",
    "test['fragment_id'] += 15000\n",
    "data = pd.concat([train, test], sort=False)\n",
    "ss_tool = StandardScaler()\n",
    "data[group1] = ss_tool.fit_transform(data[group1])\n",
    "\n",
    "\n",
    "train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "test['fragment_id'] -= 15000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:13.570289Z",
     "start_time": "2020-08-21T10:51:13.509246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'acc_xg',\n",
       " 'acc_yg',\n",
       " 'acc_zg',\n",
       " 'acc',\n",
       " 'xy',\n",
       " 'accg',\n",
       " 'xy_g',\n",
       " 'thetax',\n",
       " 'thetay',\n",
       " 'thetaz',\n",
       " 'g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:51:15.510838Z",
     "start_time": "2020-08-21T10:51:15.461803Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=len(group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:15.390959Z",
     "start_time": "2020-08-21T10:09:15.328381Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))\n",
    "\n",
    "# from scipy.fftpack import fft\n",
    "# from scipy.signal import resample\n",
    "# def get_fft_values(y_values, N, f_s):\n",
    "#     f_values = np.linspace(0.0, f_s/2.0, N//2)\n",
    "#     fft_values_ = fft(y_values)\n",
    "#     plt.plot(fft_values_)\n",
    "#     plt.show()\n",
    "#     print(fft_values_.shape)\n",
    "#     fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "#     print(fft_values.shape)\n",
    "#     return f_values, fft_values\n",
    "\n",
    "# tmp = train[train.fragment_id == 0][:sample_num]\n",
    "\n",
    "# get_fft_values(tmp[\"acc\"].values,60,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:47<00:00, 317.19it/s]\n",
      "100%|██████████| 16000/16000 [00:49<00:00, 325.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def df2array(df,num,x):\n",
    "    for i in tqdm(range(num)):\n",
    "        tmp = df[df.fragment_id == i][:sample_num]\n",
    "        length=len(tmp)\n",
    "        if length<50:\n",
    "            new_tmp=tmp.copy()\n",
    "            while len(tmp)<60:\n",
    "                new_tmp['time_point']=tmp['time_point'].max()+new_tmp['time_point']\n",
    "                tmp=pd.concat([tmp,new_tmp],ignore_index=True)\n",
    "        x[i,:,:,0] = resample(tmp[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "    return x\n",
    "x=df2array(train,15000,x)\n",
    "t=df2array(test,16000,t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "CPU times: user 26.6 ms, sys: 2.02 ms, total: 28.6 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 一个完成了的generator\n",
    "def data_generator(data,label,class17label,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    class17label: series\n",
    "    \"\"\"\n",
    "    class17label=np.asarray(class17label)\n",
    "    length=len(data)\n",
    "    seq_length=len(data[0])\n",
    "    half_seq_length=int(seq_length/2)\n",
    "    \n",
    "    # index2label\n",
    "    index2label=dict(zip(range(length),class17label))\n",
    "    \n",
    "    label2index={}\n",
    "#     print(class17label)\n",
    "    for i in range(length):\n",
    "#         print(class17label[i],label2index.get(class17label[i],[]))\n",
    "        label2index[class17label[i]]=label2index.get(class17label[i],[])\n",
    "        label2index[class17label[i]].append(i)\n",
    "\n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        if random.choice([0,1,1,1]):\n",
    "            # minxup\n",
    "            #one specific index -> label -> all the index belong to this\n",
    "            choice_index=[random.choice(label2index[index2label[x]]) for x in inds]   # get the random choice seq(waiting for concat)\n",
    "            # 1st 前1/2 seq_length 点原始  后1/2 seq_length 点随机\n",
    "            res_x_orig=data[inds,:half_seq_length]   #原始\n",
    "            res_x=data[choice_index,half_seq_length:]   #需要加入的\n",
    "\n",
    "    #         print(inds)\n",
    "    #         print(data.shape,res_x_orig.shape,res_x.shape,np.concatenate((res_x_orig,res_x),axis=1).shape)\n",
    "#             yield np.concatenate((res_x_orig,res_x),axis=1),\\\n",
    "#                     [label[0][inds],label[1][inds],label[2][inds]]\n",
    "            yield np.concatenate((res_x_orig,res_x),axis=1),label[inds]\n",
    "        else:\n",
    "        \n",
    "            yield data[inds],label[inds]\n",
    "            \n",
    "    \n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x,y,y,32):\n",
    "    print(a.shape,b.shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 一个完成了的generator\n",
    "# def data_generator(data,label,class17label,batch_size):\n",
    "#     \"\"\"\n",
    "#     data:array  (7292, 60, 14, 1)\n",
    "#     label:array (7292,)\n",
    "#     class17label: series\n",
    "#     \"\"\"\n",
    "#     class17label=np.asarray(class17label)\n",
    "#     length=len(data)\n",
    "#     seq_length=len(data[0])\n",
    "#     half_seq_length=int(seq_length/2)\n",
    "    \n",
    "#     # index2label\n",
    "#     index2label=dict(zip(range(length),class17label))\n",
    "    \n",
    "#     label2index={}\n",
    "# #     print(class17label)\n",
    "#     for i in range(length):\n",
    "# #         print(class17label[i],label2index.get(class17label[i],[]))\n",
    "#         label2index[class17label[i]]=label2index.get(class17label[i],[])\n",
    "#         label2index[class17label[i]].append(i)\n",
    "\n",
    "#     count=0\n",
    "#     np.random.seed(seed)# 保证结果可重复\n",
    "    \n",
    "#     while True:\n",
    "        \n",
    "#         if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "#             count=0\n",
    "#             shuffle_index = list(range(length))\n",
    "#             np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "#         start = count * batch_size  ## batch的起始点\n",
    "#         end = (count + 1) * batch_size ## batch的终点\n",
    "#         inds=shuffle_index[start:end]\n",
    "\n",
    "#         count+=1\n",
    "        \n",
    "#         if random.choice([0,1,1,1]):\n",
    "#             # minxup\n",
    "#             #one specific index -> label -> all the index belong to this\n",
    "#             choice_index=[random.choice(label2index[index2label[x]]) for x in inds]   # get the random choice seq(waiting for concat)\n",
    "#             # 1st 前1/2 seq_length 点原始  后1/2 seq_length 点随机\n",
    "#             res_x_orig=data[inds,:half_seq_length]   #原始\n",
    "#             res_x=data[choice_index,half_seq_length:]   #需要加入的\n",
    "\n",
    "#     #         print(inds)\n",
    "#     #         print(data.shape,res_x_orig.shape,res_x.shape,np.concatenate((res_x_orig,res_x),axis=1).shape)\n",
    "#             yield np.concatenate((res_x_orig,res_x),axis=1),\\\n",
    "#                     [label[0][inds],label[1][inds],label[2][inds]]\n",
    "#         else:\n",
    "        \n",
    "#             yield data[inds],[label[0][inds],label[1][inds],label[2][inds]]\n",
    "            \n",
    "    \n",
    "\n",
    "# count=0\n",
    "# for a,b in data_generator(x,[y,y,y],y,32):\n",
    "#     print(a.shape,b[0].shape)\n",
    "#     count+=1\n",
    "#     if count==20:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:56:36.797851Z",
     "start_time": "2020-08-21T10:56:29.285067Z"
    },
    "code_folding": [
     21,
     36
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60, 14)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 60, 3), (Non 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 60, 32)       6016        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 60, 16)       1216        tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 60, 16)       1216        tf_op_layer_split[0][3]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split[0][4]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 60, 16)       1152        tf_op_layer_split[0][5]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 60, 32)       64          lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 60, 16)       32          lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 60, 16)       32          lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 60, 16)       32          lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 60, 16)       32          lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 60, 16)       32          lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 60, 16)       32          lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 60, 32)       0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60, 16)       0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60, 16)       0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 60, 16)       0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 60, 16)       0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 60, 16)       0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 60, 16)       0           layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 60, 32)       8320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 60, 16)       2112        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 60, 16)       2112        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 60, 16)       2112        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 60, 16)       2112        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 60, 16)       2112        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 60, 16)       2112        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 60, 32)       64          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 60, 16)       32          lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 60, 16)       32          lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 60, 16)       32          lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 60, 16)       32          lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 60, 16)       32          lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 60, 16)       32          lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60, 32)       0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 60, 16)       0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 60, 16)       0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 60, 16)       0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 60, 16)       0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 60, 16)       0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 60, 16)       0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 60, 32)       8320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 60, 16)       2112        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 60, 16)       2112        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 60, 16)       2112        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 60, 16)       2112        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 60, 16)       2112        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 60, 16)       2112        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 60, 32)       64          lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 60, 16)       32          lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 60, 16)       32          lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 60, 16)       32          lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 60, 16)       32          lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 60, 16)       32          lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 60, 16)       32          lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 60, 32)       0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 60, 16)       0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 60, 16)       0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 60, 16)       0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 60, 16)       0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 60, 16)       0           layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 60, 16)       0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 16)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 16)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 16)           0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 16)           0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 16)           0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 16)           0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128)          512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          33024       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,660\n",
      "Trainable params: 111,892\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def LSTM_A(INPUT,INPUT_SIZE = 8,CELL_SIZE = 64):\n",
    "    TIME_STEPS = 60\n",
    "    OUTPUT_SIZE = 19\n",
    "    \n",
    "    activateion_fun = 'tanh'\n",
    "#     inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(INPUT)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "#     x = Attention()([x,x])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True)(x)\n",
    "#     x = Attention()([x,x])   # 现在改的\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalAveragePooling1D()(x)   \n",
    "    return x\n",
    "\n",
    "def LSTM_Model():\n",
    "    \n",
    "    TIME_STEPS = 60\n",
    "    INPUT_SIZE = len(group1)\n",
    "    OUTPUT_SIZE = 19\n",
    "    activateion_fun = 'tanh'\n",
    "    INPUT = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    part = tf.split(INPUT,axis=2, num_or_size_splits = [3,3,2,2,3,1])\n",
    "    A = LSTM_A(INPUT,CELL_SIZE = 32)\n",
    "    A1 = LSTM_A(part[0],3,CELL_SIZE = 16)\n",
    "    A2 = LSTM_A(part[1],3,CELL_SIZE = 16)\n",
    "    A3 = LSTM_A(part[2],2,CELL_SIZE = 16)\n",
    "    A4 = LSTM_A(part[3],2,CELL_SIZE = 16)\n",
    "    A5 = LSTM_A(part[4],3,CELL_SIZE = 16)\n",
    "    A6 = LSTM_A(part[5],1,CELL_SIZE = 16)\n",
    "    \n",
    "    x = Concatenate()([A,A1,A2,A3,A4,A5,A6])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    X = BatchNormalization()(x)\n",
    "#     X = Dropout(0.5)(X)\n",
    "\n",
    "    X = Dense(64)(X)\n",
    "\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='19class')(X) #小类\n",
    "    print(output3.shape)\n",
    "    return Model(INPUT, output3)\n",
    "\n",
    "LSTM_Model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T10:56:35.269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 60, 14, 1)\n",
      "(None, 20)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 60, 14)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 60, 3), (Non 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 32)       6016        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_1[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, 60, 16)       1216        tf_op_layer_split_1[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 60, 16)       1216        tf_op_layer_split_1[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_1[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, 60, 16)       1152        tf_op_layer_split_1[0][5]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 60, 32)       64          lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 60, 16)       32          lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 60, 16)       32          lstm_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 60, 16)       32          lstm_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 60, 16)       32          lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 60, 16)       32          lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 60, 16)       32          lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 32)       0           layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 60, 16)       0           layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 60, 16)       0           layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 60, 16)       0           layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 60, 16)       0           layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 60, 16)       0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 60, 16)       0           layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 60, 32)       8320        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  (None, 60, 16)       2112        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 60, 16)       2112        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 60, 16)       2112        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, 60, 16)       2112        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (None, 60, 16)       2112        dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, 60, 16)       2112        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 60, 32)       64          lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 60, 16)       32          lstm_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 60, 16)       32          lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 60, 16)       32          lstm_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 60, 16)       32          lstm_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 60, 16)       32          lstm_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, 60, 16)       32          lstm_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 60, 32)       0           layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 60, 16)       0           layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 60, 16)       0           layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 60, 16)       0           layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 60, 16)       0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 60, 16)       0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 60, 16)       0           layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 32)       8320        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                  (None, 60, 16)       2112        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 60, 16)       2112        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 60, 16)       2112        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 60, 16)       2112        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (None, 60, 16)       2112        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, 60, 16)       2112        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 60, 32)       64          lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 60, 16)       32          lstm_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 60, 16)       32          lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 60, 16)       32          lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 60, 16)       32          lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 60, 16)       32          lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, 60, 16)       32          lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 32)       0           layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 60, 16)       0           layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 60, 16)       0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 60, 16)       0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 60, 16)       0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 60, 16)       0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 60, 16)       0           layer_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 32)           0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 16)           0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 16)           0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 16)           0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 16)           0           dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 16)           0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 16)           0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          33024       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16448       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,660\n",
      "Trainable params: 111,892\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 46s 4ms/sample - loss: 1.8560 - acc: 0.4717 - val_loss: 1.7058 - val_acc: 0.5067\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 1.3535 - acc: 0.6168 - val_loss: 1.4254 - val_acc: 0.6080\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 1.2204 - acc: 0.6700 - val_loss: 1.2315 - val_acc: 0.6867\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 1.1494 - acc: 0.6910 - val_loss: 1.1593 - val_acc: 0.6963\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 1.0982 - acc: 0.7163 - val_loss: 1.0966 - val_acc: 0.7137\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 1.0460 - acc: 0.7344 - val_loss: 1.1226 - val_acc: 0.7083\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 1.0095 - acc: 0.7495 - val_loss: 1.0676 - val_acc: 0.7240\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.9789 - acc: 0.7635 - val_loss: 1.0710 - val_acc: 0.7223\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.9468 - acc: 0.7711 - val_loss: 1.0526 - val_acc: 0.7363\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.9293 - acc: 0.7747 - val_loss: 1.0291 - val_acc: 0.7457\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 6s 500us/sample - loss: 0.9050 - acc: 0.7846 - val_loss: 1.0237 - val_acc: 0.7537\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.8903 - acc: 0.7931 - val_loss: 1.0299 - val_acc: 0.7570\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.8639 - acc: 0.8043 - val_loss: 1.0332 - val_acc: 0.7530\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.8437 - acc: 0.8121 - val_loss: 1.0008 - val_acc: 0.7657\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.8317 - acc: 0.8223 - val_loss: 0.9983 - val_acc: 0.7703\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.8049 - acc: 0.8257 - val_loss: 0.9945 - val_acc: 0.7677\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 6s 489us/sample - loss: 0.7995 - acc: 0.8311 - val_loss: 0.9975 - val_acc: 0.7750\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.7882 - acc: 0.8360 - val_loss: 1.0368 - val_acc: 0.7583\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.7715 - acc: 0.8427 - val_loss: 1.0165 - val_acc: 0.7700\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 6s 499us/sample - loss: 0.7660 - acc: 0.8442 - val_loss: 0.9940 - val_acc: 0.7860\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.7520 - acc: 0.8493 - val_loss: 0.9931 - val_acc: 0.7743\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.7476 - acc: 0.8474 - val_loss: 0.9663 - val_acc: 0.7923\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.7280 - acc: 0.8617 - val_loss: 0.9946 - val_acc: 0.7797\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.7246 - acc: 0.8623 - val_loss: 0.9949 - val_acc: 0.7810\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.7142 - acc: 0.8656 - val_loss: 1.0049 - val_acc: 0.7787\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.7087 - acc: 0.8677 - val_loss: 0.9987 - val_acc: 0.7843\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.6966 - acc: 0.8766 - val_loss: 1.0150 - val_acc: 0.7737\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.6991 - acc: 0.8735 - val_loss: 1.0268 - val_acc: 0.7763\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6840 - acc: 0.8788 - val_loss: 0.9951 - val_acc: 0.7830\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.6768 - acc: 0.8818 - val_loss: 0.9956 - val_acc: 0.7893\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.6647 - acc: 0.8886 - val_loss: 0.9993 - val_acc: 0.7863\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.6612 - acc: 0.8910 - val_loss: 0.9852 - val_acc: 0.7897\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.6455 - acc: 0.8946 - val_loss: 0.9961 - val_acc: 0.7930\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.6427 - acc: 0.8963 - val_loss: 0.9847 - val_acc: 0.7957\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6420 - acc: 0.9003 - val_loss: 0.9967 - val_acc: 0.7933\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.6416 - acc: 0.8994 - val_loss: 1.0170 - val_acc: 0.7870\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.6375 - acc: 0.9023 - val_loss: 1.0241 - val_acc: 0.7813\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 6s 509us/sample - loss: 0.6253 - acc: 0.9046 - val_loss: 0.9915 - val_acc: 0.7960\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6146 - acc: 0.9138 - val_loss: 1.0152 - val_acc: 0.7900\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 6s 505us/sample - loss: 0.6168 - acc: 0.9120 - val_loss: 0.9907 - val_acc: 0.7967\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.6066 - acc: 0.9155 - val_loss: 1.0115 - val_acc: 0.7857\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.6068 - acc: 0.9187 - val_loss: 1.0069 - val_acc: 0.7940\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5915 - acc: 0.9223 - val_loss: 1.0153 - val_acc: 0.7833\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.5911 - acc: 0.9194 - val_loss: 0.9881 - val_acc: 0.7933\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.5872 - acc: 0.9228 - val_loss: 1.0121 - val_acc: 0.7957\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.5792 - acc: 0.9276 - val_loss: 1.0102 - val_acc: 0.7913\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5757 - acc: 0.9302 - val_loss: 1.0074 - val_acc: 0.7917\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.5826 - acc: 0.9244 - val_loss: 0.9998 - val_acc: 0.7960\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5693 - acc: 0.9318 - val_loss: 1.0142 - val_acc: 0.7953\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5723 - acc: 0.9279 - val_loss: 1.0329 - val_acc: 0.7880\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.5693 - acc: 0.9317 - val_loss: 1.0056 - val_acc: 0.8020\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5606 - acc: 0.9358 - val_loss: 1.0191 - val_acc: 0.7900\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5677 - acc: 0.9322 - val_loss: 1.0245 - val_acc: 0.7840\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.5586 - acc: 0.9353 - val_loss: 1.0120 - val_acc: 0.7917\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.5542 - acc: 0.9377 - val_loss: 1.0194 - val_acc: 0.7867\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5446 - acc: 0.9431 - val_loss: 1.0310 - val_acc: 0.7833\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.5393 - acc: 0.9463 - val_loss: 1.0086 - val_acc: 0.7973\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5468 - acc: 0.9415 - val_loss: 1.0159 - val_acc: 0.7937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5397 - acc: 0.9464 - val_loss: 1.0185 - val_acc: 0.7910\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.5297 - acc: 0.9513 - val_loss: 1.0072 - val_acc: 0.8060\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5314 - acc: 0.9492 - val_loss: 1.0150 - val_acc: 0.7943\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5342 - acc: 0.9481 - val_loss: 1.0209 - val_acc: 0.7887\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.5310 - acc: 0.9498 - val_loss: 1.0339 - val_acc: 0.7917\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5252 - acc: 0.9503 - val_loss: 1.0038 - val_acc: 0.8003\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.5271 - acc: 0.9513 - val_loss: 1.0221 - val_acc: 0.7940\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5220 - acc: 0.9532 - val_loss: 1.0280 - val_acc: 0.7947\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5185 - acc: 0.9551 - val_loss: 1.0404 - val_acc: 0.7873\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5196 - acc: 0.9540 - val_loss: 1.0325 - val_acc: 0.7953\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.5216 - acc: 0.9523 - val_loss: 1.0288 - val_acc: 0.7957\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5157 - acc: 0.9550 - val_loss: 1.0330 - val_acc: 0.7873\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.5129 - acc: 0.9567 - val_loss: 1.0228 - val_acc: 0.7980\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.5065 - acc: 0.9613 - val_loss: 1.0247 - val_acc: 0.7933\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5085 - acc: 0.9571 - val_loss: 1.0250 - val_acc: 0.7967\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5063 - acc: 0.9586 - val_loss: 1.0217 - val_acc: 0.8033\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5016 - acc: 0.9637 - val_loss: 1.0313 - val_acc: 0.7960\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5022 - acc: 0.9622 - val_loss: 1.0224 - val_acc: 0.8013\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5025 - acc: 0.9593 - val_loss: 1.0285 - val_acc: 0.8003\n",
      "Epoch 78/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.9643\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.4986 - acc: 0.9642 - val_loss: 1.0212 - val_acc: 0.7967\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.4758 - acc: 0.9746 - val_loss: 0.9884 - val_acc: 0.8090\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4690 - acc: 0.9768 - val_loss: 0.9994 - val_acc: 0.8063\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4642 - acc: 0.9773 - val_loss: 0.9978 - val_acc: 0.8067\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.4613 - acc: 0.9789 - val_loss: 0.9902 - val_acc: 0.8070\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4655 - acc: 0.9766 - val_loss: 0.9965 - val_acc: 0.8060\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.4620 - acc: 0.9777 - val_loss: 0.9996 - val_acc: 0.8037\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4619 - acc: 0.9797 - val_loss: 0.9941 - val_acc: 0.8070\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4581 - acc: 0.9798 - val_loss: 0.9986 - val_acc: 0.8047\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4593 - acc: 0.9793 - val_loss: 1.0107 - val_acc: 0.8007\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4575 - acc: 0.9783 - val_loss: 1.0015 - val_acc: 0.8050\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.4569 - acc: 0.9799 - val_loss: 1.0177 - val_acc: 0.7990\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.4595 - acc: 0.9791 - val_loss: 0.9964 - val_acc: 0.8080\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4554 - acc: 0.9796 - val_loss: 0.9985 - val_acc: 0.8033\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4557 - acc: 0.9795 - val_loss: 1.0045 - val_acc: 0.8057\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.4588 - acc: 0.9803 - val_loss: 0.9959 - val_acc: 0.8083\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4566 - acc: 0.9807 - val_loss: 0.9933 - val_acc: 0.8090\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4529 - acc: 0.9828 - val_loss: 0.9909 - val_acc: 0.8067\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.4519 - acc: 0.9825 - val_loss: 0.9888 - val_acc: 0.8067\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 6s 494us/sample - loss: 0.4556 - acc: 0.9800 - val_loss: 1.0026 - val_acc: 0.8120\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.4522 - acc: 0.9818 - val_loss: 1.0052 - val_acc: 0.8107\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4535 - acc: 0.9823 - val_loss: 1.0004 - val_acc: 0.8080\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.4546 - acc: 0.9805 - val_loss: 1.0088 - val_acc: 0.8023\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.4520 - acc: 0.9826 - val_loss: 1.0058 - val_acc: 0.8060\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.4525 - acc: 0.9835 - val_loss: 1.0042 - val_acc: 0.8057\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.4522 - acc: 0.9809 - val_loss: 1.0109 - val_acc: 0.8047\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.4505 - acc: 0.9825 - val_loss: 1.0067 - val_acc: 0.8070\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.4555 - acc: 0.9805 - val_loss: 1.0102 - val_acc: 0.8090\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.4475 - acc: 0.9836 - val_loss: 1.0075 - val_acc: 0.8060\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4491 - acc: 0.9827 - val_loss: 1.0061 - val_acc: 0.8033\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.4477 - acc: 0.9826 - val_loss: 1.0228 - val_acc: 0.8030\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.4495 - acc: 0.9827 - val_loss: 1.0154 - val_acc: 0.8020\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4479 - acc: 0.9821 - val_loss: 1.0034 - val_acc: 0.8083\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.4469 - acc: 0.9842 - val_loss: 1.0109 - val_acc: 0.8113\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.4466 - acc: 0.9843 - val_loss: 1.0033 - val_acc: 0.8037\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.4460 - acc: 0.9830 - val_loss: 1.0052 - val_acc: 0.8093\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4460 - acc: 0.9837 - val_loss: 1.0112 - val_acc: 0.8090\n",
      "Epoch 115/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.9842\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.4449 - acc: 0.9842 - val_loss: 1.0021 - val_acc: 0.8090\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4364 - acc: 0.9877 - val_loss: 0.9957 - val_acc: 0.8037\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.4330 - acc: 0.9879 - val_loss: 0.9915 - val_acc: 0.8090\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4326 - acc: 0.9888 - val_loss: 0.9924 - val_acc: 0.8107\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4314 - acc: 0.9900 - val_loss: 0.9951 - val_acc: 0.8093\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.4301 - acc: 0.9899 - val_loss: 0.9909 - val_acc: 0.8093\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.4298 - acc: 0.9911 - val_loss: 0.9937 - val_acc: 0.8117\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.4287 - acc: 0.9904 - val_loss: 0.9919 - val_acc: 0.8110\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.4277 - acc: 0.9898 - val_loss: 0.9979 - val_acc: 0.8073\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.4305 - acc: 0.9905 - val_loss: 0.9957 - val_acc: 0.8057\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.4294 - acc: 0.9890 - val_loss: 0.9938 - val_acc: 0.8047\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4299 - acc: 0.9896 - val_loss: 1.0017 - val_acc: 0.8103\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.4288 - acc: 0.9896 - val_loss: 0.9994 - val_acc: 0.8043\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4285 - acc: 0.9900 - val_loss: 0.9978 - val_acc: 0.8120\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.4284 - acc: 0.9900 - val_loss: 1.0032 - val_acc: 0.8093\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4282 - acc: 0.9895 - val_loss: 0.9984 - val_acc: 0.8097\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4267 - acc: 0.9908 - val_loss: 1.0023 - val_acc: 0.8110\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.4277 - acc: 0.9887 - val_loss: 1.0008 - val_acc: 0.8083\n",
      "Epoch 133/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.9884\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.4291 - acc: 0.9883 - val_loss: 0.9986 - val_acc: 0.8070\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4234 - acc: 0.9922 - val_loss: 0.9940 - val_acc: 0.8103\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.4210 - acc: 0.9932 - val_loss: 0.9942 - val_acc: 0.8090\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.4233 - acc: 0.9908 - val_loss: 0.9947 - val_acc: 0.8093\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.4217 - acc: 0.9919 - val_loss: 0.9945 - val_acc: 0.8097\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.4231 - acc: 0.9916 - val_loss: 0.9951 - val_acc: 0.8097\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4207 - acc: 0.9937 - val_loss: 0.9928 - val_acc: 0.8103\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4232 - acc: 0.9912 - val_loss: 0.9952 - val_acc: 0.8097\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.4225 - acc: 0.9921 - val_loss: 0.9954 - val_acc: 0.8070\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.4219 - acc: 0.9920 - val_loss: 0.9941 - val_acc: 0.8103\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.4216 - acc: 0.9927 - val_loss: 0.9935 - val_acc: 0.8127\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.4217 - acc: 0.9919 - val_loss: 0.9922 - val_acc: 0.8110\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.4222 - acc: 0.9912 - val_loss: 0.9993 - val_acc: 0.8080\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.4209 - acc: 0.9926 - val_loss: 0.9910 - val_acc: 0.8123\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.4198 - acc: 0.9923 - val_loss: 0.9993 - val_acc: 0.8073\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.4207 - acc: 0.9916 - val_loss: 0.9995 - val_acc: 0.8087\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4221 - acc: 0.9923 - val_loss: 0.9965 - val_acc: 0.8097\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.4199 - acc: 0.9928 - val_loss: 0.9939 - val_acc: 0.8097\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.4199 - acc: 0.9926 - val_loss: 0.9997 - val_acc: 0.8090\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4195 - acc: 0.9932 - val_loss: 0.9976 - val_acc: 0.8127\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.4181 - acc: 0.9932 - val_loss: 0.9968 - val_acc: 0.8113\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4188 - acc: 0.9922 - val_loss: 0.9973 - val_acc: 0.8093\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.4215 - acc: 0.9913 - val_loss: 1.0010 - val_acc: 0.8093\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.4182 - acc: 0.9927 - val_loss: 1.0002 - val_acc: 0.8080\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4200 - acc: 0.9930 - val_loss: 0.9987 - val_acc: 0.8107\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4193 - acc: 0.9918 - val_loss: 0.9977 - val_acc: 0.8053\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.4194 - acc: 0.9929 - val_loss: 0.9997 - val_acc: 0.8087\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4194 - acc: 0.9923 - val_loss: 1.0018 - val_acc: 0.8087\n",
      "Epoch 161/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.9928\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.4184 - acc: 0.9929 - val_loss: 1.0002 - val_acc: 0.8077\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4172 - acc: 0.9935 - val_loss: 0.9987 - val_acc: 0.8107\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.4181 - acc: 0.9933 - val_loss: 0.9988 - val_acc: 0.8087\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4158 - acc: 0.9946 - val_loss: 1.0021 - val_acc: 0.8073\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4151 - acc: 0.9946 - val_loss: 0.9976 - val_acc: 0.8073\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4144 - acc: 0.9947 - val_loss: 0.9989 - val_acc: 0.8080\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.4164 - acc: 0.9931 - val_loss: 0.9984 - val_acc: 0.8063\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4153 - acc: 0.9936 - val_loss: 1.0000 - val_acc: 0.8087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4158 - acc: 0.9935 - val_loss: 1.0007 - val_acc: 0.8083\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.4157 - acc: 0.9941 - val_loss: 1.0000 - val_acc: 0.8063\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.4148 - acc: 0.9942 - val_loss: 0.9988 - val_acc: 0.8090\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.4141 - acc: 0.9948 - val_loss: 0.9983 - val_acc: 0.8093\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4176 - acc: 0.9942 - val_loss: 1.0002 - val_acc: 0.8070\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.4153 - acc: 0.9933 - val_loss: 0.9969 - val_acc: 0.8087\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.4149 - acc: 0.9946 - val_loss: 1.0005 - val_acc: 0.8083\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4153 - acc: 0.9944 - val_loss: 1.0001 - val_acc: 0.8103\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.4146 - acc: 0.9939 - val_loss: 1.0013 - val_acc: 0.8100\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.4164 - acc: 0.9943 - val_loss: 1.0001 - val_acc: 0.8103\n",
      "Epoch 179/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.9946\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4147 - acc: 0.9947 - val_loss: 0.9997 - val_acc: 0.8087\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.4147 - acc: 0.9941 - val_loss: 0.9986 - val_acc: 0.8073\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.4145 - acc: 0.9947 - val_loss: 0.9992 - val_acc: 0.8067\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4134 - acc: 0.9948 - val_loss: 0.9986 - val_acc: 0.8077\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.4134 - acc: 0.9942 - val_loss: 0.9986 - val_acc: 0.8093\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.4128 - acc: 0.9942 - val_loss: 0.9988 - val_acc: 0.8093\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4137 - acc: 0.9948 - val_loss: 0.9992 - val_acc: 0.8077\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.4133 - acc: 0.9952 - val_loss: 0.9989 - val_acc: 0.8113\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.4128 - acc: 0.9942 - val_loss: 1.0004 - val_acc: 0.8100\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4156 - acc: 0.9939 - val_loss: 0.9990 - val_acc: 0.8107\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.4141 - acc: 0.9941 - val_loss: 0.9993 - val_acc: 0.8100\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4133 - acc: 0.9950 - val_loss: 0.9976 - val_acc: 0.8120\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4120 - acc: 0.9955 - val_loss: 0.9994 - val_acc: 0.8087\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4136 - acc: 0.9943 - val_loss: 0.9988 - val_acc: 0.8110\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4133 - acc: 0.9949 - val_loss: 0.9975 - val_acc: 0.8097\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4122 - acc: 0.9950 - val_loss: 1.0005 - val_acc: 0.8123\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.4128 - acc: 0.9946 - val_loss: 1.0008 - val_acc: 0.8080\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.4128 - acc: 0.9948 - val_loss: 0.9984 - val_acc: 0.8097\n",
      "Epoch 197/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.9954\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4121 - acc: 0.9953 - val_loss: 1.0006 - val_acc: 0.8087\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4139 - acc: 0.9945 - val_loss: 0.9996 - val_acc: 0.8107\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.4137 - acc: 0.9946 - val_loss: 0.9993 - val_acc: 0.8110\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4133 - acc: 0.9950 - val_loss: 0.9988 - val_acc: 0.8100\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.4118 - acc: 0.9941 - val_loss: 0.9990 - val_acc: 0.8097\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.4135 - acc: 0.9937 - val_loss: 0.9991 - val_acc: 0.8117\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4121 - acc: 0.9952 - val_loss: 0.9988 - val_acc: 0.8103\n",
      "Epoch 00203: early stopping\n",
      "0.813\n",
      "0.82949\n",
      "(15000, 60, 14, 1)\n",
      "(None, 20)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 60, 14)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_2 (TensorFlow [(None, 60, 3), (Non 0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 60, 32)       6016        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                  (None, 60, 16)       1216        tf_op_layer_split_2[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                  (None, 60, 16)       1216        tf_op_layer_split_2[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_57 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_60 (LSTM)                  (None, 60, 16)       1152        tf_op_layer_split_2[0][5]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, 60, 32)       64          lstm_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, 60, 16)       32          lstm_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, 60, 16)       32          lstm_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_51 (LayerNo (None, 60, 16)       32          lstm_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_54 (LayerNo (None, 60, 16)       32          lstm_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (None, 60, 16)       32          lstm_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_60 (LayerNo (None, 60, 16)       32          lstm_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 60, 32)       0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 60, 16)       0           layer_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 60, 16)       0           layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 60, 16)       0           layer_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 60, 16)       0           layer_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 60, 16)       0           layer_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 60, 16)       0           layer_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 60, 32)       8320        dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 60, 16)       2112        dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_49 (LSTM)                  (None, 60, 16)       2112        dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_52 (LSTM)                  (None, 60, 16)       2112        dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                  (None, 60, 16)       2112        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_58 (LSTM)                  (None, 60, 16)       2112        dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  (None, 60, 16)       2112        dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, 60, 32)       64          lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, 60, 16)       32          lstm_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, 60, 16)       32          lstm_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_52 (LayerNo (None, 60, 16)       32          lstm_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_55 (LayerNo (None, 60, 16)       32          lstm_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (None, 60, 16)       32          lstm_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_61 (LayerNo (None, 60, 16)       32          lstm_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 60, 32)       0           layer_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 60, 16)       0           layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 60, 16)       0           layer_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 60, 16)       0           layer_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 60, 16)       0           layer_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 60, 16)       0           layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 60, 16)       0           layer_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 60, 32)       8320        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 60, 16)       2112        dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_50 (LSTM)                  (None, 60, 16)       2112        dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_53 (LSTM)                  (None, 60, 16)       2112        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_56 (LSTM)                  (None, 60, 16)       2112        dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_59 (LSTM)                  (None, 60, 16)       2112        dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_62 (LSTM)                  (None, 60, 16)       2112        dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, 60, 32)       64          lstm_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, 60, 16)       32          lstm_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, 60, 16)       32          lstm_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_53 (LayerNo (None, 60, 16)       32          lstm_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (None, 60, 16)       32          lstm_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (None, 60, 16)       32          lstm_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, 60, 16)       32          lstm_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 60, 32)       0           layer_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 60, 16)       0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 60, 16)       0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 60, 16)       0           layer_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 60, 16)       0           layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 60, 16)       0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 60, 16)       0           layer_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 32)           0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 16)           0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 16)           0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 16)           0           dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 16)           0           dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 16)           0           dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 16)           0           dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "                                                                 global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          33024       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           16448       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,660\n",
      "Trainable params: 111,892\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 43s 4ms/sample - loss: 1.8058 - acc: 0.4734 - val_loss: 1.6960 - val_acc: 0.5060\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 1.3515 - acc: 0.6177 - val_loss: 1.4007 - val_acc: 0.6283\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 6s 495us/sample - loss: 1.2305 - acc: 0.6641 - val_loss: 1.2576 - val_acc: 0.6723\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 1.1586 - acc: 0.6923 - val_loss: 1.1616 - val_acc: 0.7067\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 6s 498us/sample - loss: 1.0944 - acc: 0.7181 - val_loss: 1.1125 - val_acc: 0.7103\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 1.0412 - acc: 0.7348 - val_loss: 1.0904 - val_acc: 0.7160\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 1.0201 - acc: 0.7412 - val_loss: 1.0814 - val_acc: 0.7270\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.9877 - acc: 0.7547 - val_loss: 1.1031 - val_acc: 0.7220\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.9559 - acc: 0.7692 - val_loss: 1.0357 - val_acc: 0.7483\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.9361 - acc: 0.7789 - val_loss: 1.0570 - val_acc: 0.7450\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.9109 - acc: 0.7861 - val_loss: 1.0458 - val_acc: 0.7483\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.9036 - acc: 0.7887 - val_loss: 1.0374 - val_acc: 0.7527\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.8834 - acc: 0.7936 - val_loss: 1.0442 - val_acc: 0.7607\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.8606 - acc: 0.8062 - val_loss: 1.0226 - val_acc: 0.7537\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.8435 - acc: 0.8124 - val_loss: 0.9941 - val_acc: 0.7683\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.8275 - acc: 0.8213 - val_loss: 1.0381 - val_acc: 0.7630\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.8109 - acc: 0.8246 - val_loss: 1.0234 - val_acc: 0.7623\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.7942 - acc: 0.8336 - val_loss: 1.0023 - val_acc: 0.7727\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.7916 - acc: 0.8352 - val_loss: 1.0290 - val_acc: 0.7713\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.7723 - acc: 0.8429 - val_loss: 1.0166 - val_acc: 0.7597\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.7669 - acc: 0.8440 - val_loss: 1.0270 - val_acc: 0.7660\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.7505 - acc: 0.8527 - val_loss: 1.0096 - val_acc: 0.7730\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.7423 - acc: 0.8562 - val_loss: 1.0030 - val_acc: 0.7690\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.7321 - acc: 0.8602 - val_loss: 1.0007 - val_acc: 0.7740\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.7268 - acc: 0.8635 - val_loss: 0.9996 - val_acc: 0.7773\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.7162 - acc: 0.8652 - val_loss: 1.0050 - val_acc: 0.7687\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.7122 - acc: 0.8648 - val_loss: 1.0101 - val_acc: 0.7797\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.7037 - acc: 0.8700 - val_loss: 1.0034 - val_acc: 0.7673\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.6912 - acc: 0.8768 - val_loss: 1.0147 - val_acc: 0.7760\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.6850 - acc: 0.8778 - val_loss: 1.0011 - val_acc: 0.7807\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.6667 - acc: 0.8884 - val_loss: 0.9992 - val_acc: 0.7767\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.6648 - acc: 0.8898 - val_loss: 1.0379 - val_acc: 0.7713\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.6607 - acc: 0.8913 - val_loss: 1.0129 - val_acc: 0.7790\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.6500 - acc: 0.8982 - val_loss: 1.0099 - val_acc: 0.7793\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.6428 - acc: 0.9006 - val_loss: 1.0017 - val_acc: 0.7777\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.6354 - acc: 0.9043 - val_loss: 1.0045 - val_acc: 0.7837\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.6332 - acc: 0.9018 - val_loss: 1.0139 - val_acc: 0.7747\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.6306 - acc: 0.9043 - val_loss: 1.0181 - val_acc: 0.7837\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.6260 - acc: 0.9081 - val_loss: 1.0194 - val_acc: 0.7817\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.6141 - acc: 0.9140 - val_loss: 1.0188 - val_acc: 0.7840\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6147 - acc: 0.9122 - val_loss: 1.0313 - val_acc: 0.7830\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.6081 - acc: 0.9147 - val_loss: 1.0095 - val_acc: 0.7840\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 499us/sample - loss: 0.6063 - acc: 0.9139 - val_loss: 1.0131 - val_acc: 0.7897\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5931 - acc: 0.9233 - val_loss: 1.0119 - val_acc: 0.7887\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.5938 - acc: 0.9196 - val_loss: 1.0198 - val_acc: 0.7863\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5859 - acc: 0.9242 - val_loss: 1.0265 - val_acc: 0.7813\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5849 - acc: 0.9252 - val_loss: 1.0303 - val_acc: 0.7863\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.5801 - acc: 0.9293 - val_loss: 1.0178 - val_acc: 0.7897\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.5737 - acc: 0.9286 - val_loss: 1.0374 - val_acc: 0.7860\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5727 - acc: 0.9302 - val_loss: 1.0252 - val_acc: 0.7780\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5710 - acc: 0.9292 - val_loss: 1.0103 - val_acc: 0.7920\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.5710 - acc: 0.9308 - val_loss: 1.0420 - val_acc: 0.7810\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5611 - acc: 0.9357 - val_loss: 1.0183 - val_acc: 0.7930\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5631 - acc: 0.9364 - val_loss: 1.0183 - val_acc: 0.7900\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.5581 - acc: 0.9349 - val_loss: 1.0269 - val_acc: 0.7913\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.5558 - acc: 0.9384 - val_loss: 1.0179 - val_acc: 0.7937\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5448 - acc: 0.9435 - val_loss: 1.0354 - val_acc: 0.7937\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.5485 - acc: 0.9408 - val_loss: 1.0166 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5415 - acc: 0.9437 - val_loss: 1.0156 - val_acc: 0.7937\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5361 - acc: 0.9484 - val_loss: 1.0528 - val_acc: 0.7903\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5382 - acc: 0.9476 - val_loss: 1.0323 - val_acc: 0.7923\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5319 - acc: 0.9494 - val_loss: 1.0333 - val_acc: 0.7890\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.5245 - acc: 0.9538 - val_loss: 1.0276 - val_acc: 0.7930\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5282 - acc: 0.9519 - val_loss: 1.0229 - val_acc: 0.7910\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.5258 - acc: 0.9515 - val_loss: 1.0681 - val_acc: 0.7857\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5288 - acc: 0.9494 - val_loss: 1.0512 - val_acc: 0.7847\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5181 - acc: 0.9557 - val_loss: 1.0283 - val_acc: 0.7937\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5196 - acc: 0.9535 - val_loss: 1.0444 - val_acc: 0.7830\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5224 - acc: 0.9528 - val_loss: 1.0208 - val_acc: 0.7973\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.5100 - acc: 0.9579 - val_loss: 1.0368 - val_acc: 0.7933\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5067 - acc: 0.9602 - val_loss: 1.0384 - val_acc: 0.7940\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.5036 - acc: 0.9621 - val_loss: 1.0378 - val_acc: 0.7947\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5028 - acc: 0.9604 - val_loss: 1.0487 - val_acc: 0.7847\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.5026 - acc: 0.9639 - val_loss: 1.0314 - val_acc: 0.7870\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5069 - acc: 0.9578 - val_loss: 1.0419 - val_acc: 0.7893\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5022 - acc: 0.9617 - val_loss: 1.0458 - val_acc: 0.7917\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5019 - acc: 0.9613 - val_loss: 1.0200 - val_acc: 0.7937\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.5014 - acc: 0.9627 - val_loss: 1.0520 - val_acc: 0.7880\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.4951 - acc: 0.9644 - val_loss: 1.0438 - val_acc: 0.7927\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4927 - acc: 0.9672 - val_loss: 1.0247 - val_acc: 0.7933\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4880 - acc: 0.9678 - val_loss: 1.0239 - val_acc: 0.7920\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4937 - acc: 0.9643 - val_loss: 1.0463 - val_acc: 0.7930\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.4916 - acc: 0.9653 - val_loss: 1.0614 - val_acc: 0.7877\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.4949 - acc: 0.9638 - val_loss: 1.0413 - val_acc: 0.7920\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4853 - acc: 0.9658 - val_loss: 1.0330 - val_acc: 0.7930\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.4900 - acc: 0.9667 - val_loss: 1.0576 - val_acc: 0.7893\n",
      "Epoch 87/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.9665\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.4854 - acc: 0.9665 - val_loss: 1.0284 - val_acc: 0.7903\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.4615 - acc: 0.9771 - val_loss: 1.0068 - val_acc: 0.8043\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.4570 - acc: 0.9791 - val_loss: 1.0101 - val_acc: 0.8040\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.4528 - acc: 0.9811 - val_loss: 1.0130 - val_acc: 0.8000\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4498 - acc: 0.9827 - val_loss: 1.0169 - val_acc: 0.7997\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.4504 - acc: 0.9812 - val_loss: 1.0152 - val_acc: 0.8000\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.4473 - acc: 0.9847 - val_loss: 1.0204 - val_acc: 0.7997\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4483 - acc: 0.9833 - val_loss: 1.0190 - val_acc: 0.7970\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.4512 - acc: 0.9825 - val_loss: 1.0286 - val_acc: 0.8003\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.4471 - acc: 0.9841 - val_loss: 1.0245 - val_acc: 0.7960\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.4473 - acc: 0.9845 - val_loss: 1.0153 - val_acc: 0.8043\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4459 - acc: 0.9852 - val_loss: 1.0234 - val_acc: 0.8023\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.4458 - acc: 0.9839 - val_loss: 1.0290 - val_acc: 0.7943\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4437 - acc: 0.9857 - val_loss: 1.0314 - val_acc: 0.7957\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.4449 - acc: 0.9835 - val_loss: 1.0213 - val_acc: 0.8000\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.4454 - acc: 0.9842 - val_loss: 1.0310 - val_acc: 0.7967\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.4452 - acc: 0.9847 - val_loss: 1.0294 - val_acc: 0.8017\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.4468 - acc: 0.9826 - val_loss: 1.0308 - val_acc: 0.8003\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.4455 - acc: 0.9835 - val_loss: 1.0254 - val_acc: 0.8010\n",
      "Epoch 106/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.9837\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4449 - acc: 0.9836 - val_loss: 1.0225 - val_acc: 0.8017\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.4363 - acc: 0.9887 - val_loss: 1.0198 - val_acc: 0.8030\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.4335 - acc: 0.9871 - val_loss: 1.0172 - val_acc: 0.8060\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4344 - acc: 0.9871 - val_loss: 1.0126 - val_acc: 0.8053\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4315 - acc: 0.9890 - val_loss: 1.0181 - val_acc: 0.8007\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.4310 - acc: 0.9901 - val_loss: 1.0211 - val_acc: 0.8010\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4313 - acc: 0.9890 - val_loss: 1.0181 - val_acc: 0.8013\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4285 - acc: 0.9912 - val_loss: 1.0172 - val_acc: 0.8057\n",
      "Epoch 114/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.4290 - acc: 0.9908 - val_loss: 1.0225 - val_acc: 0.8013\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4281 - acc: 0.9911 - val_loss: 1.0225 - val_acc: 0.8003\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.4298 - acc: 0.9883 - val_loss: 1.0137 - val_acc: 0.7993\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4290 - acc: 0.9895 - val_loss: 1.0227 - val_acc: 0.8013\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4272 - acc: 0.9912 - val_loss: 1.0210 - val_acc: 0.7967\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.4267 - acc: 0.9915 - val_loss: 1.0267 - val_acc: 0.7933\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.4276 - acc: 0.9902 - val_loss: 1.0269 - val_acc: 0.8023\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4288 - acc: 0.9891 - val_loss: 1.0240 - val_acc: 0.7993\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.4278 - acc: 0.9908 - val_loss: 1.0270 - val_acc: 0.7987\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.4280 - acc: 0.9910 - val_loss: 1.0255 - val_acc: 0.7970\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.4270 - acc: 0.9902 - val_loss: 1.0192 - val_acc: 0.8000\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.4262 - acc: 0.9910 - val_loss: 1.0170 - val_acc: 0.7990\n",
      "Epoch 126/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.9908\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.4269 - acc: 0.9907 - val_loss: 1.0145 - val_acc: 0.8040\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.4226 - acc: 0.9922 - val_loss: 1.0152 - val_acc: 0.8023\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.4230 - acc: 0.9925 - val_loss: 1.0165 - val_acc: 0.7987\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.4213 - acc: 0.9921 - val_loss: 1.0149 - val_acc: 0.8017\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.4227 - acc: 0.9923 - val_loss: 1.0181 - val_acc: 0.8017\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.4219 - acc: 0.9931 - val_loss: 1.0182 - val_acc: 0.8007\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4210 - acc: 0.9918 - val_loss: 1.0226 - val_acc: 0.8050\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.4212 - acc: 0.9929 - val_loss: 1.0208 - val_acc: 0.7987\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4210 - acc: 0.9927 - val_loss: 1.0219 - val_acc: 0.7993\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.4197 - acc: 0.9933 - val_loss: 1.0157 - val_acc: 0.7993\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.4203 - acc: 0.9933 - val_loss: 1.0207 - val_acc: 0.7987\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.4212 - acc: 0.9920 - val_loss: 1.0169 - val_acc: 0.7983\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.4200 - acc: 0.9933 - val_loss: 1.0173 - val_acc: 0.7980\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.4195 - acc: 0.9923 - val_loss: 1.0173 - val_acc: 0.7990\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.4196 - acc: 0.9928 - val_loss: 1.0163 - val_acc: 0.7983\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.4206 - acc: 0.9933 - val_loss: 1.0177 - val_acc: 0.8003\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.4197 - acc: 0.9932 - val_loss: 1.0248 - val_acc: 0.7970\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.4206 - acc: 0.9923 - val_loss: 1.0226 - val_acc: 0.7983\n",
      "Epoch 144/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.9935\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4187 - acc: 0.9935 - val_loss: 1.0214 - val_acc: 0.7990\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.4176 - acc: 0.9930 - val_loss: 1.0211 - val_acc: 0.8020\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.4193 - acc: 0.9937 - val_loss: 1.0203 - val_acc: 0.7990\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.4183 - acc: 0.9941 - val_loss: 1.0199 - val_acc: 0.7987\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.4169 - acc: 0.9936 - val_loss: 1.0199 - val_acc: 0.7960\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.4180 - acc: 0.9939 - val_loss: 1.0213 - val_acc: 0.7983\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4177 - acc: 0.9931 - val_loss: 1.0212 - val_acc: 0.7960\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.4158 - acc: 0.9932 - val_loss: 1.0212 - val_acc: 0.7987\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.4178 - acc: 0.9937 - val_loss: 1.0186 - val_acc: 0.7977\n",
      "Epoch 153/400\n",
      " 2048/12000 [====>.........................] - ETA: 4s - loss: 0.4166 - acc: 0.9922WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-45b068e211d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;31m#                              class_weight=[classweights1,classweights2,classweights3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                              )\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "proba_oof = np.zeros((15000, 20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "\n",
    "mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "\n",
    "# # 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# # 每一个大类输出 \n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "seeds = [42,39,17][:1]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "        print(train.shape)\n",
    "        mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "        new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "        model = LSTM_Model()\n",
    "        model.summary()\n",
    "#         optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "#         lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "#         wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "        model.compile(loss=custom_loss,\n",
    "#                       ,loss_weights=[3,7,21],\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.5,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'LSTM{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx][:,:,:,0], y_binary[xx],\n",
    "                  epochs=400,\n",
    "                  batch_size=256,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(train[yy][:,:,:,0], y_binary[yy]),\n",
    "                  callbacks=[plateau, early_stopping, checkpoint],\n",
    "#                              class_weight=[classweights1,classweights2,classweights3]\n",
    "                             )\n",
    "        history.append(train_res)\n",
    "        \n",
    "        model.load_weights(f'LSTM{fold}.h5')\n",
    "        proba_t[:,:20] += model.predict(test[:,:,:,0], verbose=0, batch_size=1024) / fold_num /len(seeds)\n",
    "        proba_oof[yy,:20] += model.predict(train[yy][:,:,:,0],verbose=0,batch_size=1024)/len(seeds)\n",
    "\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy,:20], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:26:28.195445Z",
     "start_time": "2020-08-21T09:26:28.099862Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8269238095238166 0.806\n",
      "1 0.8165333333333402 0.794\n",
      "2 0.8266000000000066 0.806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LSTM0.82335_dict.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "joblib.dump(oof_dict,\"LSTM%.5f_dict.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T09:27:59.699599Z",
     "start_time": "2020-08-21T09:27:58.538866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80227\n",
      "0.82335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   fragment_id  16000 non-null  int64\n",
      " 1   behavior_id  16000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 250.1 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t[:,:20], axis=1)\n",
    "oof_y = np.argmax(proba_oof[:,:20], axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../data/'\n",
    "sub = pd.read_csv(data_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "sub.to_csv('LSTM%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
