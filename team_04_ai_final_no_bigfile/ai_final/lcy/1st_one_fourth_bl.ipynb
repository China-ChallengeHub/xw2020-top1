{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 14:32:51 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.92       Driver Version: 410.92       CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:0D.0 Off |                  Off |\r\n",
      "| N/A   67C    P0   184W / 250W |   7892MiB / 16130MiB |     65%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     31139      C   ..._04/anaconda3/envs/tf2_torch/bin/python  7881MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "online\n",
    "\n",
    "if random.choice([0,1,1,1,]):\n",
    "[0.859, 0.86, 0.861, 0.843, 0.855] 0.8556000000000001\n",
    "\n",
    "[0.8789124971441612, 0.8831880936061866, 0.8818342151675468, 0.8683454177281323, 0.8759553203997628] 0.877647108809158\n",
    "\n",
    "online 0.7916984126984127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        '0': 'A_1', '1': 'B_2', '2': 'A_3', '3': 'A_4', '4': 'B_3', '5': 'C_5', '6': 'C_2', '7': 'A_5', '8': 'B_1', \n",
    "        '9': 'C_1', '10': 'A_2', '11': 'C_3', '12': 'B_5', '13': 'B_4', '14': 'C_4', \n",
    "        '15': 'D_6', '16': 'E_7', '17': 'F_8', '18': 'G_9', '19': 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[str(int(y))], mapping[str(int(y_pred))]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor_test_final.csv  sensor_train_final.csv  submit_example.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path  = '../data/final_data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay', 'thetaz', 'xy',\n",
      "       'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "    df['thetax']=np.arctan(df.acc_xg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetay']=np.arctan(df.acc_yg/\n",
    "                           np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetaz']=np.arctan(df.acc_zg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "    df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "    df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'acc_xg',\n",
       " 'acc_yg',\n",
       " 'acc_zg',\n",
       " 'acc',\n",
       " 'accg',\n",
       " 'thetax',\n",
       " 'thetay',\n",
       " 'thetaz',\n",
       " 'xy',\n",
       " 'xy_g',\n",
       " 'g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NUM=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 16000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train['fragment_id'])),len(set(test['fragment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 59/7292 [00:00<00:12, 588.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:12<00:00, 600.37it/s]\n",
      "100%|██████████| 7500/7500 [00:12<00:00, 593.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    \n",
    "    \n",
    "    \n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 60/7292 [00:00<00:12, 599.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:12<00:00, 601.63it/s]\n",
      "100%|██████████| 7500/7500 [00:12<00:00, 589.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n",
      "(3, 30, 14, 1) (3,)\n"
     ]
    }
   ],
   "source": [
    "# 一个完成了的generator\n",
    "def data_generator(data,label,shuffle,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    \"\"\"\n",
    "    \n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    length=len(data)\n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "        count+=1\n",
    "#         print(count,inds)\n",
    "        yield data[inds],label[inds]\n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x[:10],y[:10],True,3):\n",
    "    print(a.shape,b.shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBNRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "#                activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def ConvRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "               activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def squeeze_excitation_layer(x, out_dim,ratio=8):\n",
    "    '''\n",
    "    SE module performs inter-channel weighting.\n",
    "    '''\n",
    "    squeeze = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
    "    excitation = Activation('relu')(excitation)\n",
    "    excitation = Dense(units=out_dim)(excitation)\n",
    "    excitation = Activation('sigmoid')(excitation)\n",
    "    excitation = Reshape((1,1,out_dim))(excitation)\n",
    "    scale = multiply([x,excitation])\n",
    "    return scale\n",
    "\n",
    "# def SE_Residual(X):\n",
    "#     A = \n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "#     X =  Add()([X,A])\n",
    "    \n",
    "\n",
    "def lenet5(input):\n",
    "    A = ConvBNRelu(input,64,kernal_size=(3,3))\n",
    "#     B = ConvBNRelu(input,16,kernal_size=(5,1))\n",
    "#     C = ConvBNRelu(input,16,kernal_size=(7,1))\n",
    "#     ABC = layers.Concatenate()([A,B,C])\n",
    "    X = ConvBNRelu(A,128)\n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    X = AveragePooling2D()(X)\n",
    "    \n",
    "    X = ConvBNRelu(X,256)\n",
    "    X = Dropout(0.3)(X)\n",
    "#     X = squeeze_excitation_layer(X,256)\n",
    "    X = ConvBNRelu(X,512)   \n",
    "    X = Dropout(0.5)(X)\n",
    "#     X = squeeze_excitation_layer(X,512)\n",
    "#     X = GlobalMaxPooling2D()(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "#     X = BatchNormalization()(X)\n",
    "    return X\n",
    "import tensorflow as tf\n",
    "def Net(sample_num):\n",
    "    input1 = Input(shape=(sample_num, FEATURE_NUM, 1))\n",
    "    part = tf.split(input1,axis=2, num_or_size_splits = [6, 2, 6])\n",
    "#     res = tf.split(c, axis = 3, num_or_size_splits = [2, 2, 4])\n",
    "    \n",
    "    \n",
    "    X1 = Concatenate(axis=-2)([part[0],part[1]])\n",
    "    X1 = lenet5(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dense(128, activation='relu')(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "\n",
    "    X2 = Concatenate(axis=-2)([part[0],part[2]])\n",
    "    X2 = lenet5(X2)    \n",
    "    X2 = BatchNormalization()(X2)\n",
    "#     X = Dense(512, activation='relu')(X)\n",
    "#     X = BatchNormalization()(X)\n",
    "    X2 = Dense(128, activation='relu')(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    \n",
    "    X = Concatenate(axis=-1)([X1,X2])\n",
    "    \n",
    "#     X = Dense(256)(X)    \n",
    "    \n",
    "#     output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "#     X = Dense(64)(X)\n",
    "#     output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='20class')(X) #小类\n",
    "    \n",
    "    \n",
    "    return Model([input1], [output3])\n",
    "\n",
    "# model = Net(60)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 30, 6, 1), ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 8, 1)     0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 12, 1)    0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 30, 8, 64)    576         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 30, 12, 64)   576         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 30, 8, 64)    256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 30, 12, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 30, 8, 64)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 30, 12, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 30, 8, 128)   73728       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 30, 12, 128)  73728       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 30, 8, 128)   512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 30, 12, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 30, 8, 128)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 30, 12, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 30, 8, 128)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 30, 12, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 15, 4, 128)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 15, 6, 128)   0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 15, 4, 256)   294912      average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 6, 256)   294912      average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 4, 256)   1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 6, 256)   1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 4, 256)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 6, 256)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 15, 4, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 15, 6, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 4, 512)   1179648     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 15, 6, 512)   1179648     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 4, 512)   2048        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 6, 512)   2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 4, 512)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 6, 512)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 15, 4, 512)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 15, 6, 512)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 512)          2048        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512)          2048        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           dropout_3[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "20class (Dense)                 (None, 20)           1300        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,259,604\n",
      "Trainable params: 3,253,204\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.7835 - acc: 0.1692 - val_loss: 2.7093 - val_acc: 0.1712\n",
      "Epoch 2/1000\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 2.5426 - acc: 0.2095 - val_loss: 2.5469 - val_acc: 0.2158\n",
      "Epoch 3/1000\n",
      "10000/10000 [==============================] - 7s 698us/sample - loss: 2.4547 - acc: 0.2285 - val_loss: 2.3030 - val_acc: 0.2660\n",
      "Epoch 4/1000\n",
      "10000/10000 [==============================] - 7s 682us/sample - loss: 2.4030 - acc: 0.2433 - val_loss: 2.4005 - val_acc: 0.2426\n",
      "Epoch 5/1000\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 2.3584 - acc: 0.2517 - val_loss: 2.2518 - val_acc: 0.2884\n",
      "Epoch 6/1000\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 2.3201 - acc: 0.2694 - val_loss: 2.2515 - val_acc: 0.2744\n",
      "Epoch 7/1000\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 2.2899 - acc: 0.2697 - val_loss: 2.2347 - val_acc: 0.2822\n",
      "Epoch 8/1000\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.2661 - acc: 0.2776 - val_loss: 2.2038 - val_acc: 0.2792\n",
      "Epoch 9/1000\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 2.2518 - acc: 0.2866 - val_loss: 2.2030 - val_acc: 0.2798\n",
      "Epoch 10/1000\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 2.2311 - acc: 0.2896 - val_loss: 2.2535 - val_acc: 0.2840\n",
      "Epoch 11/1000\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 2.2171 - acc: 0.2940 - val_loss: 2.1783 - val_acc: 0.3120\n",
      "Epoch 12/1000\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 2.2004 - acc: 0.3005 - val_loss: 2.5223 - val_acc: 0.2904\n",
      "Epoch 13/1000\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 2.1867 - acc: 0.2990 - val_loss: 2.1489 - val_acc: 0.3256\n",
      "Epoch 14/1000\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 2.1813 - acc: 0.3050 - val_loss: 2.1269 - val_acc: 0.3088\n",
      "Epoch 15/1000\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 2.1689 - acc: 0.3098 - val_loss: 2.1316 - val_acc: 0.3114\n",
      "Epoch 16/1000\n",
      "10000/10000 [==============================] - 4s 367us/sample - loss: 2.1569 - acc: 0.3119 - val_loss: 2.1150 - val_acc: 0.3420\n",
      "Epoch 17/1000\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 2.1421 - acc: 0.3172 - val_loss: 2.1039 - val_acc: 0.3322\n",
      "Epoch 18/1000\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 2.1433 - acc: 0.3164 - val_loss: 2.1217 - val_acc: 0.3324\n",
      "Epoch 19/1000\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.1301 - acc: 0.3156 - val_loss: 2.1007 - val_acc: 0.3368\n",
      "Epoch 20/1000\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.1206 - acc: 0.3212 - val_loss: 2.1012 - val_acc: 0.3244\n",
      "Epoch 21/1000\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 2.1133 - acc: 0.3277 - val_loss: 2.0570 - val_acc: 0.3528\n",
      "Epoch 22/1000\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.1071 - acc: 0.3296 - val_loss: 2.1309 - val_acc: 0.3092\n",
      "Epoch 23/1000\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.0999 - acc: 0.3352 - val_loss: 2.1035 - val_acc: 0.3336\n",
      "Epoch 24/1000\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.0929 - acc: 0.3342 - val_loss: 2.0488 - val_acc: 0.3492\n",
      "Epoch 25/1000\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.0940 - acc: 0.3293 - val_loss: 2.0668 - val_acc: 0.3460\n",
      "Epoch 26/1000\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.0766 - acc: 0.3367 - val_loss: 2.0624 - val_acc: 0.3436\n",
      "Epoch 27/1000\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 2.0746 - acc: 0.3387 - val_loss: 2.0640 - val_acc: 0.3462\n",
      "Epoch 28/1000\n",
      "10000/10000 [==============================] - 6s 626us/sample - loss: 2.0660 - acc: 0.3421 - val_loss: 2.0822 - val_acc: 0.3356\n",
      "Epoch 29/1000\n",
      "10000/10000 [==============================] - 7s 714us/sample - loss: 2.0511 - acc: 0.3436 - val_loss: 2.0548 - val_acc: 0.3528\n",
      "Epoch 30/1000\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 2.0547 - acc: 0.3453 - val_loss: 2.0657 - val_acc: 0.3428\n",
      "Epoch 31/1000\n",
      "10000/10000 [==============================] - 7s 683us/sample - loss: 2.0377 - acc: 0.3484 - val_loss: 2.0364 - val_acc: 0.3520\n",
      "Epoch 32/1000\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 2.0326 - acc: 0.3536 - val_loss: 2.0294 - val_acc: 0.3502\n",
      "Epoch 33/1000\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 2.0387 - acc: 0.3465 - val_loss: 2.0594 - val_acc: 0.3590\n",
      "Epoch 34/1000\n",
      "10000/10000 [==============================] - 7s 691us/sample - loss: 2.0225 - acc: 0.3554 - val_loss: 2.0585 - val_acc: 0.3536\n",
      "Epoch 35/1000\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 2.0349 - acc: 0.3503 - val_loss: 2.0200 - val_acc: 0.3640\n",
      "Epoch 36/1000\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 2.0183 - acc: 0.3543 - val_loss: 2.0341 - val_acc: 0.3546\n",
      "Epoch 37/1000\n",
      "10000/10000 [==============================] - 7s 693us/sample - loss: 2.0100 - acc: 0.3584 - val_loss: 2.0418 - val_acc: 0.3498\n",
      "Epoch 38/1000\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 2.0060 - acc: 0.3566 - val_loss: 2.0342 - val_acc: 0.3540\n",
      "Epoch 39/1000\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 2.0025 - acc: 0.3603 - val_loss: 2.0155 - val_acc: 0.3540\n",
      "Epoch 40/1000\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 2.0064 - acc: 0.3629 - val_loss: 2.0303 - val_acc: 0.3604\n",
      "Epoch 41/1000\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 1.9915 - acc: 0.3615 - val_loss: 2.0154 - val_acc: 0.3634\n",
      "Epoch 42/1000\n",
      "10000/10000 [==============================] - 7s 713us/sample - loss: 1.9692 - acc: 0.3716 - val_loss: 2.0122 - val_acc: 0.3640\n",
      "Epoch 43/1000\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 1.9852 - acc: 0.3629 - val_loss: 2.0114 - val_acc: 0.3758\n",
      "Epoch 44/1000\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 1.9694 - acc: 0.3712 - val_loss: 1.9866 - val_acc: 0.3808\n",
      "Epoch 45/1000\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.9706 - acc: 0.3722 - val_loss: 2.0487 - val_acc: 0.3546\n",
      "Epoch 46/1000\n",
      "10000/10000 [==============================] - 7s 711us/sample - loss: 1.9670 - acc: 0.3692 - val_loss: 2.0392 - val_acc: 0.3598\n",
      "Epoch 47/1000\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 1.9619 - acc: 0.3771 - val_loss: 2.0288 - val_acc: 0.3676\n",
      "Epoch 48/1000\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.9548 - acc: 0.3719 - val_loss: 2.0142 - val_acc: 0.3688\n",
      "Epoch 49/1000\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 1.9535 - acc: 0.3769 - val_loss: 2.0190 - val_acc: 0.3682\n",
      "Epoch 50/1000\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 1.9455 - acc: 0.3789 - val_loss: 2.0211 - val_acc: 0.3684\n",
      "Epoch 51/1000\n",
      "10000/10000 [==============================] - 7s 717us/sample - loss: 1.9455 - acc: 0.3790 - val_loss: 2.0064 - val_acc: 0.3728\n",
      "Epoch 52/1000\n",
      "10000/10000 [==============================] - 7s 719us/sample - loss: 1.9401 - acc: 0.3776 - val_loss: 2.0031 - val_acc: 0.3690\n",
      "Epoch 53/1000\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 1.9461 - acc: 0.3769 - val_loss: 2.0327 - val_acc: 0.3606\n",
      "Epoch 54/1000\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.9315 - acc: 0.3835 - val_loss: 2.0224 - val_acc: 0.3726\n",
      "Epoch 55/1000\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.9319 - acc: 0.3777 - val_loss: 1.9960 - val_acc: 0.3760\n",
      "Epoch 56/1000\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 1.9160 - acc: 0.3917 - val_loss: 2.0032 - val_acc: 0.3636\n",
      "Epoch 57/1000\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 1.9160 - acc: 0.3890 - val_loss: 2.0048 - val_acc: 0.3692\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.9131 - acc: 0.3884 - val_loss: 1.9946 - val_acc: 0.3784\n",
      "Epoch 59/1000\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.9154 - acc: 0.3984 - val_loss: 1.9953 - val_acc: 0.3758\n",
      "Epoch 60/1000\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.9050 - acc: 0.3950 - val_loss: 2.0251 - val_acc: 0.3712\n",
      "Epoch 61/1000\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.9018 - acc: 0.3957 - val_loss: 1.9881 - val_acc: 0.3792\n",
      "Epoch 62/1000\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 1.8905 - acc: 0.3980 - val_loss: 1.9957 - val_acc: 0.3794\n",
      "Epoch 63/1000\n",
      "10000/10000 [==============================] - 7s 695us/sample - loss: 1.8836 - acc: 0.4015 - val_loss: 2.0302 - val_acc: 0.3740\n",
      "Epoch 64/1000\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 1.8946 - acc: 0.3957 - val_loss: 2.0072 - val_acc: 0.3788\n",
      "Epoch 65/1000\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 1.8836 - acc: 0.3991 - val_loss: 2.0012 - val_acc: 0.3812\n",
      "Epoch 66/1000\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 1.8933 - acc: 0.3944 - val_loss: 2.0114 - val_acc: 0.3818\n",
      "Epoch 67/1000\n",
      "10000/10000 [==============================] - 7s 724us/sample - loss: 1.8988 - acc: 0.3955 - val_loss: 2.0327 - val_acc: 0.3684\n",
      "Epoch 68/1000\n",
      "10000/10000 [==============================] - 7s 691us/sample - loss: 1.8740 - acc: 0.3981 - val_loss: 2.0041 - val_acc: 0.3812\n",
      "Epoch 69/1000\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 1.8684 - acc: 0.4078 - val_loss: 2.0127 - val_acc: 0.3814\n",
      "Epoch 70/1000\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.8673 - acc: 0.4044 - val_loss: 2.0197 - val_acc: 0.3774\n",
      "Epoch 71/1000\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 1.8549 - acc: 0.4116 - val_loss: 2.0048 - val_acc: 0.3720\n",
      "Epoch 72/1000\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 1.8606 - acc: 0.4062 - val_loss: 2.0107 - val_acc: 0.3816\n",
      "Epoch 73/1000\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 1.8637 - acc: 0.4092 - val_loss: 1.9940 - val_acc: 0.3812\n",
      "Epoch 74/1000\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 1.8542 - acc: 0.4149 - val_loss: 2.0181 - val_acc: 0.3724\n",
      "Epoch 75/1000\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.8555 - acc: 0.4142 - val_loss: 2.0153 - val_acc: 0.3756\n",
      "Epoch 76/1000\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 1.8491 - acc: 0.4145 - val_loss: 1.9915 - val_acc: 0.3860\n",
      "Epoch 77/1000\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.8450 - acc: 0.4096 - val_loss: 2.0130 - val_acc: 0.3884\n",
      "Epoch 78/1000\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 1.8454 - acc: 0.4156 - val_loss: 2.0041 - val_acc: 0.3828\n",
      "Epoch 79/1000\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 1.8302 - acc: 0.4184 - val_loss: 2.0095 - val_acc: 0.3796\n",
      "Epoch 80/1000\n",
      "10000/10000 [==============================] - 7s 716us/sample - loss: 1.8440 - acc: 0.4091 - val_loss: 2.0378 - val_acc: 0.3682\n",
      "Epoch 81/1000\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.8362 - acc: 0.4158 - val_loss: 2.0217 - val_acc: 0.3790\n",
      "Epoch 82/1000\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.8175 - acc: 0.4245 - val_loss: 2.0199 - val_acc: 0.3772\n",
      "Epoch 83/1000\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 1.8136 - acc: 0.4265 - val_loss: 2.0268 - val_acc: 0.3864\n",
      "Epoch 84/1000\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 1.8196 - acc: 0.4207 - val_loss: 2.0671 - val_acc: 0.3756\n",
      "Epoch 85/1000\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 1.8151 - acc: 0.4274 - val_loss: 2.0021 - val_acc: 0.3870\n",
      "Epoch 86/1000\n",
      "10000/10000 [==============================] - 7s 722us/sample - loss: 1.8383 - acc: 0.4140 - val_loss: 2.0265 - val_acc: 0.3844\n",
      "Epoch 87/1000\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.8168 - acc: 0.4231 - val_loss: 1.9918 - val_acc: 0.3880\n",
      "Epoch 88/1000\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 1.8084 - acc: 0.4284 - val_loss: 2.0226 - val_acc: 0.3930\n",
      "Epoch 89/1000\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 1.8164 - acc: 0.4250 - val_loss: 2.0218 - val_acc: 0.3914\n",
      "Epoch 90/1000\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 1.8025 - acc: 0.4295 - val_loss: 2.0030 - val_acc: 0.3892\n",
      "Epoch 91/1000\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 1.8009 - acc: 0.4326 - val_loss: 2.0404 - val_acc: 0.3808\n",
      "Epoch 92/1000\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 1.7862 - acc: 0.4321 - val_loss: 2.0778 - val_acc: 0.3814\n",
      "Epoch 93/1000\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 1.7910 - acc: 0.4320 - val_loss: 2.0359 - val_acc: 0.3838\n",
      "Epoch 94/1000\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.7935 - acc: 0.4314 - val_loss: 2.0916 - val_acc: 0.3752\n",
      "Epoch 95/1000\n",
      "10000/10000 [==============================] - 7s 721us/sample - loss: 1.8073 - acc: 0.4278 - val_loss: 2.0493 - val_acc: 0.3832\n",
      "Epoch 96/1000\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 1.7947 - acc: 0.4311 - val_loss: 2.0273 - val_acc: 0.3876\n",
      "Epoch 97/1000\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 1.7924 - acc: 0.4317 - val_loss: 2.0686 - val_acc: 0.3796\n",
      "Epoch 98/1000\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 1.7816 - acc: 0.4366 - val_loss: 2.0397 - val_acc: 0.3818\n",
      "Epoch 99/1000\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 1.8030 - acc: 0.4304 - val_loss: 2.0488 - val_acc: 0.3814\n",
      "Epoch 100/1000\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 1.7763 - acc: 0.4390 - val_loss: 2.0354 - val_acc: 0.3860\n",
      "Epoch 101/1000\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 1.7756 - acc: 0.4406 - val_loss: 2.0555 - val_acc: 0.3812\n",
      "Epoch 102/1000\n",
      " 6464/10000 [==================>...........] - ETA: 2s - loss: 1.7710 - acc: 0.4392WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-79fff602eec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     callbacks=[plateau3, early_stopping, checkpoint])\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    790\u001b[0m   \"\"\"\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \"\"\"\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "fold_num=3\n",
    "\n",
    "proba_t = np.zeros((15000, 20))\n",
    "proba_oof = np.zeros((16000,20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "seeds=[42]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "\n",
    "\n",
    "        model = Net(30)\n",
    "        model.summary()\n",
    "        model.compile(loss=categorical_crossentropy,\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "\n",
    "        plateau3 = ReduceLROnPlateau(monitor=\"acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.1,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'bl{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx],y_binary[xx],batch_size=32,\n",
    "                  epochs=1000,shuffle=True,\n",
    "                  validation_data=(train[yy],y_binary[yy]),\n",
    "                    callbacks=[plateau3, early_stopping, checkpoint])\n",
    "        history.append(train_res)\n",
    "\n",
    "        model.load_weights(f'bl{fold}.h5')\n",
    "        proba_t += model.predict(test, verbose=0, batch_size=1024)[2] / (fold_num*len(seeds))\n",
    "        proba_oof[yy] += model.predict(train[yy],verbose=0,batch_size=1024) [2]\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
