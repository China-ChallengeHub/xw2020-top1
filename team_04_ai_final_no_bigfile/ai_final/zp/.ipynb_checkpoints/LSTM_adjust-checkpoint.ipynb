{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T07:32:14.837685Z",
     "start_time": "2020-07-10T07:32:14.833926Z"
    }
   },
   "source": [
    "# 多个loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:04:37.592829Z",
     "start_time": "2020-08-21T12:04:37.335711Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 20:04:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 64%   70C    P2   242W / 250W |  10996MiB / 11019MiB |     66%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "|100%   74C    P2   244W / 250W |  10996MiB / 11019MiB |     67%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 52%   70C    P2   240W / 250W |  10996MiB / 11019MiB |     66%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "|ERR!   68C    P2   238W / 250W |  10996MiB / 11019MiB |     58%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 26%   49C    P2    68W / 250W |  10998MiB / 11019MiB |     28%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 17%   29C    P8     5W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 16%   30C    P8     2W / 250W |  10998MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 69%   71C    P2   241W / 250W |  10996MiB / 11019MiB |     82%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     24783      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10985MiB |\n",
      "|    1     27561      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10985MiB |\n",
      "|    2     28668      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10985MiB |\n",
      "|    3     35946      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10985MiB |\n",
      "|    4     43743      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10987MiB |\n",
      "|    6     26499      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10987MiB |\n",
      "|    7      3991      C   ...qibot/miniconda3/envs/tf_2.1/bin/python 10985MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:27.803542Z",
     "start_time": "2020-08-21T12:08:27.793566Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:31.934751Z",
     "start_time": "2020-08-21T12:08:31.267837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "# def acc_combo(y, y_pred):\n",
    "#     # 数值ID与行为编码的对应关系\n",
    "#     mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "#         4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "#         8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "#         12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "#         16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "#     # 将行为ID转为编码\n",
    "#     code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "#     if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "#         return 1.0\n",
    "#     elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "#         return 1.0/7\n",
    "#     elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "#         return 1.0/3\n",
    "#     else:\n",
    "#         return 0.0\n",
    "\n",
    "\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:05:10.717506Z",
     "start_time": "2020-08-21T10:05:09.954698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jiaozibei\r\n",
      "tensorflow_addons-0.11.1-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "tensorflow_addons-0.9.1-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "typeguard-2.9.1-py3-none-any.whl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:47.435927Z",
     "start_time": "2020-08-21T12:08:46.897484Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "# data_test['fragment_id'] += 100000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:54.906017Z",
     "start_time": "2020-08-21T12:08:53.407694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n",
      "(420421, 12)\n",
      "(7292, 60, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "#     df['thetax']=np.arctan(df.acc_xg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetay']=np.arctan(df.acc_yg/\n",
    "#                            np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetaz']=np.arctan(df.acc_zg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "#     df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "#     df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "train=add_features(train)\n",
    "test=add_features(test)\n",
    "\n",
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "# group1 = [\"acc_x\",\"acc_y\",\"acc_z\",\"acc\",\"acc_xg\",\"acc_yg\",\"acc_zg\",\"accg\"]\n",
    "\n",
    "\n",
    "test['fragment_id'] += 100000\n",
    "data = pd.concat([train, test], sort=False)\n",
    "ss_tool = StandardScaler()\n",
    "data[group1] = ss_tool.fit_transform(data[group1])\n",
    "\n",
    "\n",
    "train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "test['fragment_id'] -= 100000\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "\n",
    "x = np.zeros((7292, sample_num, len(group1), 1))\n",
    "t = np.zeros((7500, sample_num, len(group1), 1))\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:54.949822Z",
     "start_time": "2020-08-21T12:08:54.907877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x', 'acc_y', 'acc_z', 'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'g']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:08:56.459275Z",
     "start_time": "2020-08-21T12:08:56.417356Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:09:15.262984Z",
     "start_time": "2020-08-21T12:09:15.213975Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import resample\n",
    "def get_fft_values(y_values, N, f_s):\n",
    "    f_values = np.linspace(0.0, f_s/2.0, N//2)\n",
    "    fft_values_ = fft(y_values)\n",
    "    plt.plot(fft_values_)\n",
    "    plt.show()\n",
    "    print(fft_values_.shape)\n",
    "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "    print(fft_values.shape)\n",
    "    return f_values, fft_values\n",
    "\n",
    "# tmp = train[train.fragment_id == 0][:sample_num]\n",
    "\n",
    "# get_fft_values(tmp[\"acc\"].values,60,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:10:42.792688Z",
     "start_time": "2020-08-21T12:09:23.892867Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/15000 [00:00<00:29, 499.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:36<00:00, 408.44it/s]\n",
      "100%|██████████| 16000/16000 [00:40<00:00, 394.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test['fragment_id'] += 100000\n",
    "data = pd.concat([train, test], sort=False)\n",
    "ss_tool = StandardScaler()\n",
    "data[group1] = ss_tool.fit_transform(data[group1])\n",
    "train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "test['fragment_id'] -= 100000                         \n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(15000)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(16000)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T10:09:14.528524Z",
     "start_time": "2020-08-21T10:09:14.461847Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from tensorflow.python.keras import backend as K\n",
    "# from tensorflow.python.util.tf_export import keras_export\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     \"\"\"Learning Rate Schedule\n",
    "#     Learning rate is scheduled to be reduced after 20, 30 epochs.\n",
    "#     Called automatically every epoch as part of callbacks during training.\n",
    "#     # Arguments\n",
    "#         epoch (int): The number of epochs\n",
    "#     # Returns\n",
    "#         lr (float32): learning rate\n",
    "#     \"\"\"\n",
    "#     lr = 5e-4\n",
    "#     if epoch >= 150:\n",
    "#         lr *= 1e-1\n",
    "#     elif epoch >= 100:\n",
    "#         lr *= 1e-1\n",
    "#     elif epoch >= 50:\n",
    "#         lr *= 1e-1\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "# def wd_schedule(epoch):\n",
    "#     \"\"\"Weight Decay Schedule\n",
    "#     Weight decay is scheduled to be reduced after 20, 30 epochs.\n",
    "#     Called automatically every epoch as part of callbacks during training.\n",
    "#     # Arguments\n",
    "#         epoch (int): The number of epochs\n",
    "#     # Returns\n",
    "#         wd (float32): weight decay\n",
    "#     \"\"\"\n",
    "#     wd = 2e-5\n",
    "\n",
    "#     if epoch >= 100:\n",
    "#         wd *= 2e-2\n",
    "#     elif epoch >= 50:\n",
    "#         wd *= 1e-1\n",
    "#     print('Weight decay: ', wd)\n",
    "#     return wd\n",
    "\n",
    "\n",
    "# # just copy the implement of LearningRateScheduler, and then change the lr with weight_decay\n",
    "# @keras_export('keras.callbacks.WeightDecayScheduler')\n",
    "# class WeightDecayScheduler(Callback):\n",
    "#     \"\"\"Weight Decay Scheduler.\n",
    "\n",
    "#     Arguments:\n",
    "#         schedule: a function that takes an epoch index as input\n",
    "#             (integer, indexed from 0) and returns a new\n",
    "#             weight decay as output (float).\n",
    "#         verbose: int. 0: quiet, 1: update messages.\n",
    "\n",
    "#     ```python\n",
    "#     # This function keeps the weight decay at 0.001 for the first ten epochs\n",
    "#     # and decreases it exponentially after that.\n",
    "#     def scheduler(epoch):\n",
    "#       if epoch < 10:\n",
    "#         return 0.001\n",
    "#       else:\n",
    "#         return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "#     callback = WeightDecayScheduler(scheduler)\n",
    "#     model.fit(data, labels, epochs=100, callbacks=[callback],\n",
    "#               validation_data=(val_data, val_labels))\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, schedule, verbose=0):\n",
    "#         super(WeightDecayScheduler, self).__init__()\n",
    "#         self.schedule = schedule\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         if not hasattr(self.model.optimizer, 'weight_decay'):\n",
    "#             raise ValueError('Optimizer must have a \"weight_decay\" attribute.')\n",
    "#         try:  # new API\n",
    "#             weight_decay = float(K.get_value(self.model.optimizer.weight_decay))\n",
    "#             weight_decay = self.schedule(epoch, weight_decay)\n",
    "#         except TypeError:  # Support for old API for backward compatibility\n",
    "#             weight_decay = self.schedule(epoch)\n",
    "#         if not isinstance(weight_decay, (float, np.float32, np.float64)):\n",
    "#             raise ValueError('The output of the \"schedule\" function '\n",
    "#                              'should be float.')\n",
    "#         K.set_value(self.model.optimizer.weight_decay, weight_decay)\n",
    "#         if self.verbose > 0:\n",
    "#             print('\\nEpoch %05d: WeightDecayScheduler reducing weight '\n",
    "#                   'decay to %s.' % (epoch + 1, weight_decay))\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         logs['weight_decay'] = K.get_value(self.model.optimizer.weight_decay)\n",
    "\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# #     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "# #     gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# #     for gpu in gpus:\n",
    "# #         tf.config.experimental.set_memory_growth(gpu, enable=True)\n",
    "# #     print(gpus)\n",
    "# #     cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "# #     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# #     x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# #     model = tf.keras.models.Sequential([\n",
    "# #         tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "# #         tf.keras.layers.AveragePooling2D(),\n",
    "# #         tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "# #         tf.keras.layers.AveragePooling2D(),\n",
    "# #         tf.keras.layers.Flatten(),\n",
    "# #         tf.keras.layers.Dense(10, activation='softmax')\n",
    "# #     ])\n",
    "\n",
    "# #     optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "# #     # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# #     tb_callback = tf.keras.callbacks.TensorBoard(os.path.join('logs', 'adamw'),\n",
    "# #                                                  profile_batch=0)\n",
    "# #     lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "# #     wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "\n",
    "# #     model.compile(optimizer=optimizer,\n",
    "# #                   loss='sparse_categorical_crossentropy',\n",
    "# #                   metrics=['accuracy'])\n",
    "\n",
    "# #     model.fit(x_train, y_train, epochs=40, validation_split=0.1,\n",
    "# #               callbacks=[tb_callback, lr_callback, wd_callback])\n",
    "\n",
    "# #     model.evaluate(x_test, y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T12:25:50.890996Z",
     "start_time": "2020-08-21T12:25:46.347118Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 60, 3), (Non 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 3)        0           tf_op_layer_split[0][2]          \n",
      "                                                                 tf_op_layer_split[0][3]          \n",
      "                                                                 tf_op_layer_split[0][4]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][3]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 60, 32)       5376        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 60, 16)       1280        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 60, 32)       4736        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 60, 32)       4736        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 60, 32)       64          lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 60, 16)       32          lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 60, 16)       32          lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 60, 16)       32          lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 60, 32)       64          lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 60, 32)       64          lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 60, 32)       0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60, 16)       0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60, 16)       0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 60, 16)       0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 60, 32)       0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 60, 32)       0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 60, 32)       8320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 60, 16)       2112        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 60, 16)       2112        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 60, 16)       2112        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 60, 32)       8320        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 60, 32)       8320        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 60, 32)       64          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 60, 16)       32          lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 60, 16)       32          lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 60, 16)       32          lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 60, 32)       64          lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 60, 32)       64          lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60, 32)       0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 60, 16)       0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 60, 16)       0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 60, 16)       0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 60, 32)       0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 60, 32)       0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 60, 32)       8320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 60, 16)       2112        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 60, 16)       2112        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 60, 16)       2112        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 60, 32)       8320        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 60, 32)       8320        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 60, 32)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 60, 16)       0           lstm_5[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 60, 16)       0           lstm_8[0][0]                     \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 60, 16)       0           lstm_11[0][0]                    \n",
      "                                                                 lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 60, 32)       0           lstm_14[0][0]                    \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 60, 32)       0           lstm_17[0][0]                    \n",
      "                                                                 lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 60, 32)       64          attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 60, 16)       32          attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 60, 16)       32          attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 60, 16)       32          attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 60, 32)       64          attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 60, 32)       64          attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 60, 32)       0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 60, 16)       0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 60, 16)       0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 60, 16)       0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 60, 32)       0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 60, 32)       0           layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 16)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 16)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 16)           0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 144)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 144)          576         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          37120       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def LSTM_A(input,INPUT_SIZE = 8,CELL_SIZE = 64):\n",
    "    TIME_STEPS = 60\n",
    "    OUTPUT_SIZE = 19\n",
    "    \n",
    "    activateion_fun = 'tanh'\n",
    "#     inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "#     x = Attention()([x,x])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True)(x)\n",
    "    x = Attention()([x,x])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalAveragePooling1D()(x)   \n",
    "    return x\n",
    "\n",
    "def LSTM_B(input,INPUT_SIZE = 8):\n",
    "    TIME_STEPS = 60\n",
    "    OUTPUT_SIZE = 19\n",
    "    CELL_SIZE = 128\n",
    "    activateion_fun = 'tanh'\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(CELL_SIZE*2, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE*2), return_sequences=False)(x)\n",
    "    x = LayerNormalization()(x) \n",
    "    return x\n",
    "\n",
    "def LSTM_C(input,INPUT_SIZE = 8):\n",
    "    TIME_STEPS = 60\n",
    "    INPUT_SIZE = 8\n",
    "    OUTPUT_SIZE = 19\n",
    "    CELL_SIZE = 64\n",
    "    activateion_fun = 'tanh'\n",
    "#     inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE*2, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE*2), return_sequences=False)(x)\n",
    "    x = LayerNormalization()(x) \n",
    "    return x\n",
    "import tensorflow as tf\n",
    "def LSTM_Model():\n",
    "    \n",
    "    TIME_STEPS = 60\n",
    "    INPUT_SIZE = len(group1)\n",
    "    OUTPUT_SIZE = 19\n",
    "    activateion_fun = 'tanh'\n",
    "    inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    part = tf.split(inputs,axis=2, num_or_size_splits = [3,3,1,1,1])\n",
    "    A = LSTM_A(inputs,CELL_SIZE = 32)\n",
    "    A1 = LSTM_A(part[0],3,CELL_SIZE = 16)\n",
    "    A2 = LSTM_A(part[1],3,CELL_SIZE = 16)\n",
    "    A3 = LSTM_A(Concatenate()([part[2],part[3],part[4]]),3,CELL_SIZE = 16)\n",
    "    A4 = LSTM_A(Concatenate()([part[0],part[2]]),4,CELL_SIZE = 32)\n",
    "    A5 = LSTM_A(Concatenate()([part[1],part[3]]),4,CELL_SIZE = 32)\n",
    "    \n",
    "#     A4 = LSTM_A(part[3],6,CELL_SIZE = 16)    \n",
    "#     B = LSTM_B(inputs,INPUT_SIZE=9)\n",
    "#     C = LSTM_C(inputs,CELL_SIZE=46)\n",
    "    x = Concatenate()([A,A1,A2,A3,A4,A5])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    X = BatchNormalization()(x)\n",
    "    output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "    print(X.shape)\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "    X = Dense(64)(X)\n",
    "    output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(64)(X)\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='19class')(X) #小类\n",
    "    print(output3.shape)\n",
    "    return Model([inputs], output3)\n",
    "\n",
    "LSTM_Model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T10:09:21.288Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_18 (TensorFlo [(None, 60, 3), (Non 0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_18[0][2]       \n",
      "                                                                 tf_op_layer_split_18[0][3]       \n",
      "                                                                 tf_op_layer_split_18[0][4]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_18[0][0]       \n",
      "                                                                 tf_op_layer_split_18[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_18[0][1]       \n",
      "                                                                 tf_op_layer_split_18[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_288 (LSTM)                 (None, 60, 32)       5376        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_291 (LSTM)                 (None, 60, 16)       1280        tf_op_layer_split_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_294 (LSTM)                 (None, 60, 16)       1280        tf_op_layer_split_18[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_297 (LSTM)                 (None, 60, 16)       1280        concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_300 (LSTM)                 (None, 60, 32)       4736        concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_303 (LSTM)                 (None, 60, 32)       4736        concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_288 (LayerN (None, 60, 32)       64          lstm_288[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_291 (LayerN (None, 60, 16)       32          lstm_291[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_294 (LayerN (None, 60, 16)       32          lstm_294[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_297 (LayerN (None, 60, 16)       32          lstm_297[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_300 (LayerN (None, 60, 32)       64          lstm_300[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_303 (LayerN (None, 60, 32)       64          lstm_303[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_294 (Dropout)           (None, 60, 32)       0           layer_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_297 (Dropout)           (None, 60, 16)       0           layer_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_300 (Dropout)           (None, 60, 16)       0           layer_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_303 (Dropout)           (None, 60, 16)       0           layer_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_306 (Dropout)           (None, 60, 32)       0           layer_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_309 (Dropout)           (None, 60, 32)       0           layer_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_289 (LSTM)                 (None, 60, 32)       8320        dropout_294[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_292 (LSTM)                 (None, 60, 16)       2112        dropout_297[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_295 (LSTM)                 (None, 60, 16)       2112        dropout_300[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_298 (LSTM)                 (None, 60, 16)       2112        dropout_303[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_301 (LSTM)                 (None, 60, 32)       8320        dropout_306[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_304 (LSTM)                 (None, 60, 32)       8320        dropout_309[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_289 (LayerN (None, 60, 32)       64          lstm_289[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_292 (LayerN (None, 60, 16)       32          lstm_292[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_295 (LayerN (None, 60, 16)       32          lstm_295[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_298 (LayerN (None, 60, 16)       32          lstm_298[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_301 (LayerN (None, 60, 32)       64          lstm_301[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_304 (LayerN (None, 60, 32)       64          lstm_304[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_295 (Dropout)           (None, 60, 32)       0           layer_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_298 (Dropout)           (None, 60, 16)       0           layer_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_301 (Dropout)           (None, 60, 16)       0           layer_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_304 (Dropout)           (None, 60, 16)       0           layer_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_307 (Dropout)           (None, 60, 32)       0           layer_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_310 (Dropout)           (None, 60, 32)       0           layer_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_290 (LSTM)                 (None, 60, 32)       8320        dropout_295[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_293 (LSTM)                 (None, 60, 16)       2112        dropout_298[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_296 (LSTM)                 (None, 60, 16)       2112        dropout_301[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_299 (LSTM)                 (None, 60, 16)       2112        dropout_304[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_302 (LSTM)                 (None, 60, 32)       8320        dropout_307[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_305 (LSTM)                 (None, 60, 32)       8320        dropout_310[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_96 (Attention)        (None, 60, 32)       0           lstm_290[0][0]                   \n",
      "                                                                 lstm_290[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_97 (Attention)        (None, 60, 16)       0           lstm_293[0][0]                   \n",
      "                                                                 lstm_293[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_98 (Attention)        (None, 60, 16)       0           lstm_296[0][0]                   \n",
      "                                                                 lstm_296[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_99 (Attention)        (None, 60, 16)       0           lstm_299[0][0]                   \n",
      "                                                                 lstm_299[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_100 (Attention)       (None, 60, 32)       0           lstm_302[0][0]                   \n",
      "                                                                 lstm_302[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_101 (Attention)       (None, 60, 32)       0           lstm_305[0][0]                   \n",
      "                                                                 lstm_305[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_290 (LayerN (None, 60, 32)       64          attention_96[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_293 (LayerN (None, 60, 16)       32          attention_97[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_296 (LayerN (None, 60, 16)       32          attention_98[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_299 (LayerN (None, 60, 16)       32          attention_99[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_302 (LayerN (None, 60, 32)       64          attention_100[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_305 (LayerN (None, 60, 32)       64          attention_101[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_296 (Dropout)           (None, 60, 32)       0           layer_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_299 (Dropout)           (None, 60, 16)       0           layer_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_302 (Dropout)           (None, 60, 16)       0           layer_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_305 (Dropout)           (None, 60, 16)       0           layer_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_308 (Dropout)           (None, 60, 32)       0           layer_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_311 (Dropout)           (None, 60, 32)       0           layer_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_96 (Gl (None, 32)           0           dropout_296[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_97 (Gl (None, 16)           0           dropout_299[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_98 (Gl (None, 16)           0           dropout_302[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_99 (Gl (None, 16)           0           dropout_305[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_100 (G (None, 32)           0           dropout_308[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_101 (G (None, 32)           0           dropout_311[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 144)          0           global_average_pooling1d_96[0][0]\n",
      "                                                                 global_average_pooling1d_97[0][0]\n",
      "                                                                 global_average_pooling1d_98[0][0]\n",
      "                                                                 global_average_pooling1d_99[0][0]\n",
      "                                                                 global_average_pooling1d_100[0][0\n",
      "                                                                 global_average_pooling1d_101[0][0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 144)          576         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 256)          37120       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 256)          1024        dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 64)           16448       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 64)           4160        dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_56[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n",
      "12000/12000 [==============================] - 45s 4ms/sample - loss: 1.8316 - acc: 0.4736 - val_loss: 1.7460 - val_acc: 0.4943\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 7s 558us/sample - loss: 1.3710 - acc: 0.6077 - val_loss: 1.4536 - val_acc: 0.5963\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 7s 561us/sample - loss: 1.2397 - acc: 0.6592 - val_loss: 1.2562 - val_acc: 0.6693\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 7s 616us/sample - loss: 1.1594 - acc: 0.6897 - val_loss: 1.1518 - val_acc: 0.6927\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 7s 556us/sample - loss: 1.0915 - acc: 0.7126 - val_loss: 1.1417 - val_acc: 0.6920\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 1.0472 - acc: 0.7257 - val_loss: 1.1004 - val_acc: 0.7133\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 7s 594us/sample - loss: 1.0033 - acc: 0.7463 - val_loss: 1.0668 - val_acc: 0.7337\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 7s 573us/sample - loss: 0.9692 - acc: 0.7586 - val_loss: 1.0231 - val_acc: 0.7433\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 7s 555us/sample - loss: 0.9444 - acc: 0.7702 - val_loss: 1.0357 - val_acc: 0.7427\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 7s 598us/sample - loss: 0.9208 - acc: 0.7801 - val_loss: 1.0312 - val_acc: 0.7470\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 7s 585us/sample - loss: 0.8886 - acc: 0.7938 - val_loss: 1.0073 - val_acc: 0.7573\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 7s 575us/sample - loss: 0.8674 - acc: 0.8051 - val_loss: 1.0072 - val_acc: 0.7610\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 7s 591us/sample - loss: 0.8421 - acc: 0.8126 - val_loss: 0.9651 - val_acc: 0.7770\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 7s 564us/sample - loss: 0.8335 - acc: 0.8131 - val_loss: 0.9767 - val_acc: 0.7703\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 7s 588us/sample - loss: 0.8069 - acc: 0.8293 - val_loss: 0.9754 - val_acc: 0.7720\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 7s 599us/sample - loss: 0.7969 - acc: 0.8315 - val_loss: 0.9608 - val_acc: 0.7803\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 6s 542us/sample - loss: 0.7832 - acc: 0.8356 - val_loss: 0.9761 - val_acc: 0.7803\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 6s 534us/sample - loss: 0.7633 - acc: 0.8467 - val_loss: 0.9759 - val_acc: 0.7757\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 7s 606us/sample - loss: 0.7516 - acc: 0.8501 - val_loss: 0.9559 - val_acc: 0.7883\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 7s 562us/sample - loss: 0.7442 - acc: 0.8535 - val_loss: 0.9635 - val_acc: 0.7840\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 7s 594us/sample - loss: 0.7264 - acc: 0.8608 - val_loss: 0.9526 - val_acc: 0.7893\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 7s 579us/sample - loss: 0.7137 - acc: 0.8676 - val_loss: 0.9480 - val_acc: 0.7927\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 528us/sample - loss: 0.7094 - acc: 0.8713 - val_loss: 0.9656 - val_acc: 0.7880\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 7s 561us/sample - loss: 0.6959 - acc: 0.8766 - val_loss: 0.9561 - val_acc: 0.7857\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 7s 591us/sample - loss: 0.6901 - acc: 0.8780 - val_loss: 0.9484 - val_acc: 0.7963\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 7s 559us/sample - loss: 0.6662 - acc: 0.8896 - val_loss: 0.9572 - val_acc: 0.7930\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 0.6587 - acc: 0.8944 - val_loss: 0.9298 - val_acc: 0.7990\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 530us/sample - loss: 0.6518 - acc: 0.8945 - val_loss: 0.9420 - val_acc: 0.7897\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 7s 566us/sample - loss: 0.6395 - acc: 0.9005 - val_loss: 0.9407 - val_acc: 0.7987\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 7s 558us/sample - loss: 0.6347 - acc: 0.9053 - val_loss: 0.9579 - val_acc: 0.7947\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 7s 551us/sample - loss: 0.6256 - acc: 0.9085 - val_loss: 0.9669 - val_acc: 0.7907\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 7s 562us/sample - loss: 0.6213 - acc: 0.9115 - val_loss: 0.9574 - val_acc: 0.7953\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 7s 594us/sample - loss: 0.6069 - acc: 0.9170 - val_loss: 0.9242 - val_acc: 0.8057\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 7s 554us/sample - loss: 0.6034 - acc: 0.9197 - val_loss: 0.9513 - val_acc: 0.7970\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 7s 555us/sample - loss: 0.5966 - acc: 0.9243 - val_loss: 0.9615 - val_acc: 0.7940\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 7s 578us/sample - loss: 0.5904 - acc: 0.9258 - val_loss: 0.9340 - val_acc: 0.8070\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 0.5821 - acc: 0.9295 - val_loss: 0.9452 - val_acc: 0.8080\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 6s 532us/sample - loss: 0.5771 - acc: 0.9305 - val_loss: 0.9369 - val_acc: 0.8053\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 7s 550us/sample - loss: 0.5686 - acc: 0.9346 - val_loss: 0.9476 - val_acc: 0.8023\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 7s 547us/sample - loss: 0.5723 - acc: 0.9337 - val_loss: 0.9467 - val_acc: 0.8080\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 7s 546us/sample - loss: 0.5622 - acc: 0.9366 - val_loss: 0.9586 - val_acc: 0.8047\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 6s 538us/sample - loss: 0.5677 - acc: 0.9352 - val_loss: 0.9655 - val_acc: 0.8007\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 539us/sample - loss: 0.5518 - acc: 0.9417 - val_loss: 0.9349 - val_acc: 0.8050\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 7s 554us/sample - loss: 0.5453 - acc: 0.9464 - val_loss: 0.9589 - val_acc: 0.8063\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 7s 590us/sample - loss: 0.5462 - acc: 0.9425 - val_loss: 0.9470 - val_acc: 0.8083\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 7s 600us/sample - loss: 0.5361 - acc: 0.9491 - val_loss: 0.9356 - val_acc: 0.8110\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 0.5361 - acc: 0.9488 - val_loss: 0.9629 - val_acc: 0.8007\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 7s 559us/sample - loss: 0.5353 - acc: 0.9488 - val_loss: 0.9333 - val_acc: 0.8110\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 7s 565us/sample - loss: 0.5334 - acc: 0.9492 - val_loss: 0.9450 - val_acc: 0.8093\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 7s 552us/sample - loss: 0.5258 - acc: 0.9534 - val_loss: 0.9489 - val_acc: 0.8037\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 7s 571us/sample - loss: 0.5216 - acc: 0.9547 - val_loss: 0.9411 - val_acc: 0.8077\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 7s 597us/sample - loss: 0.5205 - acc: 0.9537 - val_loss: 0.9410 - val_acc: 0.8113\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 7s 591us/sample - loss: 0.5213 - acc: 0.9551 - val_loss: 0.9322 - val_acc: 0.8160\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 7s 560us/sample - loss: 0.5103 - acc: 0.9592 - val_loss: 0.9644 - val_acc: 0.8137\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 7s 565us/sample - loss: 0.5160 - acc: 0.9561 - val_loss: 0.9504 - val_acc: 0.8123\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 7s 576us/sample - loss: 0.5004 - acc: 0.9642 - val_loss: 0.9564 - val_acc: 0.8073\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 7s 561us/sample - loss: 0.5059 - acc: 0.9613 - val_loss: 0.9593 - val_acc: 0.8057\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 6s 540us/sample - loss: 0.5081 - acc: 0.9610 - val_loss: 0.9487 - val_acc: 0.8097\n",
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 7s 566us/sample - loss: 0.4973 - acc: 0.9653 - val_loss: 0.9532 - val_acc: 0.8043\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 7s 563us/sample - loss: 0.4877 - acc: 0.9692 - val_loss: 0.9381 - val_acc: 0.8123\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 7s 579us/sample - loss: 0.4890 - acc: 0.9676 - val_loss: 0.9561 - val_acc: 0.8073\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 0.4888 - acc: 0.9685 - val_loss: 0.9541 - val_acc: 0.8093\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 7s 591us/sample - loss: 0.4866 - acc: 0.9686 - val_loss: 0.9352 - val_acc: 0.8173\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 7s 566us/sample - loss: 0.4914 - acc: 0.9671 - val_loss: 0.9397 - val_acc: 0.8080\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 7s 609us/sample - loss: 0.4839 - acc: 0.9684 - val_loss: 0.9513 - val_acc: 0.8053\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 7s 569us/sample - loss: 0.4807 - acc: 0.9719 - val_loss: 0.9587 - val_acc: 0.8010\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 7s 570us/sample - loss: 0.4743 - acc: 0.9753 - val_loss: 0.9616 - val_acc: 0.8060\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 7s 582us/sample - loss: 0.4815 - acc: 0.9716 - val_loss: 0.9559 - val_acc: 0.8130\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 7s 544us/sample - loss: 0.4744 - acc: 0.9750 - val_loss: 0.9421 - val_acc: 0.8080\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 7s 600us/sample - loss: 0.4673 - acc: 0.9756 - val_loss: 0.9473 - val_acc: 0.8127\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 7s 579us/sample - loss: 0.4752 - acc: 0.9738 - val_loss: 0.9565 - val_acc: 0.8020\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 7s 593us/sample - loss: 0.4696 - acc: 0.9762 - val_loss: 0.9718 - val_acc: 0.8030\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 7s 571us/sample - loss: 0.4767 - acc: 0.9723 - val_loss: 0.9624 - val_acc: 0.8067\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 7s 566us/sample - loss: 0.4626 - acc: 0.9786 - val_loss: 0.9390 - val_acc: 0.8147\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 7s 565us/sample - loss: 0.4688 - acc: 0.9763 - val_loss: 0.9427 - val_acc: 0.8103\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 7s 567us/sample - loss: 0.4640 - acc: 0.9793 - val_loss: 0.9513 - val_acc: 0.8087\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 7s 573us/sample - loss: 0.4634 - acc: 0.9787 - val_loss: 0.9433 - val_acc: 0.8110\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 7s 556us/sample - loss: 0.4616 - acc: 0.9784 - val_loss: 0.9683 - val_acc: 0.8053\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 7s 563us/sample - loss: 0.4650 - acc: 0.9756 - val_loss: 0.9504 - val_acc: 0.8073\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 7s 584us/sample - loss: 0.4556 - acc: 0.9791 - val_loss: 0.9620 - val_acc: 0.8150\n",
      "Epoch 81/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.9814\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 7s 584us/sample - loss: 0.4531 - acc: 0.9813 - val_loss: 0.9566 - val_acc: 0.8053\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 7s 590us/sample - loss: 0.4346 - acc: 0.9877 - val_loss: 0.9339 - val_acc: 0.8170\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 7s 594us/sample - loss: 0.4284 - acc: 0.9915 - val_loss: 0.9323 - val_acc: 0.8153\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 7s 547us/sample - loss: 0.4266 - acc: 0.9904 - val_loss: 0.9278 - val_acc: 0.8143\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 7s 579us/sample - loss: 0.4245 - acc: 0.9931 - val_loss: 0.9225 - val_acc: 0.8173\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 7s 615us/sample - loss: 0.4252 - acc: 0.9912 - val_loss: 0.9334 - val_acc: 0.8190\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 7s 578us/sample - loss: 0.4230 - acc: 0.9915 - val_loss: 0.9318 - val_acc: 0.8177\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 7s 613us/sample - loss: 0.4252 - acc: 0.9913 - val_loss: 0.9282 - val_acc: 0.8210\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 7s 552us/sample - loss: 0.4244 - acc: 0.9922 - val_loss: 0.9312 - val_acc: 0.8130\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 7s 559us/sample - loss: 0.4236 - acc: 0.9912 - val_loss: 0.9330 - val_acc: 0.8193\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 7s 571us/sample - loss: 0.4227 - acc: 0.9918 - val_loss: 0.9371 - val_acc: 0.8197\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 7s 553us/sample - loss: 0.4247 - acc: 0.9916 - val_loss: 0.9386 - val_acc: 0.8173\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 7s 558us/sample - loss: 0.4215 - acc: 0.9927 - val_loss: 0.9471 - val_acc: 0.8143\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 7s 559us/sample - loss: 0.4225 - acc: 0.9918 - val_loss: 0.9287 - val_acc: 0.8193\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 7s 557us/sample - loss: 0.4233 - acc: 0.9918 - val_loss: 0.9392 - val_acc: 0.8170\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 7s 559us/sample - loss: 0.4212 - acc: 0.9925 - val_loss: 0.9377 - val_acc: 0.8140\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 7s 556us/sample - loss: 0.4202 - acc: 0.9930 - val_loss: 0.9375 - val_acc: 0.8163\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 7s 573us/sample - loss: 0.4190 - acc: 0.9930 - val_loss: 0.9359 - val_acc: 0.8157\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 7s 564us/sample - loss: 0.4207 - acc: 0.9919 - val_loss: 0.9385 - val_acc: 0.8150\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 7s 572us/sample - loss: 0.4193 - acc: 0.9923 - val_loss: 0.9370 - val_acc: 0.8197\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 7s 576us/sample - loss: 0.4192 - acc: 0.9925 - val_loss: 0.9433 - val_acc: 0.8143\n"
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "proba_oof = np.zeros((15000, 20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "\n",
    "mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "\n",
    "# # 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# # 每一个大类输出 \n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "seeds = [42,39,17][:1]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "        print(train.shape)\n",
    "        mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "        new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "        model = LSTM_Model()\n",
    "        model.summary()\n",
    "#         optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "#         lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "#         wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "        model.compile(loss=custom_loss,\n",
    "#                       ,loss_weights=[3,7,21],\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.5,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'LSTM{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx][:,:,:,0], y_binary[xx],\n",
    "                  epochs=400,\n",
    "                  batch_size=256,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(train[yy][:,:,:,0], y_binary[yy]),\n",
    "                  callbacks=[plateau, early_stopping, checkpoint],\n",
    "#                              class_weight=[classweights1,classweights2,classweights3]\n",
    "                             )\n",
    "        history.append(train_res)\n",
    "\n",
    "\n",
    "    #     # 找到对应最高的 val_acc 对应的epoch，预测left+1+right次\n",
    "    #     left=2\n",
    "    #     right=2\n",
    "    #     max_acc_index=history[fold].history['val_acc'].index(np.max(history[fold].history['val_acc']))+1\n",
    "\n",
    "    #     save_filelist=os.listdir(save_dir)\n",
    "    #     save_filelist.sort()\n",
    "    #     select_blending=save_filelist[max_acc_index-left : max_acc_index+right]\n",
    "    #     print(select_blending)\n",
    "    #     for file in select_blending:\n",
    "    #         model.load_weights(save_dir+'/'+file)\n",
    "    #         proba_t += model.predict(t, verbose=0, batch_size=1024) / (fold_num*len(select_blending))\n",
    "    #         proba_oof[yy] = model.predict(train[yy],verbose=0,batch_size=1024) / len(select_blending)\n",
    "\n",
    "        model.load_weights(f'LSTM{fold}.h5')\n",
    "        proba_t[:,:20] += model.predict(test[:,:,:,0], verbose=0, batch_size=1024) / fold_num /len(seeds)\n",
    "        proba_oof[yy,:20] += model.predict(train[yy][:,:,:,0],verbose=0,batch_size=1024)/len(seeds)\n",
    "\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy,:20], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T11:58:24.274255Z",
     "start_time": "2020-08-21T11:58:24.219470Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8377460317460333 0.823\n",
      "1 0.8428095238095245 0.825\n",
      "2 0.8279206349206353 0.807\n",
      "3 0.842952380952381 0.824\n",
      "4 0.8362222222222224 0.818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LSTM8370.83753_dict.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "# joblib.dump(oof_dict,\"LSTM837%.5f_dict.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T11:57:14.545441Z",
     "start_time": "2020-08-21T11:57:13.844517Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192\n",
      "0.83753\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD6CAYAAABNu5eFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYK0lEQVR4nO3df5BdZX3H8ffHRCKglNBsMO7GJtpADYwV2KapP5ASKwFpAiq6jD9SxaZmgoKtP0hxREczQ9W21lFiU4gEpYQtP0y0RRKjSDsDpBt+mF9E1gaTJUt2rW2l2gkmfPvHeTJzvdzdu+fc3WXj83nN3LnnPuc8z/ne3buf++y5596riMDMzPLwvOe6ADMzGz8OfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDQNfUlrJA1I2l7X/gFJuyXtkPTZmvYVknrTuvNq2s+StC2t+6Ikje5dMTOzZiaPYJsbgS8BNx1pkPSHwGLglRFxUNL01D4X6AJOA14CfEfSKRFxGFgFLAXuB/4FWAjc1Wzn06ZNi1mzZpW4S2ZmtnXr1p9ERFt9e9PQj4h7Jc2qa14GXBsRB9M2A6l9MbAute+R1AvMk/Q4cEJE3Acg6SbgIkYQ+rNmzaKnp6fZZmZmVkPSjxu1Vz2mfwrwOkkPSPq+pN9L7e3Avprt+lJbe1qubx+q2KWSeiT1DA4OVizRzMzqVQ39ycBUYD7wEaA7HaNvdJw+hmlvKCJWR0RnRHS2tT3rvxMzM6uoauj3AXdEYQvwDDAttc+s2a4D2J/aOxq0m5nZOKoa+t8AzgWQdApwDPATYAPQJWmKpNnAHGBLRPQDT0man/4jeDewvuXqzcyslKYv5Eq6BTgHmCapD7gGWAOsSadxPg0sieLjOndI6gZ2AoeA5enMHShe/L0ROJbiBdymL+Kamdno0kT/aOXOzs7w2TtmZuVI2hoRnfXtfkeumVlGHPpmZhlx6JuZZWQkH8NgZmbjbOBLGyv1m375G4dd75m+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlpGvqS1kgaSN+HW7/uw5JC0rSathWSeiXtlnReTftZkraldV9MX5BuZmbjaCQz/RuBhfWNkmYCfwTsrWmbC3QBp6U+10malFavApYCc9LlWWOamdnYahr6EXEv8NMGq/4W+ChQ+83qi4F1EXEwIvYAvcA8STOAEyLivii+if0m4KKWqzczs1IqHdOXtAh4IiIeqVvVDuyrud2X2trTcn37UOMvldQjqWdwcLBKiWZm1kDp0Jd0HHA18IlGqxu0xTDtDUXE6ojojIjOtra2siWamdkQqnxH7suB2cAj6bXYDuBBSfMoZvAza7btAPan9o4G7WZmNo5Kz/QjYltETI+IWRExiyLQz4yIJ4ENQJekKZJmU7xguyUi+oGnJM1PZ+28G1g/enfDzMxGoulMX9ItwDnANEl9wDURcUOjbSNih6RuYCdwCFgeEYfT6mUUZwIdC9yVLpax89e/pXSfuxbfPgaVHP0uvv3fKvW78y2vHeVKbKJrGvoRcWmT9bPqbq8EVjbYrgc4vWR9ZmY2ivyOXDOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy0jT0Ja2RNCBpe03b5yQ9KukHku6UdGLNuhWSeiXtlnReTftZkraldV9MX5BuZmbjaCQz/RuBhXVtm4DTI+KVwA+BFQCS5gJdwGmpz3WSJqU+q4ClwJx0qR/TzMzGWNPQj4h7gZ/WtW2MiEPp5v1AR1peDKyLiIMRsQfoBeZJmgGcEBH3RUQANwEXjdadMDOzkRmNY/rvBe5Ky+3Avpp1famtPS3XtzckaamkHkk9g4ODo1CimZlBi6Ev6WrgEHDzkaYGm8Uw7Q1FxOqI6IyIzra2tlZKNDOzGpOrdpS0BLgQWJAO2UAxg59Zs1kHsD+1dzRoNzOzcVRppi9pIfAxYFFE/KJm1QagS9IUSbMpXrDdEhH9wFOS5qezdt4NrG+xdjMzK6npTF/SLcA5wDRJfcA1FGfrTAE2pTMv74+I90fEDkndwE6Kwz7LI+JwGmoZxZlAx1K8BnAXZmY2rpqGfkRc2qD5hmG2XwmsbNDeA5xeqjozMxtVfkeumVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZaTyRyvb0evvv3Ze843q/Nm77h6DSsxsvHmmb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGmoa+pDWSBiRtr2k7SdImSY+l66k161ZI6pW0W9J5Ne1nSdqW1n0xfUG6mZmNo5HM9G8EFta1XQVsjog5wOZ0G0lzgS7gtNTnOkmTUp9VwFJgTrrUj2lmZmOsaehHxL3AT+uaFwNr0/Ja4KKa9nURcTAi9gC9wDxJM4ATIuK+iAjgppo+ZmY2Tqoe0z85IvoB0vX01N4O7KvZri+1tafl+vaGJC2V1COpZ3BwsGKJZmZWb7RfyG10nD6GaW8oIlZHRGdEdLa1tY1acWZmuasa+gfSIRvS9UBq7wNm1mzXAexP7R0N2s3MbBxVDf0NwJK0vARYX9PeJWmKpNkUL9huSYeAnpI0P5218+6aPmZmNk6afsqmpFuAc4BpkvqAa4BrgW5JlwF7gUsAImKHpG5gJ3AIWB4Rh9NQyyjOBDoWuCtdzMxsHDUN/Yi4dIhVC4bYfiWwskF7D3B6qerMzGxU+R25ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWWk6dclDkfSh4D3AQFsA94DHAfcCswCHgfeFhH/lbZfAVwGHAY+GBF3j2Q/g6u+Xqm+tmXvrNTPzOzXVeWZvqR24INAZ0ScDkwCuoCrgM0RMQfYnG4jaW5afxqwELhO0qTWyjczszJaPbwzGThW0mSKGf5+YDGwNq1fC1yUlhcD6yLiYETsAXqBeS3u38zMSqh8eCcinpD0eWAv8H/AxojYKOnkiOhP2/RLmp66tAP31wzRl9qeRdJSYCnAS1/60qolmpk9Zw58YWvpPidfedYYVPKrWjm8M5Vi9j4beAlwvKThDqKrQVs02jAiVkdEZ0R0trW1VS3RzMzqtPJC7huAPRExCCDpDuDVwAFJM9IsfwYwkLbvA2bW9O+gOBxkJdz21YWV+r31Pd8e5UrM7GjUyjH9vcB8ScdJErAA2AVsAJakbZYA69PyBqBL0hRJs4E5wJYW9m9mZiW1ckz/AUm3AQ8Ch4CHgNXAC4FuSZdRPDFckrbfIakb2Jm2Xx4Rh1us38zMSmjpPP2IuAa4pq75IMWsv9H2K4GVrezTzMyqayn0zZ5LF9z5mUr9/uXij49yJWZHD38Mg5lZRhz6ZmYZ8eEdMwPg7Xf0lu5z65t/ewwqac1D1w8036iBM943vflGvwY80zczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIy2FvqQTJd0m6VFJuyT9gaSTJG2S9Fi6nlqz/QpJvZJ2Szqv9fLNzKyMVmf6fwd8OyJ+B/hdYBdwFbA5IuYAm9NtJM0FuoDTgIXAdZImtbh/MzMrofKXqEg6ATgb+BOAiHgaeFrSYuCctNla4B7gY8BiYF1EHAT2SOoF5gH3Va3B7NfFotu+WbrPhrf+8RhU8ty669afVOp3/tunjXIlv75amem/DBgEvirpIUnXSzoeODki+gHS9ZGvo2kH9tX070ttzyJpqaQeST2Dg4MtlGhmZrVaCf3JwJnAqog4A/g56VDOENSgLRptGBGrI6IzIjrb2tpaKNHMzGq18h25fUBfRDyQbt9GEfoHJM2IiH5JM4CBmu1n1vTvAPa3sH8zm2C+fOeB0n2WX3zyGFRiQ6k804+IJ4F9kk5NTQuAncAGYElqWwKsT8sbgC5JUyTNBuYAW6ru38zMymtlpg/wAeBmSccA/wG8h+KJpFvSZcBe4BKAiNghqZviieEQsDwiDre4fzMzK6Gl0I+Ih4HOBqsWDLH9SmBlK/s0M7PqWp3pZ+XRLy8u3ed3lq9vvpGZ2Thx6I+ze/7hTaX7nPOn/zwGlZhZjvzZO2ZmGXHom5llxId3LGtvumNV6T7//OZlY1CJ2fjIJvSfXPWZSv1evOzjo1yJmU10/Z99onSfGR9t+KkyE44P75iZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlpOfQlTZL0kKRvpdsnSdok6bF0PbVm2xWSeiXtlnReq/s2M7NyRmOmfwWwq+b2VcDmiJgDbE63kTQX6AJOAxYC10maNAr7NzOzEWop9CV1AG8Crq9pXgysTctrgYtq2tdFxMGI2AP0AvNa2b+ZmZXT6kz/C8BHgWdq2k6OiH6AdD09tbcD+2q260ttzyJpqaQeST2Dg4MtlmhmZkdUDn1JFwIDEbF1pF0atEWjDSNidUR0RkRnW1tb1RLNzKxOK9+c9RpgkaQLgBcAJ0j6OnBA0oyI6Jc0AxhI2/cBM2v6dwD7W9i/mZmVVHmmHxErIqIjImZRvED73Yh4J7ABWJI2WwKsT8sbgC5JUyTNBuYAWypXbmZmpY3Fd+ReC3RLugzYC1wCEBE7JHUDO4FDwPKIODwG+zczsyGMSuhHxD3APWn5P4EFQ2y3Elg5Gvs0M7Py/I5cM7OMOPTNzDLi0Dczy8hYvJBrGfhkd/mPTvrk2+4eg0rMrAzP9M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLSOXQlzRT0vck7ZK0Q9IVqf0kSZskPZaup9b0WSGpV9JuSeU/m9fMzFrSykz/EPAXEfEKYD6wXNJc4Cpgc0TMATan26R1XcBpwELgOkmTWinezMzKqfwlKhHRD/Sn5ack7QLagcXAOWmztRRfmP6x1L4uIg4CeyT1AvOA+6rWYDYRXHjbzaX7fOut7xiDSsyaG5Vj+pJmAWcADwAnpyeEI08M09Nm7cC+mm59qa3ReEsl9UjqGRwcHI0SzcyMUQh9SS8EbgeujIifDbdpg7ZotGFErI6IzojobGtra7VEMzNLWgp9Sc+nCPybI+KO1HxA0oy0fgYwkNr7gJk13TuA/a3s38zMymnl7B0BNwC7IuJvalZtAJak5SXA+pr2LklTJM0G5gBbqu7fzMzKq/xCLvAa4F3ANkkPp7a/BK4FuiVdBuwFLgGIiB2SuoGdFGf+LI+Iwy3s38zMSmrl7J1/o/FxeoAFQ/RZCaysuk8zM2uN35FrZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUbGPfQlLZS0W1KvpKvGe/9mZjkb19CXNAn4MnA+MBe4VNLc8azBzCxn4z3Tnwf0RsR/RMTTwDpg8TjXYGaWLUXE+O1MeiuwMCLel26/C/j9iLi8brulwNJ081Rg9zDDTgN+0mJpE2GMiVDDRBljItQwGmNMhBomyhgToYaJMsZ41fBbEdFW3zi5xR2XpQZtz3rWiYjVwOoRDSj1RERnS0VNgDEmQg0TZYyJUMNojDERapgoY0yEGibKGM91DeN9eKcPmFlzuwPYP841mJlla7xD/9+BOZJmSzoG6AI2jHMNZmbZGtfDOxFxSNLlwN3AJGBNROxocdgRHQY6CsaYCDVMlDEmQg2jMcZEqGGijDERapgoYzynNYzrC7lmZvbc8jtyzcwy4tA3M8vIUR36rX6kg6Q1kgYkba+4/5mSvidpl6Qdkq6oMMYLJG2R9Ega41MVa5kk6SFJ36rY/3FJ2yQ9LKmn4hgnSrpN0qPpZ/IHJfufmvZ/5PIzSVeWHOND6ee4XdItkl5Q7l6ApCtS/x0j3X+jx5KkkyRtkvRYup5aYYxLUh3PSGp6it4QY3wu/U5+IOlOSSeW7P/p1PdhSRslvaRsDTXrPiwpJE2rcD8+KemJmsfHBVXqkPSBlBs7JH22ZA231uz/cUkPV7gfr5J0/5G/NUnzKozxu5LuS3+z35R0wnBj/IqIOCovFC8E/wh4GXAM8Agwt+QYZwNnAtsr1jADODMtvwj4YYUaBLwwLT8feACYX6GWPwf+EfhWxfvyODCtxd/JWuB9afkY4MQWf79PUrzBZKR92oE9wLHpdjfwJyX3ezqwHTiO4kSH7wBzqjyWgM8CV6Xlq4C/qjDGKyjeoHgP0FmxjjcCk9PyXw1XxxD9T6hZ/iDwlbI1pPaZFCdx/LjZY22IOj4JfLjE77LRGH+YfqdT0u3pZe9Hzfq/Bj5RoYaNwPlp+QLgngpj/Dvw+rT8XuDTI/25HM0z/ZY/0iEi7gV+WrWAiOiPiAfT8lPALorgKTNGRMT/ppvPT5dSr65L6gDeBFxfpt9oSjONs4EbACLi6Yj47xaGXAD8KCJ+XLLfZOBYSZMpgrvs+0BeAdwfEb+IiEPA94GLm3Ua4rG0mOKJkHR9UdkxImJXRAz3jvSRjLEx3ReA+yneH1Om/89qbh5Pk8fnMH9Xfwt8tFn/JmOM2BBjLAOujYiDaZuBKjVIEvA24JYKNQRwZGb+GzR5jA4xxqnAvWl5E/CW4caodTSHfjuwr+Z2HyUDdzRJmgWcQTFTL9t3Uvo3cQDYFBFlx/gCxR/TM2X3XSOAjZK2qvgYjLJeBgwCX02Hma6XdHwL9XTR5A+qXkQ8AXwe2Av0A/8TERtL7nc7cLak35R0HMVMbGaTPkM5OSL6U239wPSK44ym9wJ3le0kaaWkfcA7gE9U6L8IeCIiHinbt87l6VDTmmaHy4ZwCvA6SQ9I+r6k36tYx+uAAxHxWIW+VwKfSz/PzwMrKoyxHViUli+hxGP0aA79EX2kw3iQ9ELgduDKulnRiETE4Yh4FcUMbJ6k00vs+0JgICK2lt1vnddExJkUn4C6XNLZJftPpvgXdFVEnAH8nOKQRmkq3ri3CPinkv2mUsyuZwMvAY6X9M4yY0TELopDIJuAb1McNjw0bKejhKSrKe7LzWX7RsTVETEz9b282fZ1+z0OuJoKTxZ1VgEvB15F8aT+1xXGmAxMBeYDHwG606y9rEspOSmpsQz4UPp5foj033FJ76X4O91KcWj56ZF2PJpDf0J8pIOk51ME/s0RcUcrY6XDIfcAC0t0ew2wSNLjFIe4zpX09Qr73p+uB4A7KQ6fldEH9NX8l3IbxZNAFecDD0bEgZL93gDsiYjBiPglcAfw6rI7j4gbIuLMiDib4t/qKrM5gAOSZgCk6yEPJYw1SUuAC4F3RDoQXNE/UuJQQvJyiifiR9LjtAN4UNKLywwSEQfSBOkZ4B8o/xiF4nF6RzqsuoXiv+NhX1Sulw4dvhm4tcL+AZZQPDahmNiUvh8R8WhEvDEizqJ48vnRSPsezaH/nH+kQ5oh3ADsioi/qThG25GzKSQdSxFcj460f0SsiIiOiJhF8TP4bkSUmt1KOl7Si44sU7zwV+qMpoh4Etgn6dTUtADYWWaMGlVnUXuB+ZKOS7+bBRSvs5QiaXq6finFH3fVGd0Gij9w0vX6iuO0RNJC4GPAooj4RYX+c2puLqLE4xMgIrZFxPSImJUep30UJ0A8WbKOGTU3L6bkYzT5BnBuGu8UihMOyn7i5RuARyOir8L+oZicvj4tn0uFSUXNY/R5wMeBr4y480hf8Z2IF4rjrT+keJa7ukL/Wyj+TfwlxQPxspL9X0txSOkHwMPpckHJMV4JPJTG2E6TswGajHUOFc7eoTge/0i67Kjys0zjvAroSfflG8DUCmMcB/wn8BsVa/gURShtB75GOkuj5Bj/SvGE9QiwoOpjCfhNYDPFH/Vm4KQKY1yclg8CB4C7K4zRS/H615HH6JBn3wzR//b08/wB8E2gvWwNdesfp/nZO43q+BqwLdWxAZhRYYxjgK+n+/MgcG7Z+wHcCLy/hcfFa4Gt6fH1AHBWhTGuoMi+HwLXkj5dYSQXfwyDmVlGjubDO2ZmVpJD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OM/D/rw2/NdOVFGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   fragment_id  16000 non-null  int64\n",
      " 1   behavior_id  16000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 250.1 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t[:,:20], axis=1)\n",
    "oof_y = np.argmax(proba_oof[:,:20], axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../data/'\n",
    "sub = pd.read_csv(data_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "sub.to_csv('LSTM5fold%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:04:26.093831Z",
     "start_time": "2020-07-28T02:04:25.118669Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm,classes,title='Confusion Matrix'):\n",
    "\n",
    "    plt.figure(figsize=(12, 9), dpi=100)\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    sns.heatmap(cm,annot=True)\n",
    "    plt.title(title)\n",
    "    plt.xticks(ticks=range(19),labels=classes)\n",
    "    plt.yticks(ticks=range(19),labels=classes)\n",
    "    \n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predict label')\n",
    "    plt.show()\n",
    "    \n",
    "# classes表示不同类别的名称，比如这有6个类别\n",
    "num2detail_mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "\n",
    "classes = [num2detail_mapping[int(i)]for i in range(19)]\n",
    "print(classes)\n",
    "# 获取混淆矩阵\n",
    "cm = confusion_matrix(train_y, oof_y,normalize='true')\n",
    "cm = np.round(cm,2)\n",
    "plot_confusion_matrix(cm,classes, title='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:44:35.740664Z",
     "start_time": "2020-07-11T12:44:31.939200Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    sns.distplot(proba_test[i],label=num2detail_mapping[i])\n",
    "plt.xlim([0,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:47:07.814637Z",
     "start_time": "2020-07-11T12:47:07.753317Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:50:03.709385Z",
     "start_time": "2020-07-11T12:50:03.623527Z"
    }
   },
   "outputs": [],
   "source": [
    "class0_distribution = list(filter(lambda x:x>0.8,proba_test[1]))\n",
    "sum(sum(proba_test > 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proba_test = np.zeros((7500, 19))\n",
    "best_fold = [6,10,14,13]\n",
    "for fold in best_fold:\n",
    "    model = Net()\n",
    "    model.load_weights(f'fold{fold}.h5')\n",
    "    proba_test += model.predict(test, verbose=0, batch_size=1024) / len(best_fold)\n",
    "    \n",
    "labels = np.argmax(proba_test, axis=1)\n",
    "data_path = '../../data/'\n",
    "sub = pd.read_csv(data_path+'提交结果示例.csv')\n",
    "sub['behavior_id'] = labels\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "scores = np.mean(np.array(oof_comm)[best_fold])\n",
    "print(scores)\n",
    "sub.to_csv('nn%.5f.csv' % scores, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T08:20:14.945162Z",
     "start_time": "2020-07-21T08:20:14.895692Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.load('0721_conv2_2_net_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_2.1]",
   "language": "python",
   "name": "conda-env-tf_2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
