{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T07:32:14.837685Z",
     "start_time": "2020-07-10T07:32:14.833926Z"
    }
   },
   "source": [
    "# 多个loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:39:09.609191Z",
     "start_time": "2020-08-20T15:39:09.352741Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 21 13:48:04 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.92       Driver Version: 410.92       CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:0D.0 Off |                  Off |\r\n",
      "| N/A   55C    P0    88W / 250W |   7892MiB / 16130MiB |     60%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     24386      C   ..._04/anaconda3/envs/tf2_torch/bin/python  7881MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:37.624345Z",
     "start_time": "2020-08-20T15:45:36.222448Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:38.057090Z",
     "start_time": "2020-08-20T15:45:37.625949Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "# def acc_combo(y, y_pred):\n",
    "#     # 数值ID与行为编码的对应关系\n",
    "#     mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "#         4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "#         8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "#         12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "#         16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "#     # 将行为ID转为编码\n",
    "#     code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "#     if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "#         return 1.0\n",
    "#     elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "#         return 1.0/7\n",
    "#     elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "#         return 1.0/3\n",
    "#     else:\n",
    "#         return 0.0\n",
    "\n",
    "\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:38.213844Z",
     "start_time": "2020-08-20T15:45:38.058481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: 无法访问../../zp: 没有那个文件或目录\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:38.683055Z",
     "start_time": "2020-08-20T15:45:38.218062Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../data/final_data/'\n",
    "data_train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "data_test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = data_train.groupby('fragment_id')['behavior_id'].min()\n",
    "# data_test['fragment_id'] += 100000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:38.719933Z",
     "start_time": "2020-08-20T15:45:38.684452Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "    df['thetax']=np.arctan(df.acc_xg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetay']=np.arctan(df.acc_yg/\n",
    "                           np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetaz']=np.arctan(df.acc_zg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "    df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "    df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:38.967906Z",
     "start_time": "2020-08-20T15:45:38.799257Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay', 'thetaz', 'xy',\n",
      "       'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train=add_features(data_train)\n",
    "test=add_features(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:39.580643Z",
     "start_time": "2020-08-20T15:45:39.538346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'acc_xg',\n",
       " 'acc_yg',\n",
       " 'acc_zg',\n",
       " 'acc',\n",
       " 'accg',\n",
       " 'thetax',\n",
       " 'thetay',\n",
       " 'thetaz',\n",
       " 'xy',\n",
       " 'xy_g',\n",
       " 'g']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:40.211080Z",
     "start_time": "2020-08-20T15:45:40.173810Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:45:40.802723Z",
     "start_time": "2020-08-20T15:45:40.762267Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import resample\n",
    "def get_fft_values(y_values, N, f_s):\n",
    "    f_values = np.linspace(0.0, f_s/2.0, N//2)\n",
    "    fft_values_ = fft(y_values)\n",
    "    plt.plot(fft_values_)\n",
    "    plt.show()\n",
    "    print(fft_values_.shape)\n",
    "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "    print(fft_values.shape)\n",
    "    return f_values, fft_values\n",
    "\n",
    "# tmp = train[train.fragment_id == 0][:sample_num]\n",
    "\n",
    "# get_fft_values(tmp[\"acc\"].values,60,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:46:14.424480Z",
     "start_time": "2020-08-20T15:45:41.421383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:26<00:00, 576.84it/s]\n",
      "100%|██████████| 16000/16000 [00:27<00:00, 576.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test['fragment_id'] += 100000\n",
    "# data = pd.concat([train, test], sort=False)\n",
    "# ss_tool = StandardScaler()\n",
    "# data[group1] = ss_tool.fit_transform(data[group1])\n",
    "# train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "# test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "# test['fragment_id'] -= 100000                         \n",
    "# train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "# test = test[['fragment_id', 'time_point']+group1]\n",
    "# print(train.columns)\n",
    "\n",
    "for i in tqdm(range(15000)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(16000)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:47:26.036261Z",
     "start_time": "2020-08-20T16:47:25.987463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from tensorflow.python.keras import backend as K\n",
    "# from tensorflow.python.util.tf_export import keras_export\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     \"\"\"Learning Rate Schedule\n",
    "#     Learning rate is scheduled to be reduced after 20, 30 epochs.\n",
    "#     Called automatically every epoch as part of callbacks during training.\n",
    "#     # Arguments\n",
    "#         epoch (int): The number of epochs\n",
    "#     # Returns\n",
    "#         lr (float32): learning rate\n",
    "#     \"\"\"\n",
    "#     lr = 5e-4\n",
    "#     if epoch >= 150:\n",
    "#         lr *= 1e-1\n",
    "#     elif epoch >= 100:\n",
    "#         lr *= 1e-1\n",
    "#     elif epoch >= 50:\n",
    "#         lr *= 1e-1\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "# def wd_schedule(epoch):\n",
    "#     \"\"\"Weight Decay Schedule\n",
    "#     Weight decay is scheduled to be reduced after 20, 30 epochs.\n",
    "#     Called automatically every epoch as part of callbacks during training.\n",
    "#     # Arguments\n",
    "#         epoch (int): The number of epochs\n",
    "#     # Returns\n",
    "#         wd (float32): weight decay\n",
    "#     \"\"\"\n",
    "#     wd = 2e-5\n",
    "\n",
    "#     if epoch >= 100:\n",
    "#         wd *= 2e-2\n",
    "#     elif epoch >= 50:\n",
    "#         wd *= 1e-1\n",
    "#     print('Weight decay: ', wd)\n",
    "#     return wd\n",
    "\n",
    "\n",
    "# # just copy the implement of LearningRateScheduler, and then change the lr with weight_decay\n",
    "# @keras_export('keras.callbacks.WeightDecayScheduler')\n",
    "# class WeightDecayScheduler(Callback):\n",
    "#     \"\"\"Weight Decay Scheduler.\n",
    "\n",
    "#     Arguments:\n",
    "#         schedule: a function that takes an epoch index as input\n",
    "#             (integer, indexed from 0) and returns a new\n",
    "#             weight decay as output (float).\n",
    "#         verbose: int. 0: quiet, 1: update messages.\n",
    "\n",
    "#     ```python\n",
    "#     # This function keeps the weight decay at 0.001 for the first ten epochs\n",
    "#     # and decreases it exponentially after that.\n",
    "#     def scheduler(epoch):\n",
    "#       if epoch < 10:\n",
    "#         return 0.001\n",
    "#       else:\n",
    "#         return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "#     callback = WeightDecayScheduler(scheduler)\n",
    "#     model.fit(data, labels, epochs=100, callbacks=[callback],\n",
    "#               validation_data=(val_data, val_labels))\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, schedule, verbose=0):\n",
    "#         super(WeightDecayScheduler, self).__init__()\n",
    "#         self.schedule = schedule\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         if not hasattr(self.model.optimizer, 'weight_decay'):\n",
    "#             raise ValueError('Optimizer must have a \"weight_decay\" attribute.')\n",
    "#         try:  # new API\n",
    "#             weight_decay = float(K.get_value(self.model.optimizer.weight_decay))\n",
    "#             weight_decay = self.schedule(epoch, weight_decay)\n",
    "#         except TypeError:  # Support for old API for backward compatibility\n",
    "#             weight_decay = self.schedule(epoch)\n",
    "#         if not isinstance(weight_decay, (float, np.float32, np.float64)):\n",
    "#             raise ValueError('The output of the \"schedule\" function '\n",
    "#                              'should be float.')\n",
    "#         K.set_value(self.model.optimizer.weight_decay, weight_decay)\n",
    "#         if self.verbose > 0:\n",
    "#             print('\\nEpoch %05d: WeightDecayScheduler reducing weight '\n",
    "#                   'decay to %s.' % (epoch + 1, weight_decay))\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         logs['weight_decay'] = K.get_value(self.model.optimizer.weight_decay)\n",
    "\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# #     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "# #     gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# #     for gpu in gpus:\n",
    "# #         tf.config.experimental.set_memory_growth(gpu, enable=True)\n",
    "# #     print(gpus)\n",
    "# #     cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "# #     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# #     x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# #     model = tf.keras.models.Sequential([\n",
    "# #         tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "# #         tf.keras.layers.AveragePooling2D(),\n",
    "# #         tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "# #         tf.keras.layers.AveragePooling2D(),\n",
    "# #         tf.keras.layers.Flatten(),\n",
    "# #         tf.keras.layers.Dense(10, activation='softmax')\n",
    "# #     ])\n",
    "\n",
    "# #     optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "# #     # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# #     tb_callback = tf.keras.callbacks.TensorBoard(os.path.join('logs', 'adamw'),\n",
    "# #                                                  profile_batch=0)\n",
    "# #     lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "# #     wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "\n",
    "# #     model.compile(optimizer=optimizer,\n",
    "# #                   loss='sparse_categorical_crossentropy',\n",
    "# #                   metrics=['accuracy'])\n",
    "\n",
    "# #     model.fit(x_train, y_train, epochs=40, validation_split=0.1,\n",
    "# #               callbacks=[tb_callback, lr_callback, wd_callback])\n",
    "\n",
    "# #     model.evaluate(x_test, y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:47:44.511564Z",
     "start_time": "2020-08-20T16:47:43.989848Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ConvBNRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "#                activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def ConvRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "               activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def squeeze_excitation_layer(x, out_dim,ratio=8):\n",
    "    '''\n",
    "    SE module performs inter-channel weighting.\n",
    "    '''\n",
    "    squeeze = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
    "    excitation = Activation('relu')(excitation)\n",
    "    excitation = Dense(units=out_dim)(excitation)\n",
    "    excitation = Activation('sigmoid')(excitation)\n",
    "    excitation = Reshape((1,1,out_dim))(excitation)\n",
    "    scale = multiply([x,excitation])\n",
    "    return scale\n",
    "\n",
    "# def SE_Residual(X):\n",
    "#     A = \n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "#     X =  Add()([X,A])\n",
    "    \n",
    "\n",
    "def lenet5(input):\n",
    "    A = ConvBNRelu(input,64,kernal_size=(3,3))\n",
    "#     B = ConvBNRelu(input,16,kernal_size=(5,1))\n",
    "#     C = ConvBNRelu(input,16,kernal_size=(7,1))\n",
    "#     ABC = layers.Concatenate()([A,B,C])\n",
    "    X = ConvBNRelu(A,128)\n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    X = AveragePooling2D()(X)\n",
    "    \n",
    "    X = ConvBNRelu(X,256)\n",
    "    X = Dropout(0.3)(X)\n",
    "#     X = squeeze_excitation_layer(X,256)\n",
    "    X = ConvBNRelu(X,512)   \n",
    "    X = Dropout(0.5)(X)\n",
    "#     X = squeeze_excitation_layer(X,512)\n",
    "#     X = GlobalMaxPooling2D()(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "#     X = BatchNormalization()(X)\n",
    "    return X\n",
    "import tensorflow as tf\n",
    "def Net(sample_num):\n",
    "    input1 = Input(shape=(sample_num, FEATURE_NUM, 1))\n",
    "    part = tf.split(input1,axis=2, num_or_size_splits = [6, 2, 6])\n",
    "#     res = tf.split(c, axis = 3, num_or_size_splits = [2, 2, 4])\n",
    "    \n",
    "    \n",
    "    X1 = Concatenate(axis=-2)([part[0],part[1]])\n",
    "    X1 = lenet5(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dense(128, activation='relu')(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "\n",
    "    X2 = Concatenate(axis=-2)([part[0],part[2]])\n",
    "    X2 = lenet5(X2)    \n",
    "    X2 = BatchNormalization()(X2)\n",
    "#     X = Dense(512, activation='relu')(X)\n",
    "#     X = BatchNormalization()(X)\n",
    "    X2 = Dense(128, activation='relu')(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    \n",
    "    X = Concatenate(axis=-1)([X1,X2])\n",
    "    \n",
    "#     X = Dense(256)(X)    \n",
    "    \n",
    "    output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "    X = Dense(64)(X)\n",
    "    output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='19class')(X) #小类\n",
    "    \n",
    "    \n",
    "    return Model([input1], output3)\n",
    "\n",
    "# model = Net(30)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T16:47:49.677446Z",
     "start_time": "2020-08-20T16:47:48.736343Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team_04/anaconda3/envs/tf2_torch/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19], y=0         11\n",
      "1         11\n",
      "2         11\n",
      "3         11\n",
      "4         11\n",
      "          ..\n",
      "391789    16\n",
      "391790    16\n",
      "391791    16\n",
      "391792    16\n",
      "391793    16\n",
      "Name: behavior_id, Length: 391794, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 两个输出    \n",
    "\n",
    "# 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# # y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id'])\n",
    "# classweights1=compute_class_weight(\"balanced\",['A','B','C','D'],\\\n",
    "#                                    pd.read_csv(root_path+'sensor_train.csv')['behavior_id'].apply(lambda x:mapping[x][0]))\n",
    "# classweights1=pd.DataFrame(classweights1)[0].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "# classweights2=compute_class_weight(\"balanced\",list(range(7)),\\\n",
    "#                                    pd.read_csv(root_path+'sensor_train.csv')['behavior_id'].apply(lambda x:int(mapping[x][2])))\n",
    "# classweights2=pd.DataFrame(classweights2)[0].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id'])\n",
    "classweights3=compute_class_weight(\"balanced\",np.array(range(20)), pd.read_csv(root_path+'sensor_train_final.csv')['behavior_id'])\n",
    "classweights3=pd.DataFrame(classweights3)[0].to_dict()\n",
    "# classweights1,classweights2,classweights3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-20T16:47:48.425Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 60, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_9 (TensorFlow [(None, 60, 6, 1), ( 0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 60, 8, 1)     0           tf_op_layer_split_9[0][0]        \n",
      "                                                                 tf_op_layer_split_9[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 60, 12, 1)    0           tf_op_layer_split_9[0][0]        \n",
      "                                                                 tf_op_layer_split_9[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 60, 8, 64)    576         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 60, 12, 64)   576         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 60, 8, 64)    256         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 60, 12, 64)   256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 60, 8, 64)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 60, 12, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 60, 8, 128)   73728       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 60, 12, 128)  73728       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 60, 8, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 60, 12, 128)  512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 60, 8, 128)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 60, 12, 128)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 60, 8, 128)   0           activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 60, 12, 128)  0           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 30, 4, 128)   0           dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 30, 6, 128)   0           dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 30, 4, 256)   294912      average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 30, 6, 256)   294912      average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 30, 4, 256)   1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 30, 6, 256)   1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 30, 4, 256)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 30, 6, 256)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 30, 4, 256)   0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 30, 6, 256)   0           activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 30, 4, 512)   1179648     dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 30, 6, 512)   1179648     dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 30, 4, 512)   2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 30, 6, 512)   2048        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 30, 4, 512)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 30, 6, 512)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 30, 4, 512)   0           activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 30, 6, 512)   0           activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 512)          0           dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_19 (Gl (None, 512)          0           dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 512)          2048        global_average_pooling2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 512)          2048        global_average_pooling2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 128)          65664       batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          65664       batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 128)          512         dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 128)          512         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 128)          0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 128)          0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 256)          0           dropout_75[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 64)           16448       concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 64)           4160        dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,263,764\n",
      "Trainable params: 3,257,364\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 8s 625us/sample - loss: 2.0533 - acc: 0.3885 - val_loss: 1.9765 - val_acc: 0.3993\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.7877 - acc: 0.4559 - val_loss: 2.1563 - val_acc: 0.3227\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 1.6866 - acc: 0.4952 - val_loss: 2.1520 - val_acc: 0.3960\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 1.6152 - acc: 0.5182 - val_loss: 1.6970 - val_acc: 0.5093\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 1.5668 - acc: 0.5335 - val_loss: 1.9073 - val_acc: 0.4677\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 1.5105 - acc: 0.5541 - val_loss: 1.6712 - val_acc: 0.5170\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.4718 - acc: 0.5717 - val_loss: 1.5911 - val_acc: 0.5330\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 1.4518 - acc: 0.5792 - val_loss: 1.6401 - val_acc: 0.5333\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 1.4092 - acc: 0.5894 - val_loss: 1.8382 - val_acc: 0.4893\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.3909 - acc: 0.5994 - val_loss: 1.4619 - val_acc: 0.5987\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 1.3714 - acc: 0.6050 - val_loss: 1.3947 - val_acc: 0.6057\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 1.3428 - acc: 0.6204 - val_loss: 1.4258 - val_acc: 0.5900\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 1.3143 - acc: 0.6332 - val_loss: 1.4229 - val_acc: 0.6063\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.3016 - acc: 0.6367 - val_loss: 1.3791 - val_acc: 0.6200\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 1.2828 - acc: 0.6420 - val_loss: 1.3647 - val_acc: 0.6227\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.2625 - acc: 0.6523 - val_loss: 1.5720 - val_acc: 0.5560\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.2407 - acc: 0.6611 - val_loss: 1.2635 - val_acc: 0.6543\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 1.2196 - acc: 0.6684 - val_loss: 1.4772 - val_acc: 0.5897\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 1.2171 - acc: 0.6648 - val_loss: 1.2204 - val_acc: 0.6677\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 1.1915 - acc: 0.6754 - val_loss: 1.2729 - val_acc: 0.6677\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.1905 - acc: 0.6796 - val_loss: 1.5755 - val_acc: 0.5793\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.1666 - acc: 0.6878 - val_loss: 1.3026 - val_acc: 0.6450\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 1.1572 - acc: 0.6922 - val_loss: 1.2539 - val_acc: 0.6603\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.1463 - acc: 0.7009 - val_loss: 1.2400 - val_acc: 0.6653\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 1.1293 - acc: 0.6981 - val_loss: 1.2383 - val_acc: 0.6697\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 1.1211 - acc: 0.7076 - val_loss: 1.2420 - val_acc: 0.6700\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.1121 - acc: 0.7074 - val_loss: 1.2259 - val_acc: 0.6707\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.0981 - acc: 0.7107 - val_loss: 1.3948 - val_acc: 0.6303\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 1.0926 - acc: 0.7198 - val_loss: 1.1861 - val_acc: 0.6983\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.0740 - acc: 0.7261 - val_loss: 1.2439 - val_acc: 0.6710\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.0640 - acc: 0.7280 - val_loss: 1.2386 - val_acc: 0.6660\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.0558 - acc: 0.7315 - val_loss: 1.4037 - val_acc: 0.6297\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.0563 - acc: 0.7321 - val_loss: 1.1286 - val_acc: 0.7170\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 1.0367 - acc: 0.7405 - val_loss: 1.3836 - val_acc: 0.6483\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.0377 - acc: 0.7367 - val_loss: 1.2945 - val_acc: 0.6617\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 1.0166 - acc: 0.7462 - val_loss: 1.1450 - val_acc: 0.7073\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 1.0101 - acc: 0.7527 - val_loss: 1.0676 - val_acc: 0.7350\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.9921 - acc: 0.7543 - val_loss: 1.0565 - val_acc: 0.7413\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.9943 - acc: 0.7611 - val_loss: 1.0967 - val_acc: 0.7210\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.9941 - acc: 0.7548 - val_loss: 1.0776 - val_acc: 0.7337\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.9789 - acc: 0.7632 - val_loss: 1.0815 - val_acc: 0.7257\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.9806 - acc: 0.7608 - val_loss: 1.3992 - val_acc: 0.6407\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.9630 - acc: 0.7670 - val_loss: 1.0843 - val_acc: 0.7363\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9628 - acc: 0.7659 - val_loss: 1.1841 - val_acc: 0.7053\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.9552 - acc: 0.7705 - val_loss: 1.0909 - val_acc: 0.7247\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9372 - acc: 0.7796 - val_loss: 1.0528 - val_acc: 0.7410\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9317 - acc: 0.7796 - val_loss: 1.1010 - val_acc: 0.7303\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9293 - acc: 0.7789 - val_loss: 1.3030 - val_acc: 0.6767\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.9286 - acc: 0.7862 - val_loss: 1.0356 - val_acc: 0.7543\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.9033 - acc: 0.7905 - val_loss: 1.1303 - val_acc: 0.7207\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.9212 - acc: 0.7858 - val_loss: 1.0845 - val_acc: 0.7390\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.9080 - acc: 0.7899 - val_loss: 1.1010 - val_acc: 0.7343\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.8932 - acc: 0.8000 - val_loss: 1.4609 - val_acc: 0.6483\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.8948 - acc: 0.7947 - val_loss: 1.1429 - val_acc: 0.7107\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8859 - acc: 0.7956 - val_loss: 1.0744 - val_acc: 0.7477\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.8823 - acc: 0.8013 - val_loss: 1.0036 - val_acc: 0.7697\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8852 - acc: 0.7982 - val_loss: 1.0445 - val_acc: 0.7550\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8748 - acc: 0.8023 - val_loss: 1.4125 - val_acc: 0.6567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8725 - acc: 0.8059 - val_loss: 1.0966 - val_acc: 0.7293\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.8585 - acc: 0.8116 - val_loss: 1.2787 - val_acc: 0.6917\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.8446 - acc: 0.8164 - val_loss: 1.0136 - val_acc: 0.7700\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.8500 - acc: 0.8092 - val_loss: 1.0262 - val_acc: 0.7633\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.8444 - acc: 0.8172 - val_loss: 1.0055 - val_acc: 0.7727\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.8433 - acc: 0.8167 - val_loss: 1.2041 - val_acc: 0.7083\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8357 - acc: 0.8195 - val_loss: 1.1694 - val_acc: 0.7160\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.8361 - acc: 0.8187 - val_loss: 1.0098 - val_acc: 0.7647\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8255 - acc: 0.8209 - val_loss: 1.0693 - val_acc: 0.7437\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.8152 - acc: 0.8301 - val_loss: 1.0682 - val_acc: 0.7487\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.8177 - acc: 0.8277 - val_loss: 1.0613 - val_acc: 0.7547\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.8120 - acc: 0.8284 - val_loss: 1.0083 - val_acc: 0.7773\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8011 - acc: 0.8347 - val_loss: 1.0810 - val_acc: 0.7540\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7966 - acc: 0.8367 - val_loss: 1.0713 - val_acc: 0.7613\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7924 - acc: 0.8409 - val_loss: 1.0100 - val_acc: 0.7687\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.7883 - acc: 0.8422 - val_loss: 1.0122 - val_acc: 0.7703\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.7835 - acc: 0.8422 - val_loss: 1.0508 - val_acc: 0.7640\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7814 - acc: 0.8433 - val_loss: 1.1279 - val_acc: 0.7407\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.7844 - acc: 0.8461 - val_loss: 1.0250 - val_acc: 0.7700\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.7804 - acc: 0.8450 - val_loss: 1.1514 - val_acc: 0.7270\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.7630 - acc: 0.8531 - val_loss: 1.2333 - val_acc: 0.7103\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.7745 - acc: 0.8478 - val_loss: 0.9906 - val_acc: 0.7833\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7554 - acc: 0.8528 - val_loss: 1.1154 - val_acc: 0.7377\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.7634 - acc: 0.8496 - val_loss: 1.0856 - val_acc: 0.7553\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.7577 - acc: 0.8542 - val_loss: 1.0419 - val_acc: 0.7650\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.7513 - acc: 0.8567 - val_loss: 1.0698 - val_acc: 0.7557\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.7531 - acc: 0.8570 - val_loss: 1.0822 - val_acc: 0.7613\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7485 - acc: 0.8587 - val_loss: 1.3701 - val_acc: 0.6733\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7369 - acc: 0.8653 - val_loss: 1.1238 - val_acc: 0.7367\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7369 - acc: 0.8622 - val_loss: 1.1037 - val_acc: 0.7473\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7361 - acc: 0.8614 - val_loss: 1.2566 - val_acc: 0.7060\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7255 - acc: 0.8670 - val_loss: 1.0671 - val_acc: 0.7610\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.7318 - acc: 0.8662 - val_loss: 1.0072 - val_acc: 0.7873\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7218 - acc: 0.8709 - val_loss: 1.0209 - val_acc: 0.7800\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.7122 - acc: 0.8709 - val_loss: 1.0198 - val_acc: 0.7807\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.7179 - acc: 0.8677 - val_loss: 1.0667 - val_acc: 0.7667\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.7145 - acc: 0.8727 - val_loss: 1.0483 - val_acc: 0.7693\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.7051 - acc: 0.8799 - val_loss: 1.1056 - val_acc: 0.7543\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.7048 - acc: 0.8748 - val_loss: 1.0042 - val_acc: 0.7920\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7043 - acc: 0.8756 - val_loss: 1.0672 - val_acc: 0.7770\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.6992 - acc: 0.8773 - val_loss: 0.9933 - val_acc: 0.7963\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6974 - acc: 0.8829 - val_loss: 1.0583 - val_acc: 0.7647\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6874 - acc: 0.8846 - val_loss: 1.0614 - val_acc: 0.7700\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6942 - acc: 0.8819 - val_loss: 1.0512 - val_acc: 0.7737\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6816 - acc: 0.8834 - val_loss: 1.0081 - val_acc: 0.7883\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6696 - acc: 0.8901 - val_loss: 1.1717 - val_acc: 0.7393\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6780 - acc: 0.8897 - val_loss: 0.9967 - val_acc: 0.7923\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6717 - acc: 0.8898 - val_loss: 0.9972 - val_acc: 0.7947\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6832 - acc: 0.8831 - val_loss: 1.0207 - val_acc: 0.7827\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6641 - acc: 0.8942 - val_loss: 1.0366 - val_acc: 0.7763\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6706 - acc: 0.8933 - val_loss: 1.0791 - val_acc: 0.7643\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6667 - acc: 0.8936 - val_loss: 1.0041 - val_acc: 0.7947\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6658 - acc: 0.8898 - val_loss: 1.0326 - val_acc: 0.7730\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6569 - acc: 0.8972 - val_loss: 1.0050 - val_acc: 0.7847\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6582 - acc: 0.8971 - val_loss: 1.0516 - val_acc: 0.7840\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6518 - acc: 0.8975 - val_loss: 1.0152 - val_acc: 0.7850\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6423 - acc: 0.9054 - val_loss: 1.0215 - val_acc: 0.7870\n",
      "Epoch 116/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6482 - acc: 0.9020 - val_loss: 1.1081 - val_acc: 0.7503\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6432 - acc: 0.9012 - val_loss: 1.7119 - val_acc: 0.6330\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6467 - acc: 0.9010 - val_loss: 1.0356 - val_acc: 0.7907\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6412 - acc: 0.9042 - val_loss: 1.0913 - val_acc: 0.7677\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6401 - acc: 0.9068 - val_loss: 1.0071 - val_acc: 0.7860\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6378 - acc: 0.9062 - val_loss: 1.0094 - val_acc: 0.7940\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6367 - acc: 0.9050 - val_loss: 0.9958 - val_acc: 0.7910\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6280 - acc: 0.9091 - val_loss: 1.0923 - val_acc: 0.7673\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6305 - acc: 0.9109 - val_loss: 1.0356 - val_acc: 0.7863\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6270 - acc: 0.9085 - val_loss: 1.0067 - val_acc: 0.7950\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6270 - acc: 0.9086 - val_loss: 1.5884 - val_acc: 0.6557\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6275 - acc: 0.9122 - val_loss: 1.0636 - val_acc: 0.7703\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6252 - acc: 0.9098 - val_loss: 0.9839 - val_acc: 0.7957\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6161 - acc: 0.9157 - val_loss: 1.0142 - val_acc: 0.7890\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6170 - acc: 0.9136 - val_loss: 1.0813 - val_acc: 0.7703\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6199 - acc: 0.9117 - val_loss: 1.1541 - val_acc: 0.7520\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6169 - acc: 0.9135 - val_loss: 1.0933 - val_acc: 0.7650\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6057 - acc: 0.9191 - val_loss: 1.0888 - val_acc: 0.7680\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.6078 - acc: 0.9186 - val_loss: 0.9681 - val_acc: 0.8057\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6047 - acc: 0.9180 - val_loss: 1.0550 - val_acc: 0.7790\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6052 - acc: 0.9211 - val_loss: 1.0463 - val_acc: 0.7867\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6073 - acc: 0.9174 - val_loss: 1.0378 - val_acc: 0.7873\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6007 - acc: 0.9218 - val_loss: 0.9810 - val_acc: 0.8007\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6043 - acc: 0.9190 - val_loss: 1.1442 - val_acc: 0.7487\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5920 - acc: 0.9263 - val_loss: 1.0427 - val_acc: 0.7897\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6002 - acc: 0.9200 - val_loss: 0.9918 - val_acc: 0.7973\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5922 - acc: 0.9250 - val_loss: 1.0423 - val_acc: 0.7850\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5932 - acc: 0.9243 - val_loss: 0.9973 - val_acc: 0.7933\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5907 - acc: 0.9246 - val_loss: 1.0939 - val_acc: 0.7583\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5826 - acc: 0.9289 - val_loss: 1.3725 - val_acc: 0.7057\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5893 - acc: 0.9252 - val_loss: 1.1069 - val_acc: 0.7710\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5845 - acc: 0.9301 - val_loss: 0.9962 - val_acc: 0.8030\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5766 - acc: 0.9324 - val_loss: 1.0115 - val_acc: 0.7963\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5818 - acc: 0.9287 - val_loss: 1.0004 - val_acc: 0.7947\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5781 - acc: 0.9337 - val_loss: 1.1772 - val_acc: 0.7487\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5733 - acc: 0.9331 - val_loss: 1.0669 - val_acc: 0.7773\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5815 - acc: 0.9306 - val_loss: 1.0076 - val_acc: 0.7953\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5735 - acc: 0.9317 - val_loss: 1.0072 - val_acc: 0.7947\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5773 - acc: 0.9321 - val_loss: 1.1126 - val_acc: 0.7617\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5761 - acc: 0.9299 - val_loss: 0.9837 - val_acc: 0.8023\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5745 - acc: 0.9324 - val_loss: 1.0064 - val_acc: 0.8023\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5728 - acc: 0.9306 - val_loss: 1.2786 - val_acc: 0.7163\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5760 - acc: 0.9321 - val_loss: 1.0360 - val_acc: 0.7923\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5664 - acc: 0.9367 - val_loss: 1.0114 - val_acc: 0.7990\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5640 - acc: 0.9382 - val_loss: 1.0510 - val_acc: 0.7827\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5687 - acc: 0.9352 - val_loss: 1.0055 - val_acc: 0.7973\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5647 - acc: 0.9369 - val_loss: 1.9157 - val_acc: 0.6023\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5613 - acc: 0.9355 - val_loss: 1.0202 - val_acc: 0.7983\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5558 - acc: 0.9377 - val_loss: 1.0156 - val_acc: 0.7927\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5514 - acc: 0.9407 - val_loss: 1.0011 - val_acc: 0.7990\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5598 - acc: 0.9408 - val_loss: 1.0017 - val_acc: 0.7920\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5591 - acc: 0.9378 - val_loss: 0.9887 - val_acc: 0.7963\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5513 - acc: 0.9434 - val_loss: 1.0963 - val_acc: 0.7583\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5503 - acc: 0.9411 - val_loss: 1.1395 - val_acc: 0.7553\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5540 - acc: 0.9422 - val_loss: 0.9741 - val_acc: 0.8127\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5520 - acc: 0.9427 - val_loss: 1.3791 - val_acc: 0.6987\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5478 - acc: 0.9448 - val_loss: 1.0088 - val_acc: 0.7913\n",
      "Epoch 173/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5464 - acc: 0.9427 - val_loss: 1.0286 - val_acc: 0.7910\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5438 - acc: 0.9463 - val_loss: 0.9996 - val_acc: 0.8027\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5415 - acc: 0.9459 - val_loss: 0.9886 - val_acc: 0.8003\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5417 - acc: 0.9451 - val_loss: 1.0187 - val_acc: 0.7947\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5429 - acc: 0.9464 - val_loss: 1.0579 - val_acc: 0.7817\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5451 - acc: 0.9416 - val_loss: 0.9970 - val_acc: 0.8053\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5387 - acc: 0.9468 - val_loss: 1.3264 - val_acc: 0.7093\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5408 - acc: 0.9476 - val_loss: 1.0228 - val_acc: 0.7830\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5363 - acc: 0.9468 - val_loss: 1.0065 - val_acc: 0.7940\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5311 - acc: 0.9503 - val_loss: 1.0351 - val_acc: 0.7977\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5267 - acc: 0.9536 - val_loss: 1.0180 - val_acc: 0.8003\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5383 - acc: 0.9481 - val_loss: 1.0807 - val_acc: 0.7727\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.5388 - acc: 0.9486 - val_loss: 1.0084 - val_acc: 0.7997\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5322 - acc: 0.9488 - val_loss: 1.0434 - val_acc: 0.7903\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5320 - acc: 0.9507 - val_loss: 1.0184 - val_acc: 0.7910\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5272 - acc: 0.9527 - val_loss: 1.1092 - val_acc: 0.7750\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5285 - acc: 0.9503 - val_loss: 0.9943 - val_acc: 0.8023\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5221 - acc: 0.9551 - val_loss: 1.1244 - val_acc: 0.7663\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5277 - acc: 0.9523 - val_loss: 1.0444 - val_acc: 0.7897\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5221 - acc: 0.9544 - val_loss: 1.1433 - val_acc: 0.7597\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5275 - acc: 0.9498 - val_loss: 1.0641 - val_acc: 0.7857\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5255 - acc: 0.9546 - val_loss: 1.0072 - val_acc: 0.8053\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5295 - acc: 0.9507 - val_loss: 1.0042 - val_acc: 0.8000\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5207 - acc: 0.9546 - val_loss: 1.0834 - val_acc: 0.7723\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5148 - acc: 0.9562 - val_loss: 1.0391 - val_acc: 0.7843\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5177 - acc: 0.9563 - val_loss: 1.0027 - val_acc: 0.8057\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5192 - acc: 0.9543 - val_loss: 1.0059 - val_acc: 0.8037\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5183 - acc: 0.9551 - val_loss: 1.0345 - val_acc: 0.7927\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5185 - acc: 0.9560 - val_loss: 1.0569 - val_acc: 0.7873\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5129 - acc: 0.9575 - val_loss: 1.1037 - val_acc: 0.7760\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.5141 - acc: 0.9585 - val_loss: 0.9895 - val_acc: 0.8130\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5126 - acc: 0.9585 - val_loss: 0.9926 - val_acc: 0.8013\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5113 - acc: 0.9586 - val_loss: 1.0098 - val_acc: 0.7990\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5130 - acc: 0.9574 - val_loss: 0.9997 - val_acc: 0.8063\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5156 - acc: 0.9552 - val_loss: 1.0047 - val_acc: 0.8083\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5167 - acc: 0.9563 - val_loss: 1.0177 - val_acc: 0.7937\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5078 - acc: 0.9591 - val_loss: 1.0790 - val_acc: 0.7787\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5095 - acc: 0.9578 - val_loss: 1.0269 - val_acc: 0.7930\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4968 - acc: 0.9660 - val_loss: 1.0072 - val_acc: 0.8017\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5067 - acc: 0.9599 - val_loss: 1.0362 - val_acc: 0.7917\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5068 - acc: 0.9593 - val_loss: 1.3025 - val_acc: 0.7107\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5037 - acc: 0.9636 - val_loss: 0.9935 - val_acc: 0.8010\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5013 - acc: 0.9617 - val_loss: 0.9954 - val_acc: 0.7950\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5006 - acc: 0.9646 - val_loss: 0.9647 - val_acc: 0.8060\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5005 - acc: 0.9630 - val_loss: 1.2143 - val_acc: 0.7520\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5056 - acc: 0.9614 - val_loss: 1.0968 - val_acc: 0.7603\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4981 - acc: 0.9637 - val_loss: 1.0397 - val_acc: 0.7937\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4962 - acc: 0.9632 - val_loss: 0.9959 - val_acc: 0.7990\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4994 - acc: 0.9628 - val_loss: 1.0109 - val_acc: 0.7990\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5087 - acc: 0.9569 - val_loss: 1.0081 - val_acc: 0.7967\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4962 - acc: 0.9632 - val_loss: 1.0629 - val_acc: 0.7850\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5021 - acc: 0.9629 - val_loss: 1.1451 - val_acc: 0.7590\n",
      "Epoch 225/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4941 - acc: 0.9654 - val_loss: 1.0095 - val_acc: 0.8047\n",
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4982 - acc: 0.9618 - val_loss: 1.0086 - val_acc: 0.7997\n",
      "Epoch 227/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4981 - acc: 0.9622 - val_loss: 1.0334 - val_acc: 0.7913\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4968 - acc: 0.9635 - val_loss: 0.9816 - val_acc: 0.8073\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4954 - acc: 0.9630 - val_loss: 1.2734 - val_acc: 0.7213\n",
      "Epoch 230/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4915 - acc: 0.9644 - val_loss: 1.0169 - val_acc: 0.7970\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4930 - acc: 0.9653 - val_loss: 0.9972 - val_acc: 0.8060\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4894 - acc: 0.9645 - val_loss: 1.0332 - val_acc: 0.7913\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4826 - acc: 0.9690 - val_loss: 0.9745 - val_acc: 0.8113\n",
      "Epoch 234/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4945 - acc: 0.9646 - val_loss: 1.1221 - val_acc: 0.7657\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4917 - acc: 0.9666 - val_loss: 1.0076 - val_acc: 0.8000\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4880 - acc: 0.9663 - val_loss: 0.9909 - val_acc: 0.8043\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4834 - acc: 0.9694 - val_loss: 1.0029 - val_acc: 0.8010\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4915 - acc: 0.9651 - val_loss: 1.0399 - val_acc: 0.7940\n",
      "Epoch 239/400\n",
      "11840/12000 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.9683\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4863 - acc: 0.9677 - val_loss: 1.0489 - val_acc: 0.7823\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4618 - acc: 0.9783 - val_loss: 0.9428 - val_acc: 0.8150\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4551 - acc: 0.9796 - val_loss: 0.9447 - val_acc: 0.8150\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.4518 - acc: 0.9813 - val_loss: 0.9634 - val_acc: 0.8153\n",
      "Epoch 243/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4543 - acc: 0.9803 - val_loss: 0.9542 - val_acc: 0.8173\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4523 - acc: 0.9783 - val_loss: 0.9552 - val_acc: 0.8117\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4466 - acc: 0.9826 - val_loss: 0.9528 - val_acc: 0.8117\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4509 - acc: 0.9803 - val_loss: 0.9796 - val_acc: 0.8030\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4488 - acc: 0.9833 - val_loss: 0.9500 - val_acc: 0.8117\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4449 - acc: 0.9827 - val_loss: 0.9529 - val_acc: 0.8107\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4508 - acc: 0.9822 - val_loss: 0.9559 - val_acc: 0.8157\n",
      "Epoch 250/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4456 - acc: 0.9824 - val_loss: 0.9539 - val_acc: 0.8160\n",
      "Epoch 251/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4472 - acc: 0.9818 - val_loss: 0.9565 - val_acc: 0.8157\n",
      "Epoch 252/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4462 - acc: 0.9824 - val_loss: 0.9547 - val_acc: 0.8173\n",
      "Epoch 253/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4427 - acc: 0.9841 - val_loss: 0.9470 - val_acc: 0.8147\n",
      "Epoch 254/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4451 - acc: 0.9823 - val_loss: 0.9697 - val_acc: 0.8097\n",
      "Epoch 255/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4477 - acc: 0.9827 - val_loss: 0.9488 - val_acc: 0.8163\n",
      "Epoch 256/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4449 - acc: 0.9826 - val_loss: 0.9842 - val_acc: 0.8030\n",
      "Epoch 257/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4441 - acc: 0.9841 - val_loss: 0.9475 - val_acc: 0.8167\n",
      "Epoch 258/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.4466 - acc: 0.9815 - val_loss: 0.9570 - val_acc: 0.8180\n",
      "Epoch 259/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.4440 - acc: 0.9838 - val_loss: 0.9469 - val_acc: 0.8200\n",
      "Epoch 260/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4419 - acc: 0.9848 - val_loss: 0.9762 - val_acc: 0.8100\n",
      "Epoch 261/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4429 - acc: 0.9841 - val_loss: 1.0035 - val_acc: 0.8027\n",
      "Epoch 262/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4455 - acc: 0.9827 - val_loss: 0.9772 - val_acc: 0.8100\n",
      "Epoch 263/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4427 - acc: 0.9833 - val_loss: 0.9735 - val_acc: 0.8167\n",
      "Epoch 264/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4394 - acc: 0.9838 - val_loss: 0.9983 - val_acc: 0.8007\n",
      "Epoch 265/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4404 - acc: 0.9837 - val_loss: 0.9745 - val_acc: 0.8107\n",
      "Epoch 266/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4379 - acc: 0.9854 - val_loss: 0.9633 - val_acc: 0.8180\n",
      "Epoch 267/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4376 - acc: 0.9847 - val_loss: 0.9559 - val_acc: 0.8167\n",
      "Epoch 268/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4396 - acc: 0.9839 - val_loss: 0.9652 - val_acc: 0.8080\n",
      "Epoch 269/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4378 - acc: 0.9856 - val_loss: 0.9460 - val_acc: 0.8203\n",
      "Epoch 270/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4421 - acc: 0.9833 - val_loss: 0.9823 - val_acc: 0.8083\n",
      "Epoch 271/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4380 - acc: 0.9858 - val_loss: 0.9598 - val_acc: 0.8147\n",
      "Epoch 272/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4427 - acc: 0.9830 - val_loss: 0.9539 - val_acc: 0.8103\n",
      "Epoch 273/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4372 - acc: 0.9855 - val_loss: 0.9607 - val_acc: 0.8113\n",
      "Epoch 274/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4366 - acc: 0.9850 - val_loss: 0.9504 - val_acc: 0.8150\n",
      "Epoch 275/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4387 - acc: 0.9863 - val_loss: 0.9940 - val_acc: 0.8037\n",
      "Epoch 276/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4385 - acc: 0.9843 - val_loss: 0.9632 - val_acc: 0.8160\n",
      "Epoch 277/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4356 - acc: 0.9855 - val_loss: 0.9544 - val_acc: 0.8160\n",
      "Epoch 278/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4381 - acc: 0.9858 - val_loss: 0.9711 - val_acc: 0.8137\n",
      "Epoch 279/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4357 - acc: 0.9868 - val_loss: 0.9779 - val_acc: 0.8133\n",
      "Epoch 280/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4343 - acc: 0.9870 - val_loss: 0.9584 - val_acc: 0.8177\n",
      "Epoch 281/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4329 - acc: 0.9873 - val_loss: 0.9912 - val_acc: 0.8063\n",
      "Epoch 282/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4384 - acc: 0.9846 - val_loss: 0.9609 - val_acc: 0.8110\n",
      "Epoch 283/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4356 - acc: 0.9836 - val_loss: 0.9506 - val_acc: 0.8137\n",
      "Epoch 284/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4330 - acc: 0.9854 - val_loss: 0.9688 - val_acc: 0.8077\n",
      "Epoch 285/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4336 - acc: 0.9861 - val_loss: 0.9997 - val_acc: 0.7950\n",
      "Epoch 286/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4364 - acc: 0.9851 - val_loss: 0.9649 - val_acc: 0.8173\n",
      "Epoch 287/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4318 - acc: 0.9889 - val_loss: 0.9855 - val_acc: 0.8077\n",
      "Epoch 288/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4319 - acc: 0.9868 - val_loss: 0.9697 - val_acc: 0.8053\n",
      "Epoch 289/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4361 - acc: 0.9864 - val_loss: 0.9636 - val_acc: 0.8107\n",
      "Epoch 290/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4353 - acc: 0.9865 - val_loss: 0.9559 - val_acc: 0.8167\n",
      "Epoch 291/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4317 - acc: 0.9870 - val_loss: 0.9674 - val_acc: 0.8097\n",
      "Epoch 292/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4357 - acc: 0.9852 - val_loss: 0.9654 - val_acc: 0.8103\n",
      "Epoch 293/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.4282 - acc: 0.9888 - val_loss: 0.9620 - val_acc: 0.8217\n",
      "Epoch 294/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4332 - acc: 0.9860 - val_loss: 0.9492 - val_acc: 0.8177\n",
      "Epoch 295/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4308 - acc: 0.9892 - val_loss: 0.9525 - val_acc: 0.8180\n",
      "Epoch 296/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4345 - acc: 0.9867 - val_loss: 1.0089 - val_acc: 0.8087\n",
      "Epoch 297/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4320 - acc: 0.9864 - val_loss: 0.9560 - val_acc: 0.8153\n",
      "Epoch 298/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4324 - acc: 0.9868 - val_loss: 0.9715 - val_acc: 0.8117\n",
      "Epoch 299/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4317 - acc: 0.9871 - val_loss: 0.9666 - val_acc: 0.8110\n",
      "Epoch 300/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4305 - acc: 0.9863 - val_loss: 0.9593 - val_acc: 0.8163\n",
      "Epoch 301/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4292 - acc: 0.9879 - val_loss: 0.9641 - val_acc: 0.8163\n",
      "Epoch 302/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4315 - acc: 0.9876 - val_loss: 0.9841 - val_acc: 0.8117\n",
      "Epoch 303/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4279 - acc: 0.9888 - val_loss: 0.9527 - val_acc: 0.8127\n",
      "Epoch 304/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4281 - acc: 0.9883 - val_loss: 0.9710 - val_acc: 0.8117\n",
      "Epoch 305/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4312 - acc: 0.9869 - val_loss: 1.0472 - val_acc: 0.7797\n",
      "Epoch 306/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4328 - acc: 0.9866 - val_loss: 0.9838 - val_acc: 0.8013\n",
      "Epoch 307/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4293 - acc: 0.9870 - val_loss: 0.9459 - val_acc: 0.8163\n",
      "Epoch 308/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4316 - acc: 0.9859 - val_loss: 0.9595 - val_acc: 0.8200\n",
      "Epoch 309/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4310 - acc: 0.9872 - val_loss: 0.9476 - val_acc: 0.8143\n",
      "Epoch 310/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4276 - acc: 0.9872 - val_loss: 0.9742 - val_acc: 0.8107\n",
      "Epoch 311/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4255 - acc: 0.9898 - val_loss: 0.9608 - val_acc: 0.8130\n",
      "Epoch 312/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4288 - acc: 0.9883 - val_loss: 0.9495 - val_acc: 0.8153\n",
      "Epoch 313/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4255 - acc: 0.9890 - val_loss: 0.9976 - val_acc: 0.7983\n",
      "Epoch 314/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4267 - acc: 0.9884 - val_loss: 0.9737 - val_acc: 0.8127\n",
      "Epoch 315/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4293 - acc: 0.9887 - val_loss: 0.9892 - val_acc: 0.7983\n",
      "Epoch 316/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4240 - acc: 0.9912 - val_loss: 0.9562 - val_acc: 0.8103\n",
      "Epoch 317/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.4284 - acc: 0.9892 - val_loss: 0.9564 - val_acc: 0.8163\n",
      "Epoch 318/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4270 - acc: 0.9887 - val_loss: 0.9540 - val_acc: 0.8207\n",
      "Epoch 319/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4274 - acc: 0.9880 - val_loss: 0.9521 - val_acc: 0.8160\n",
      "Epoch 320/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4251 - acc: 0.9899 - val_loss: 0.9501 - val_acc: 0.8143\n",
      "Epoch 321/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4284 - acc: 0.9883 - val_loss: 0.9600 - val_acc: 0.8090\n",
      "Epoch 322/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4267 - acc: 0.9881 - val_loss: 0.9491 - val_acc: 0.8157\n",
      "Epoch 323/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4253 - acc: 0.9884 - val_loss: 0.9502 - val_acc: 0.8173\n",
      "Epoch 324/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4281 - acc: 0.9882 - val_loss: 0.9634 - val_acc: 0.8107\n",
      "Epoch 325/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4245 - acc: 0.9892 - val_loss: 0.9646 - val_acc: 0.8153\n",
      "Epoch 326/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4241 - acc: 0.9895 - val_loss: 0.9827 - val_acc: 0.8043\n",
      "Epoch 327/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4217 - acc: 0.9906 - val_loss: 0.9507 - val_acc: 0.8153\n",
      "Epoch 328/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4225 - acc: 0.9894 - val_loss: 1.0014 - val_acc: 0.8047\n",
      "Epoch 329/400\n",
      "11936/12000 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.9899\n",
      "Epoch 00329: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4236 - acc: 0.9899 - val_loss: 0.9778 - val_acc: 0.8123\n",
      "Epoch 330/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4154 - acc: 0.9910 - val_loss: 0.9469 - val_acc: 0.8147\n",
      "Epoch 331/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4134 - acc: 0.9925 - val_loss: 0.9570 - val_acc: 0.8123\n",
      "Epoch 332/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4118 - acc: 0.9919 - val_loss: 0.9507 - val_acc: 0.8180\n",
      "Epoch 333/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4107 - acc: 0.9938 - val_loss: 0.9463 - val_acc: 0.8217\n",
      "Epoch 334/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4152 - acc: 0.9914 - val_loss: 0.9523 - val_acc: 0.8147\n",
      "Epoch 335/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4103 - acc: 0.9937 - val_loss: 0.9510 - val_acc: 0.8147\n",
      "Epoch 336/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.4127 - acc: 0.9926 - val_loss: 0.9466 - val_acc: 0.8220\n",
      "Epoch 337/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4120 - acc: 0.9937 - val_loss: 0.9434 - val_acc: 0.8193\n",
      "Epoch 338/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.4089 - acc: 0.9936 - val_loss: 0.9412 - val_acc: 0.8200\n",
      "Epoch 339/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4112 - acc: 0.9938 - val_loss: 0.9444 - val_acc: 0.8180\n",
      "Epoch 340/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4114 - acc: 0.9931 - val_loss: 0.9545 - val_acc: 0.8133\n",
      "Epoch 341/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4089 - acc: 0.9948 - val_loss: 0.9463 - val_acc: 0.8157\n",
      "Epoch 342/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4086 - acc: 0.9942 - val_loss: 0.9476 - val_acc: 0.8160\n",
      "Epoch 343/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4090 - acc: 0.9935 - val_loss: 0.9480 - val_acc: 0.8173\n",
      "Epoch 344/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4088 - acc: 0.9944 - val_loss: 0.9399 - val_acc: 0.8193\n",
      "Epoch 345/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4104 - acc: 0.9934 - val_loss: 0.9486 - val_acc: 0.8140\n",
      "Epoch 346/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4091 - acc: 0.9937 - val_loss: 0.9487 - val_acc: 0.8150\n",
      "Epoch 347/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4067 - acc: 0.9943 - val_loss: 0.9740 - val_acc: 0.8093\n",
      "Epoch 348/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4091 - acc: 0.9938 - val_loss: 0.9371 - val_acc: 0.8190\n",
      "Epoch 349/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4108 - acc: 0.9924 - val_loss: 0.9809 - val_acc: 0.8057\n",
      "Epoch 350/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4096 - acc: 0.9927 - val_loss: 0.9418 - val_acc: 0.8187\n",
      "Epoch 351/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4106 - acc: 0.9937 - val_loss: 0.9391 - val_acc: 0.8200\n",
      "Epoch 352/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4096 - acc: 0.9931 - val_loss: 0.9467 - val_acc: 0.8180\n",
      "Epoch 353/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4064 - acc: 0.9946 - val_loss: 0.9533 - val_acc: 0.8150\n",
      "Epoch 354/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4091 - acc: 0.9933 - val_loss: 0.9530 - val_acc: 0.8127\n",
      "Epoch 355/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4070 - acc: 0.9940 - val_loss: 0.9406 - val_acc: 0.8187\n",
      "Epoch 356/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4082 - acc: 0.9937 - val_loss: 0.9562 - val_acc: 0.8113\n",
      "Epoch 357/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4082 - acc: 0.9937 - val_loss: 0.9754 - val_acc: 0.8067\n",
      "Epoch 358/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4098 - acc: 0.9933 - val_loss: 0.9498 - val_acc: 0.8170\n",
      "Epoch 359/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4096 - acc: 0.9929 - val_loss: 0.9445 - val_acc: 0.8203\n",
      "Epoch 360/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4097 - acc: 0.9930 - val_loss: 0.9557 - val_acc: 0.8160\n",
      "Epoch 361/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.4070 - acc: 0.9942 - val_loss: 0.9372 - val_acc: 0.8250\n",
      "Epoch 362/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4076 - acc: 0.9931 - val_loss: 0.9559 - val_acc: 0.8117\n",
      "Epoch 363/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4075 - acc: 0.9933 - val_loss: 0.9323 - val_acc: 0.8210\n",
      "Epoch 364/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4074 - acc: 0.9935 - val_loss: 0.9444 - val_acc: 0.8190\n",
      "Epoch 365/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4089 - acc: 0.9940 - val_loss: 0.9391 - val_acc: 0.8187\n",
      "Epoch 366/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4057 - acc: 0.9937 - val_loss: 0.9637 - val_acc: 0.8123\n",
      "Epoch 367/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4042 - acc: 0.9951 - val_loss: 0.9439 - val_acc: 0.8167\n",
      "Epoch 368/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4083 - acc: 0.9933 - val_loss: 0.9346 - val_acc: 0.8190\n",
      "Epoch 369/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4060 - acc: 0.9940 - val_loss: 0.9441 - val_acc: 0.8177\n",
      "Epoch 370/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4067 - acc: 0.9942 - val_loss: 0.9374 - val_acc: 0.8233\n",
      "Epoch 371/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4047 - acc: 0.9948 - val_loss: 0.9413 - val_acc: 0.8193\n",
      "Epoch 372/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4056 - acc: 0.9954 - val_loss: 0.9459 - val_acc: 0.8217\n",
      "Epoch 373/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4067 - acc: 0.9939 - val_loss: 0.9476 - val_acc: 0.8187\n",
      "Epoch 374/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4048 - acc: 0.9949 - val_loss: 0.9399 - val_acc: 0.8210\n",
      "Epoch 375/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4069 - acc: 0.9946 - val_loss: 0.9473 - val_acc: 0.8200\n",
      "Epoch 376/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4055 - acc: 0.9943 - val_loss: 0.9581 - val_acc: 0.8143\n",
      "Epoch 377/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4040 - acc: 0.9953 - val_loss: 0.9425 - val_acc: 0.8130\n",
      "Epoch 378/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4048 - acc: 0.9947 - val_loss: 0.9431 - val_acc: 0.8217\n",
      "Epoch 379/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4044 - acc: 0.9951 - val_loss: 0.9388 - val_acc: 0.8190\n",
      "Epoch 380/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4078 - acc: 0.9941 - val_loss: 0.9438 - val_acc: 0.8193\n",
      "Epoch 381/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4049 - acc: 0.9948 - val_loss: 0.9443 - val_acc: 0.8173\n",
      "Epoch 382/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4032 - acc: 0.9953 - val_loss: 0.9381 - val_acc: 0.8177\n",
      "Epoch 383/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4031 - acc: 0.9950 - val_loss: 0.9484 - val_acc: 0.8203\n",
      "Epoch 384/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4063 - acc: 0.9944 - val_loss: 0.9445 - val_acc: 0.8227\n",
      "Epoch 385/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4039 - acc: 0.9953 - val_loss: 0.9387 - val_acc: 0.8223\n",
      "Epoch 386/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4042 - acc: 0.9947 - val_loss: 0.9508 - val_acc: 0.8190\n",
      "Epoch 387/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4039 - acc: 0.9949 - val_loss: 0.9406 - val_acc: 0.8187\n",
      "Epoch 388/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4037 - acc: 0.9950 - val_loss: 0.9342 - val_acc: 0.8230\n",
      "Epoch 389/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4053 - acc: 0.9943 - val_loss: 0.9332 - val_acc: 0.8233\n",
      "Epoch 390/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4029 - acc: 0.9948 - val_loss: 0.9457 - val_acc: 0.8147\n",
      "Epoch 391/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4028 - acc: 0.9952 - val_loss: 0.9487 - val_acc: 0.8173\n",
      "Epoch 392/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4038 - acc: 0.9943 - val_loss: 0.9557 - val_acc: 0.8160\n",
      "Epoch 393/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4017 - acc: 0.9964 - val_loss: 0.9374 - val_acc: 0.8193\n",
      "Epoch 394/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4017 - acc: 0.9955 - val_loss: 0.9503 - val_acc: 0.8210\n",
      "Epoch 395/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4038 - acc: 0.9956 - val_loss: 0.9640 - val_acc: 0.8133\n",
      "Epoch 396/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4033 - acc: 0.9957 - val_loss: 0.9467 - val_acc: 0.8177\n",
      "Epoch 397/400\n",
      "11968/12000 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.9945\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4043 - acc: 0.9945 - val_loss: 0.9374 - val_acc: 0.8170\n",
      "Epoch 398/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4023 - acc: 0.9942 - val_loss: 0.9377 - val_acc: 0.8227\n",
      "Epoch 399/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.3993 - acc: 0.9965 - val_loss: 0.9380 - val_acc: 0.8233\n",
      "Epoch 400/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4011 - acc: 0.9961 - val_loss: 0.9395 - val_acc: 0.8210\n",
      "0.825\n",
      "0.84129\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 60, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_10 (TensorFlo [(None, 60, 6, 1), ( 0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 60, 8, 1)     0           tf_op_layer_split_10[0][0]       \n",
      "                                                                 tf_op_layer_split_10[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 60, 12, 1)    0           tf_op_layer_split_10[0][0]       \n",
      "                                                                 tf_op_layer_split_10[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 60, 8, 64)    576         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 60, 12, 64)   576         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 60, 8, 64)    256         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 60, 12, 64)   256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 60, 8, 64)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 60, 12, 64)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 60, 8, 128)   73728       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 60, 12, 128)  73728       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 60, 8, 128)   512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 60, 12, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 60, 8, 128)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 60, 12, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 60, 8, 128)   0           activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 60, 12, 128)  0           activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 30, 4, 128)   0           dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 30, 6, 128)   0           dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 30, 4, 256)   294912      average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 30, 6, 256)   294912      average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 30, 4, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 30, 6, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 30, 4, 256)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 30, 6, 256)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 30, 4, 256)   0           activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 30, 6, 256)   0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 30, 4, 512)   1179648     dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 30, 6, 512)   1179648     dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 30, 4, 512)   2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 30, 6, 512)   2048        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 30, 4, 512)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 30, 6, 512)   0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 30, 4, 512)   0           activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 30, 6, 512)   0           activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_20 (Gl (None, 512)          0           dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_21 (Gl (None, 512)          0           dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 512)          2048        global_average_pooling2d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 512)          2048        global_average_pooling2d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 128)          65664       batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 128)          65664       batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 128)          512         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 128)          512         dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 128)          0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 128)          0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 256)          0           dropout_83[0][0]                 \n",
      "                                                                 dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 64)           16448       concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 64)           4160        dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_43[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,263,764\n",
      "Trainable params: 3,257,364\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 8s 651us/sample - loss: 2.0573 - acc: 0.3802 - val_loss: 3.0138 - val_acc: 0.2560\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.7895 - acc: 0.4559 - val_loss: 2.8921 - val_acc: 0.2853\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 1.6965 - acc: 0.4840 - val_loss: 3.2332 - val_acc: 0.2783\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.6277 - acc: 0.5117 - val_loss: 1.8413 - val_acc: 0.4680\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.5839 - acc: 0.5278 - val_loss: 2.1738 - val_acc: 0.3923\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 1.5306 - acc: 0.5452 - val_loss: 1.6283 - val_acc: 0.5260\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 1.4857 - acc: 0.5642 - val_loss: 1.5512 - val_acc: 0.5560\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 1.4430 - acc: 0.5788 - val_loss: 1.8069 - val_acc: 0.4670\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 1.4250 - acc: 0.5852 - val_loss: 2.0165 - val_acc: 0.4367\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 1.3945 - acc: 0.5998 - val_loss: 1.4861 - val_acc: 0.5540\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 1.3670 - acc: 0.6067 - val_loss: 1.6256 - val_acc: 0.5640\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 1.3559 - acc: 0.6104 - val_loss: 2.0700 - val_acc: 0.4863\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 1.3242 - acc: 0.6203 - val_loss: 1.4047 - val_acc: 0.5943\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.3013 - acc: 0.6373 - val_loss: 1.3822 - val_acc: 0.6157\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 1.2799 - acc: 0.6356 - val_loss: 1.4465 - val_acc: 0.5810\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 1.2582 - acc: 0.6546 - val_loss: 1.4191 - val_acc: 0.5967\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 1.2455 - acc: 0.6547 - val_loss: 1.4109 - val_acc: 0.6197\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 1.2250 - acc: 0.6655 - val_loss: 1.5315 - val_acc: 0.5783\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 1.2117 - acc: 0.6687 - val_loss: 1.4248 - val_acc: 0.5987\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 1.1941 - acc: 0.6726 - val_loss: 1.1990 - val_acc: 0.6703\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 1.1787 - acc: 0.6810 - val_loss: 1.1637 - val_acc: 0.6927\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 1.1641 - acc: 0.6864 - val_loss: 1.1619 - val_acc: 0.6967\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 1.1426 - acc: 0.6951 - val_loss: 1.4422 - val_acc: 0.6270\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.1359 - acc: 0.6952 - val_loss: 1.2692 - val_acc: 0.6547\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 1.1250 - acc: 0.7104 - val_loss: 1.1132 - val_acc: 0.7133\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.1128 - acc: 0.7097 - val_loss: 1.2581 - val_acc: 0.6547\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 1.1035 - acc: 0.7083 - val_loss: 1.2804 - val_acc: 0.6493\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 1.0941 - acc: 0.7152 - val_loss: 1.1731 - val_acc: 0.6950\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.0875 - acc: 0.7170 - val_loss: 1.1195 - val_acc: 0.7093\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 1.0621 - acc: 0.7308 - val_loss: 1.2114 - val_acc: 0.6847\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 1.0650 - acc: 0.7286 - val_loss: 1.0746 - val_acc: 0.7293\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.0506 - acc: 0.7307 - val_loss: 1.2069 - val_acc: 0.6777\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 1.0483 - acc: 0.7336 - val_loss: 1.0604 - val_acc: 0.7280\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 1.0220 - acc: 0.7403 - val_loss: 1.1712 - val_acc: 0.6970\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.0220 - acc: 0.7402 - val_loss: 1.0905 - val_acc: 0.7250\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 1.0111 - acc: 0.7498 - val_loss: 1.4746 - val_acc: 0.6040\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.0121 - acc: 0.7472 - val_loss: 1.1007 - val_acc: 0.7120\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 1.0008 - acc: 0.7529 - val_loss: 1.0535 - val_acc: 0.7413\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9863 - acc: 0.7580 - val_loss: 1.2779 - val_acc: 0.6620\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.9857 - acc: 0.7563 - val_loss: 1.2347 - val_acc: 0.6700\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.9801 - acc: 0.7641 - val_loss: 1.0462 - val_acc: 0.7430\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9725 - acc: 0.7614 - val_loss: 1.1102 - val_acc: 0.7143\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.9653 - acc: 0.7675 - val_loss: 1.1486 - val_acc: 0.7043\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.9556 - acc: 0.7695 - val_loss: 1.2769 - val_acc: 0.6573\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.9450 - acc: 0.7731 - val_loss: 1.0273 - val_acc: 0.7557\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9399 - acc: 0.7797 - val_loss: 1.2461 - val_acc: 0.6817\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.9258 - acc: 0.7810 - val_loss: 1.0716 - val_acc: 0.7310\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.9278 - acc: 0.7782 - val_loss: 1.0055 - val_acc: 0.7597\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.9225 - acc: 0.7799 - val_loss: 1.0986 - val_acc: 0.7200\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.9168 - acc: 0.7910 - val_loss: 1.0459 - val_acc: 0.7433\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.9112 - acc: 0.7893 - val_loss: 1.0400 - val_acc: 0.7433\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9075 - acc: 0.7898 - val_loss: 1.0259 - val_acc: 0.7483\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.8950 - acc: 0.7947 - val_loss: 1.1601 - val_acc: 0.7020\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.8993 - acc: 0.7882 - val_loss: 0.9811 - val_acc: 0.7790\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.8826 - acc: 0.8019 - val_loss: 1.0715 - val_acc: 0.7370\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.8799 - acc: 0.8028 - val_loss: 1.5302 - val_acc: 0.6337\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.8714 - acc: 0.8054 - val_loss: 1.0320 - val_acc: 0.7593\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.8741 - acc: 0.7972 - val_loss: 1.1016 - val_acc: 0.7190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.8574 - acc: 0.8119 - val_loss: 1.0422 - val_acc: 0.7470\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8551 - acc: 0.8127 - val_loss: 1.4903 - val_acc: 0.6370\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8535 - acc: 0.8148 - val_loss: 1.0116 - val_acc: 0.7737\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.8362 - acc: 0.8218 - val_loss: 1.0129 - val_acc: 0.7707\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.8429 - acc: 0.8196 - val_loss: 1.1429 - val_acc: 0.7210\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.8402 - acc: 0.8193 - val_loss: 1.1399 - val_acc: 0.7207\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.8350 - acc: 0.8244 - val_loss: 1.0358 - val_acc: 0.7650\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.8197 - acc: 0.8234 - val_loss: 0.9910 - val_acc: 0.7687\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.8212 - acc: 0.8267 - val_loss: 1.0926 - val_acc: 0.7373\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.8263 - acc: 0.8219 - val_loss: 1.3028 - val_acc: 0.6747\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.8080 - acc: 0.8314 - val_loss: 1.0288 - val_acc: 0.7627\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.8038 - acc: 0.8295 - val_loss: 0.9865 - val_acc: 0.7730\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.8065 - acc: 0.8313 - val_loss: 1.3639 - val_acc: 0.6603\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7928 - acc: 0.8396 - val_loss: 1.0927 - val_acc: 0.7443\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.7905 - acc: 0.8373 - val_loss: 0.9937 - val_acc: 0.7803\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7950 - acc: 0.8360 - val_loss: 1.0507 - val_acc: 0.7697\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.7873 - acc: 0.8404 - val_loss: 1.0401 - val_acc: 0.7590\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7761 - acc: 0.8428 - val_loss: 0.9893 - val_acc: 0.7750\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.7777 - acc: 0.8428 - val_loss: 1.0689 - val_acc: 0.7380\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7762 - acc: 0.8465 - val_loss: 1.0962 - val_acc: 0.7463\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7621 - acc: 0.8505 - val_loss: 1.0497 - val_acc: 0.7620\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.7621 - acc: 0.8496 - val_loss: 1.0113 - val_acc: 0.7830\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7624 - acc: 0.8495 - val_loss: 1.3107 - val_acc: 0.6900\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7503 - acc: 0.8560 - val_loss: 1.3397 - val_acc: 0.6743\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.7499 - acc: 0.8524 - val_loss: 1.0591 - val_acc: 0.7603\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.7394 - acc: 0.8618 - val_loss: 0.9809 - val_acc: 0.7833\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.7446 - acc: 0.8590 - val_loss: 1.0414 - val_acc: 0.7683\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7510 - acc: 0.8561 - val_loss: 1.0061 - val_acc: 0.7713\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.7285 - acc: 0.8657 - val_loss: 1.0082 - val_acc: 0.7797\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.7296 - acc: 0.8657 - val_loss: 1.1182 - val_acc: 0.7410\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.7326 - acc: 0.8613 - val_loss: 0.9863 - val_acc: 0.7787\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7228 - acc: 0.8668 - val_loss: 1.0945 - val_acc: 0.7423\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.7234 - acc: 0.8669 - val_loss: 0.9953 - val_acc: 0.7870\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7237 - acc: 0.8677 - val_loss: 0.9970 - val_acc: 0.7823\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.7180 - acc: 0.8696 - val_loss: 0.9625 - val_acc: 0.7913\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.7151 - acc: 0.8706 - val_loss: 0.9790 - val_acc: 0.7927\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.7092 - acc: 0.8723 - val_loss: 1.0206 - val_acc: 0.7737\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.7029 - acc: 0.8733 - val_loss: 1.0118 - val_acc: 0.7797\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6977 - acc: 0.8822 - val_loss: 0.9714 - val_acc: 0.7917\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.7050 - acc: 0.8769 - val_loss: 1.0644 - val_acc: 0.7707\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6969 - acc: 0.8792 - val_loss: 0.9880 - val_acc: 0.7920\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6896 - acc: 0.8842 - val_loss: 1.0713 - val_acc: 0.7643\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6848 - acc: 0.8852 - val_loss: 0.9711 - val_acc: 0.7853\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6887 - acc: 0.8830 - val_loss: 1.2575 - val_acc: 0.7060\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6899 - acc: 0.8841 - val_loss: 1.0252 - val_acc: 0.7767\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6815 - acc: 0.8882 - val_loss: 1.0070 - val_acc: 0.7823\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6761 - acc: 0.8905 - val_loss: 0.9988 - val_acc: 0.7760\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6750 - acc: 0.8913 - val_loss: 1.1372 - val_acc: 0.7487\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6735 - acc: 0.8904 - val_loss: 1.0453 - val_acc: 0.7673\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6639 - acc: 0.8934 - val_loss: 1.1017 - val_acc: 0.7527\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6576 - acc: 0.8968 - val_loss: 0.9915 - val_acc: 0.7910\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6669 - acc: 0.8923 - val_loss: 1.0682 - val_acc: 0.7643\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6604 - acc: 0.8946 - val_loss: 1.1680 - val_acc: 0.7260\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6614 - acc: 0.8930 - val_loss: 1.0071 - val_acc: 0.7847\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6554 - acc: 0.8971 - val_loss: 0.9767 - val_acc: 0.7883\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6539 - acc: 0.9004 - val_loss: 1.0533 - val_acc: 0.7803\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6489 - acc: 0.8997 - val_loss: 1.0115 - val_acc: 0.7867\n",
      "Epoch 116/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6476 - acc: 0.9013 - val_loss: 1.0514 - val_acc: 0.7793\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6419 - acc: 0.9008 - val_loss: 1.0333 - val_acc: 0.7833\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6544 - acc: 0.9003 - val_loss: 0.9577 - val_acc: 0.7870\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.6431 - acc: 0.9022 - val_loss: 1.0214 - val_acc: 0.7840\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.6417 - acc: 0.9004 - val_loss: 0.9828 - val_acc: 0.7937\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6286 - acc: 0.9080 - val_loss: 1.1055 - val_acc: 0.7580\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6294 - acc: 0.9095 - val_loss: 0.9996 - val_acc: 0.7810\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6289 - acc: 0.9082 - val_loss: 1.1141 - val_acc: 0.7553\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6335 - acc: 0.9052 - val_loss: 0.9806 - val_acc: 0.7880\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.6257 - acc: 0.9108 - val_loss: 0.9677 - val_acc: 0.8007\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6215 - acc: 0.9121 - val_loss: 1.1161 - val_acc: 0.7490\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.6197 - acc: 0.9149 - val_loss: 0.9917 - val_acc: 0.8000\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6226 - acc: 0.9130 - val_loss: 0.9807 - val_acc: 0.7947\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6264 - acc: 0.9090 - val_loss: 1.0337 - val_acc: 0.7823\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6248 - acc: 0.9109 - val_loss: 0.9932 - val_acc: 0.7980\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6114 - acc: 0.9175 - val_loss: 1.1060 - val_acc: 0.7527\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6095 - acc: 0.9155 - val_loss: 1.0167 - val_acc: 0.7787\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6072 - acc: 0.9185 - val_loss: 1.3478 - val_acc: 0.6990\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6069 - acc: 0.9185 - val_loss: 0.9910 - val_acc: 0.7850\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6024 - acc: 0.9183 - val_loss: 0.9839 - val_acc: 0.7920\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.6130 - acc: 0.9132 - val_loss: 0.9732 - val_acc: 0.7973\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5968 - acc: 0.9239 - val_loss: 1.0513 - val_acc: 0.7680\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5999 - acc: 0.9222 - val_loss: 1.0064 - val_acc: 0.7883\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6070 - acc: 0.9191 - val_loss: 0.9822 - val_acc: 0.7960\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5991 - acc: 0.9236 - val_loss: 1.0534 - val_acc: 0.7753\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5954 - acc: 0.9232 - val_loss: 1.2997 - val_acc: 0.7107\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5917 - acc: 0.9230 - val_loss: 0.9877 - val_acc: 0.7927\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5938 - acc: 0.9239 - val_loss: 1.0933 - val_acc: 0.7607\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5933 - acc: 0.9224 - val_loss: 0.9881 - val_acc: 0.7997\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5877 - acc: 0.9256 - val_loss: 0.9906 - val_acc: 0.7920\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5840 - acc: 0.9279 - val_loss: 1.0626 - val_acc: 0.7663\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.5853 - acc: 0.9277 - val_loss: 0.9663 - val_acc: 0.8030\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5830 - acc: 0.9293 - val_loss: 1.0455 - val_acc: 0.7853\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5787 - acc: 0.9302 - val_loss: 1.0127 - val_acc: 0.7953\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5882 - acc: 0.9283 - val_loss: 1.0121 - val_acc: 0.7977\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5775 - acc: 0.9313 - val_loss: 1.1260 - val_acc: 0.7617\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5744 - acc: 0.9303 - val_loss: 1.0100 - val_acc: 0.7937\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5793 - acc: 0.9302 - val_loss: 1.0176 - val_acc: 0.7817\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5771 - acc: 0.9283 - val_loss: 1.0060 - val_acc: 0.7977\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5679 - acc: 0.9361 - val_loss: 1.5315 - val_acc: 0.6793\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5717 - acc: 0.9323 - val_loss: 0.9882 - val_acc: 0.7997\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5659 - acc: 0.9365 - val_loss: 1.0509 - val_acc: 0.7867\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5644 - acc: 0.9357 - val_loss: 1.1944 - val_acc: 0.7267\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5658 - acc: 0.9336 - val_loss: 1.0870 - val_acc: 0.7690\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.5680 - acc: 0.9337 - val_loss: 0.9611 - val_acc: 0.8033\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5670 - acc: 0.9362 - val_loss: 0.9871 - val_acc: 0.7990\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5623 - acc: 0.9378 - val_loss: 0.9946 - val_acc: 0.7997\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5575 - acc: 0.9383 - val_loss: 0.9883 - val_acc: 0.8027\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.5564 - acc: 0.9407 - val_loss: 0.9637 - val_acc: 0.8070\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5555 - acc: 0.9389 - val_loss: 1.1296 - val_acc: 0.7537\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5606 - acc: 0.9362 - val_loss: 1.0069 - val_acc: 0.7950\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5514 - acc: 0.9392 - val_loss: 1.2531 - val_acc: 0.7283\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5629 - acc: 0.9381 - val_loss: 0.9812 - val_acc: 0.7987\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5574 - acc: 0.9410 - val_loss: 1.0205 - val_acc: 0.7957\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5542 - acc: 0.9388 - val_loss: 0.9673 - val_acc: 0.8047\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5448 - acc: 0.9462 - val_loss: 1.0163 - val_acc: 0.7953\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.5442 - acc: 0.9459 - val_loss: 1.1106 - val_acc: 0.7637\n",
      "Epoch 173/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5452 - acc: 0.9434 - val_loss: 0.9933 - val_acc: 0.7887\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5498 - acc: 0.9438 - val_loss: 1.0367 - val_acc: 0.7983\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5457 - acc: 0.9439 - val_loss: 1.0550 - val_acc: 0.7757\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5475 - acc: 0.9418 - val_loss: 1.0214 - val_acc: 0.7870\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5388 - acc: 0.9472 - val_loss: 1.0360 - val_acc: 0.7887\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5389 - acc: 0.9473 - val_loss: 1.1751 - val_acc: 0.7483\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.5404 - acc: 0.9439 - val_loss: 0.9779 - val_acc: 0.8080\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5415 - acc: 0.9467 - val_loss: 1.0108 - val_acc: 0.7867\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5349 - acc: 0.9498 - val_loss: 1.0229 - val_acc: 0.7913\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5349 - acc: 0.9467 - val_loss: 0.9592 - val_acc: 0.8033\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5384 - acc: 0.9487 - val_loss: 1.0042 - val_acc: 0.7980\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5338 - acc: 0.9508 - val_loss: 0.9883 - val_acc: 0.8027\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5357 - acc: 0.9488 - val_loss: 1.3367 - val_acc: 0.7107\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5325 - acc: 0.9504 - val_loss: 1.0291 - val_acc: 0.7990\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5310 - acc: 0.9478 - val_loss: 1.0050 - val_acc: 0.7953\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5245 - acc: 0.9532 - val_loss: 1.0420 - val_acc: 0.7857\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5313 - acc: 0.9509 - val_loss: 0.9597 - val_acc: 0.8047\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5277 - acc: 0.9498 - val_loss: 1.0495 - val_acc: 0.7817\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5271 - acc: 0.9506 - val_loss: 1.0121 - val_acc: 0.7970\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5278 - acc: 0.9506 - val_loss: 1.0608 - val_acc: 0.7827\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.5248 - acc: 0.9518 - val_loss: 0.9725 - val_acc: 0.8090\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5278 - acc: 0.9496 - val_loss: 1.0069 - val_acc: 0.7990\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5176 - acc: 0.9571 - val_loss: 0.9749 - val_acc: 0.8000\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5206 - acc: 0.9537 - val_loss: 1.2657 - val_acc: 0.7217\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5249 - acc: 0.9511 - val_loss: 0.9824 - val_acc: 0.8010\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5222 - acc: 0.9526 - val_loss: 1.1286 - val_acc: 0.7693\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5277 - acc: 0.9511 - val_loss: 0.9962 - val_acc: 0.8043\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5228 - acc: 0.9553 - val_loss: 0.9959 - val_acc: 0.8060\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5183 - acc: 0.9545 - val_loss: 1.0002 - val_acc: 0.7987\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.5177 - acc: 0.9545 - val_loss: 0.9525 - val_acc: 0.8133\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5171 - acc: 0.9551 - val_loss: 1.1781 - val_acc: 0.7457\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5118 - acc: 0.9590 - val_loss: 1.0166 - val_acc: 0.8037\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5143 - acc: 0.9564 - val_loss: 0.9854 - val_acc: 0.8070\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.5163 - acc: 0.9565 - val_loss: 1.0313 - val_acc: 0.7907\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5149 - acc: 0.9578 - val_loss: 1.0438 - val_acc: 0.7803\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5143 - acc: 0.9560 - val_loss: 0.9747 - val_acc: 0.8130\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5066 - acc: 0.9611 - val_loss: 1.1596 - val_acc: 0.7560\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5122 - acc: 0.9592 - val_loss: 0.9788 - val_acc: 0.8027\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5125 - acc: 0.9565 - val_loss: 1.0264 - val_acc: 0.7973\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5097 - acc: 0.9599 - val_loss: 1.1804 - val_acc: 0.7497\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5075 - acc: 0.9596 - val_loss: 1.0870 - val_acc: 0.7707\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5059 - acc: 0.9604 - val_loss: 0.9793 - val_acc: 0.8023\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5005 - acc: 0.9622 - val_loss: 1.3664 - val_acc: 0.7093\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5099 - acc: 0.9579 - val_loss: 1.0071 - val_acc: 0.7990\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5010 - acc: 0.9605 - val_loss: 0.9835 - val_acc: 0.8017\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5059 - acc: 0.9613 - val_loss: 1.0219 - val_acc: 0.8073\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5079 - acc: 0.9607 - val_loss: 1.0080 - val_acc: 0.7977\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5036 - acc: 0.9619 - val_loss: 0.9931 - val_acc: 0.8027\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5011 - acc: 0.9599 - val_loss: 1.0165 - val_acc: 0.8010\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4975 - acc: 0.9641 - val_loss: 0.9871 - val_acc: 0.7980\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5001 - acc: 0.9618 - val_loss: 1.0816 - val_acc: 0.7893\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.4997 - acc: 0.9623 - val_loss: 0.9697 - val_acc: 0.8137\n",
      "Epoch 225/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4927 - acc: 0.9647 - val_loss: 1.0806 - val_acc: 0.7847\n",
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4967 - acc: 0.9607 - val_loss: 0.9914 - val_acc: 0.8027\n",
      "Epoch 227/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4961 - acc: 0.9641 - val_loss: 0.9613 - val_acc: 0.8057\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4913 - acc: 0.9651 - val_loss: 0.9775 - val_acc: 0.8107\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4942 - acc: 0.9658 - val_loss: 1.1067 - val_acc: 0.7683\n",
      "Epoch 230/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4910 - acc: 0.9663 - val_loss: 1.0170 - val_acc: 0.8033\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4895 - acc: 0.9645 - val_loss: 0.9956 - val_acc: 0.8047\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4901 - acc: 0.9669 - val_loss: 0.9837 - val_acc: 0.8090\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4918 - acc: 0.9656 - val_loss: 0.9539 - val_acc: 0.8130\n",
      "Epoch 234/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4960 - acc: 0.9628 - val_loss: 1.2700 - val_acc: 0.7257\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4924 - acc: 0.9641 - val_loss: 0.9552 - val_acc: 0.8130\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4930 - acc: 0.9648 - val_loss: 1.0001 - val_acc: 0.7983\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.4835 - acc: 0.9683 - val_loss: 0.9922 - val_acc: 0.8033\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4902 - acc: 0.9651 - val_loss: 0.9707 - val_acc: 0.8090\n",
      "Epoch 239/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4853 - acc: 0.9686 - val_loss: 0.9961 - val_acc: 0.7997\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4842 - acc: 0.9677 - val_loss: 1.0001 - val_acc: 0.8030\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4891 - acc: 0.9654 - val_loss: 0.9823 - val_acc: 0.8100\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.4889 - acc: 0.9671 - val_loss: 0.9713 - val_acc: 0.8150\n",
      "Epoch 243/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4869 - acc: 0.9656 - val_loss: 0.9740 - val_acc: 0.8053\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4818 - acc: 0.9712 - val_loss: 1.0382 - val_acc: 0.7890\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4908 - acc: 0.9663 - val_loss: 0.9849 - val_acc: 0.7997\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4792 - acc: 0.9712 - val_loss: 1.0345 - val_acc: 0.7933\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4918 - acc: 0.9656 - val_loss: 0.9694 - val_acc: 0.8123\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4823 - acc: 0.9690 - val_loss: 0.9799 - val_acc: 0.8087\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4786 - acc: 0.9724 - val_loss: 1.1545 - val_acc: 0.7593\n",
      "Epoch 250/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4844 - acc: 0.9681 - val_loss: 0.9847 - val_acc: 0.8067\n",
      "Epoch 251/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4837 - acc: 0.9686 - val_loss: 0.9680 - val_acc: 0.8140\n",
      "Epoch 252/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4793 - acc: 0.9703 - val_loss: 1.0041 - val_acc: 0.7913\n",
      "Epoch 253/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4743 - acc: 0.9727 - val_loss: 0.9816 - val_acc: 0.8037\n",
      "Epoch 254/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4840 - acc: 0.9690 - val_loss: 0.9710 - val_acc: 0.8037\n",
      "Epoch 255/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4757 - acc: 0.9730 - val_loss: 1.0272 - val_acc: 0.8037\n",
      "Epoch 256/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4751 - acc: 0.9713 - val_loss: 1.0703 - val_acc: 0.7807\n",
      "Epoch 257/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4809 - acc: 0.9683 - val_loss: 1.0400 - val_acc: 0.7997\n",
      "Epoch 258/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4761 - acc: 0.9710 - val_loss: 0.9926 - val_acc: 0.8120\n",
      "Epoch 259/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4738 - acc: 0.9728 - val_loss: 1.1851 - val_acc: 0.7580\n",
      "Epoch 260/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4756 - acc: 0.9722 - val_loss: 1.1737 - val_acc: 0.7653\n",
      "Epoch 261/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4814 - acc: 0.9703 - val_loss: 0.9728 - val_acc: 0.8147\n",
      "Epoch 262/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4684 - acc: 0.9736 - val_loss: 0.9687 - val_acc: 0.8150\n",
      "Epoch 263/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4772 - acc: 0.9697 - val_loss: 0.9562 - val_acc: 0.8120\n",
      "Epoch 264/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.4738 - acc: 0.9708 - val_loss: 0.9835 - val_acc: 0.8090\n",
      "Epoch 265/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4747 - acc: 0.9703 - val_loss: 0.9622 - val_acc: 0.8090\n",
      "Epoch 266/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4680 - acc: 0.9758 - val_loss: 1.0971 - val_acc: 0.7800\n",
      "Epoch 267/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4697 - acc: 0.9732 - val_loss: 1.0309 - val_acc: 0.7903\n",
      "Epoch 268/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.4665 - acc: 0.9758 - val_loss: 0.9641 - val_acc: 0.8177\n",
      "Epoch 269/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4690 - acc: 0.9724 - val_loss: 1.0101 - val_acc: 0.7940\n",
      "Epoch 270/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4649 - acc: 0.9758 - val_loss: 1.0119 - val_acc: 0.7977\n",
      "Epoch 271/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4730 - acc: 0.9729 - val_loss: 0.9630 - val_acc: 0.8137\n",
      "Epoch 272/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4719 - acc: 0.9725 - val_loss: 1.1691 - val_acc: 0.7573\n",
      "Epoch 273/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.4732 - acc: 0.9726 - val_loss: 0.9578 - val_acc: 0.8183\n",
      "Epoch 274/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4721 - acc: 0.9718 - val_loss: 0.9923 - val_acc: 0.8053\n",
      "Epoch 275/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4639 - acc: 0.9772 - val_loss: 0.9944 - val_acc: 0.8067\n",
      "Epoch 276/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4733 - acc: 0.9719 - val_loss: 1.0176 - val_acc: 0.7903\n",
      "Epoch 277/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4683 - acc: 0.9747 - val_loss: 0.9879 - val_acc: 0.8140\n",
      "Epoch 278/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4705 - acc: 0.9728 - val_loss: 0.9789 - val_acc: 0.8067\n",
      "Epoch 279/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4656 - acc: 0.9751 - val_loss: 0.9866 - val_acc: 0.8087\n",
      "Epoch 280/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.4662 - acc: 0.9743 - val_loss: 1.0322 - val_acc: 0.7967\n",
      "Epoch 281/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4635 - acc: 0.9770 - val_loss: 1.0555 - val_acc: 0.7857\n",
      "Epoch 282/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4690 - acc: 0.9741 - val_loss: 1.1008 - val_acc: 0.7723\n",
      "Epoch 283/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4638 - acc: 0.9753 - val_loss: 1.0157 - val_acc: 0.8020\n",
      "Epoch 284/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4639 - acc: 0.9757 - val_loss: 0.9629 - val_acc: 0.8180\n",
      "Epoch 285/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4630 - acc: 0.9760 - val_loss: 0.9688 - val_acc: 0.8147\n",
      "Epoch 286/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4627 - acc: 0.9761 - val_loss: 0.9760 - val_acc: 0.8123\n",
      "Epoch 287/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4626 - acc: 0.9765 - val_loss: 0.9930 - val_acc: 0.8083\n",
      "Epoch 288/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.4678 - acc: 0.9723 - val_loss: 0.9736 - val_acc: 0.8123\n",
      "Epoch 289/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4611 - acc: 0.9770 - val_loss: 1.0311 - val_acc: 0.7970\n",
      "Epoch 290/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.4605 - acc: 0.9778 - val_loss: 1.0219 - val_acc: 0.8040\n",
      "Epoch 291/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.4606 - acc: 0.9763 - val_loss: 0.9922 - val_acc: 0.8123\n",
      "Epoch 292/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4677 - acc: 0.9737 - val_loss: 1.0901 - val_acc: 0.7773\n",
      "Epoch 293/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.4598 - acc: 0.9786 - val_loss: 0.9654 - val_acc: 0.8063\n",
      "Epoch 294/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.4604 - acc: 0.9774 - val_loss: 1.2982 - val_acc: 0.7253\n",
      "Epoch 295/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.4646 - acc: 0.9743 - val_loss: 0.9904 - val_acc: 0.8103\n",
      "Epoch 296/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.4533 - acc: 0.9801 - val_loss: 1.0037 - val_acc: 0.8067\n",
      "Epoch 297/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.4537 - acc: 0.9808 - val_loss: 0.9609 - val_acc: 0.8240\n",
      "Epoch 298/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4651 - acc: 0.9737 - val_loss: 0.9455 - val_acc: 0.8167\n",
      "Epoch 299/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.4564 - acc: 0.9787 - val_loss: 1.0377 - val_acc: 0.7947\n",
      "Epoch 300/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.4584 - acc: 0.9780 - val_loss: 0.9703 - val_acc: 0.8093\n",
      "Epoch 301/400\n",
      "  672/12000 [>.............................] - ETA: 3s - loss: 0.4656 - acc: 0.9732"
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "proba_oof = np.zeros((15000, 20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "\n",
    "mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "\n",
    "# # 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# # 每一个大类输出 \n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "seeds = [42,39,17][:1]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "\n",
    "        mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "        new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "        model = Net(sample_num)\n",
    "        model.summary()\n",
    "#         optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "#         lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "#         wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "        model.compile(loss=custom_loss,\n",
    "#                       ,loss_weights=[3,7,21],\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.5,\n",
    "                                    patience=36)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'CNN{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx], y_binary[xx],\n",
    "                  epochs=400,\n",
    "                  batch_size=32,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(train[yy], y_binary[yy]),\n",
    "                  callbacks=[plateau, early_stopping, checkpoint],\n",
    "#                              class_weight=[classweights1,classweights2,classweights3]\n",
    "                             )\n",
    "        history.append(train_res)\n",
    "\n",
    "\n",
    "    #     # 找到对应最高的 val_acc 对应的epoch，预测left+1+right次\n",
    "    #     left=2\n",
    "    #     right=2\n",
    "    #     max_acc_index=history[fold].history['val_acc'].index(np.max(history[fold].history['val_acc']))+1\n",
    "\n",
    "    #     save_filelist=os.listdir(save_dir)\n",
    "    #     save_filelist.sort()\n",
    "    #     select_blending=save_filelist[max_acc_index-left : max_acc_index+right]\n",
    "    #     print(select_blending)\n",
    "    #     for file in select_blending:\n",
    "    #         model.load_weights(save_dir+'/'+file)\n",
    "    #         proba_t += model.predict(t, verbose=0, batch_size=1024) / (fold_num*len(select_blending))\n",
    "    #         proba_oof[yy] = model.predict(train[yy],verbose=0,batch_size=1024) / len(select_blending)\n",
    "\n",
    "        model.load_weights(f'CNN{fold}.h5')\n",
    "        proba_t[:,:20] += model.predict(test, verbose=0, batch_size=1024) / fold_num /len(seeds)\n",
    "        proba_oof[yy,:20] += model.predict(train[yy],verbose=0,batch_size=1024) /len(seeds)\n",
    "\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy,:20], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:33:26.844236Z",
     "start_time": "2020-08-20T23:33:26.115016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.798, 0.796, 0.801] 0.7983333333333333\n",
      "[0.8186476190476265, 0.8186095238095313, 0.8222380952381021] 0.8198317460317534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt   \n",
    "plt.figure(figsize=(12,20))\n",
    "start=5\n",
    "# for i in range(5):\n",
    "#     min_acc_index=history[i].history['val_loss'].index(np.min(history[i].history['val_loss']))\n",
    "#     print(str(i)+' val_loss'+str(history[i].history['val_19class_loss'][min_acc_index])+'val_loss'+str(history[i].history['val_loss'][min_acc_index]))\n",
    "# for i in range(5):\n",
    "#     min_acc_index=history[i].history['val_loss'].index(np.min(history[i].history['val_loss']))\n",
    "#     length=len(history[i].history['loss'][start:])\n",
    "#     plt.subplot(5,1,i+1)\n",
    "#     plt.plot(range(length),history[i].history['loss'][start:],label='loss')\n",
    "#     plt.plot(range(length),history[i].history['acc'][start:],label='acc')\n",
    "#     plt.plot(range(length),history[i].history['val_acc'][start:],label='val_loss')\n",
    "#     plt.plot(range(length),history[i].history['val_acc'][start:],label='val_acc')\n",
    "#     plt.title(str(i)+' val_loss'+str(history[i].history['val_loss'][min_acc_index])+'val_acc'+str(history[i].history['val_acc'][min_acc_index]))\n",
    "#     plt.legend(loc='best')\n",
    "    \n",
    "print(oof_score,np.mean(oof_score))\n",
    "print(oof_comm,np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T04:13:28.026993Z",
     "start_time": "2020-08-06T04:13:27.946217Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8186476190476265 0.798\n",
      "1 0.8186095238095313 0.796\n",
      "2 0.8222380952381021 0.801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CNN3fold0.81983_dict.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "joblib.dump(oof_dict,\"CNN3fold%.5f_dict.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:25:21.999522Z",
     "start_time": "2020-08-05T14:25:21.365728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79873\n",
      "0.81983\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   fragment_id  16000 non-null  int64\n",
      " 1   behavior_id  16000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 250.1 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t[:,:20], axis=1)\n",
    "oof_y = np.argmax(proba_oof[:,:20], axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../../data/'\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "sub.to_csv('CNN3fold%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T02:04:26.093831Z",
     "start_time": "2020-07-28T02:04:25.118669Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm,classes,title='Confusion Matrix'):\n",
    "\n",
    "    plt.figure(figsize=(12, 9), dpi=100)\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    sns.heatmap(cm,annot=True)\n",
    "    plt.title(title)\n",
    "    plt.xticks(ticks=range(19),labels=classes)\n",
    "    plt.yticks(ticks=range(19),labels=classes)\n",
    "    \n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predict label')\n",
    "    plt.show()\n",
    "    \n",
    "# classes表示不同类别的名称，比如这有6个类别\n",
    "num2detail_mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "\n",
    "classes = [num2detail_mapping[int(i)]for i in range(19)]\n",
    "print(classes)\n",
    "# 获取混淆矩阵\n",
    "cm = confusion_matrix(train_y, oof_y,normalize='true')\n",
    "cm = np.round(cm,2)\n",
    "plot_confusion_matrix(cm,classes, title='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:44:35.740664Z",
     "start_time": "2020-07-11T12:44:31.939200Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    sns.distplot(proba_test[i],label=num2detail_mapping[i])\n",
    "plt.xlim([0,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:47:07.814637Z",
     "start_time": "2020-07-11T12:47:07.753317Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T12:50:03.709385Z",
     "start_time": "2020-07-11T12:50:03.623527Z"
    }
   },
   "outputs": [],
   "source": [
    "class0_distribution = list(filter(lambda x:x>0.8,proba_test[1]))\n",
    "sum(sum(proba_test > 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net() missing 1 required positional argument: 'sample_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-500e94021d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'fold{fold}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mproba_test\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Net() missing 1 required positional argument: 'sample_num'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "labels = np.argmax(proba_test, axis=1)\n",
    "\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "scores = np.mean(np.array(oof_comm)[best_fold])\n",
    "print(scores)\n",
    "sub.to_csv('nn%.5f.csv' % scores, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T08:20:14.945162Z",
     "start_time": "2020-07-21T08:20:14.895692Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.load('0721_conv2_2_net_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
