{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:20:11.288922Z",
     "start_time": "2020-08-07T01:20:08.800567Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:20:12.099724Z",
     "start_time": "2020-08-07T01:20:11.294835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:09.573489Z",
     "start_time": "2020-08-07T01:21:08.631751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../../data/'\n",
    "train = pd.read_csv(root_path+'sensor_train.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test.csv')\n",
    "sub = pd.read_csv(root_path+'提交结果示例.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:11.031794Z",
     "start_time": "2020-08-07T01:21:10.956979Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "    df['thetax']=np.arctan(df.acc_xg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetay']=np.arctan(df.acc_yg/\n",
    "                           np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "    df['thetaz']=np.arctan(df.acc_zg/\n",
    "                           np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "    df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "    df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:11.901204Z",
     "start_time": "2020-08-07T01:21:11.602203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay', 'thetaz', 'xy',\n",
      "       'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:12.274713Z",
     "start_time": "2020-08-07T01:21:12.199265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'acc_xg',\n",
       " 'acc_yg',\n",
       " 'acc_zg',\n",
       " 'acc',\n",
       " 'accg',\n",
       " 'thetax',\n",
       " 'thetay',\n",
       " 'thetaz',\n",
       " 'xy',\n",
       " 'xy_g',\n",
       " 'g']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:12.799067Z",
     "start_time": "2020-08-07T01:21:12.713338Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:21:13.314418Z",
     "start_time": "2020-08-07T01:21:13.223131Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((7292, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((7500, sample_num, FEATURE_NUM, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:01.533955Z",
     "start_time": "2020-08-07T01:21:14.050595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/7292 [00:00<00:41, 176.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'behavior_id', 'acc_x', 'acc_y', 'acc_z',\n",
      "       'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'thetax', 'thetay',\n",
      "       'thetaz', 'xy', 'xy_g', 'g'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7292/7292 [00:23<00:00, 306.65it/s]\n",
      "100%|██████████| 7500/7500 [00:23<00:00, 320.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train[['fragment_id', 'time_point', 'behavior_id']+group1]\n",
    "test = test[['fragment_id', 'time_point']+group1]\n",
    "print(train.columns)\n",
    "\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:sample_num]\n",
    "    x[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:sample_num]\n",
    "    t[i,:,:,0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1)[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:13.878811Z",
     "start_time": "2020-08-07T01:22:13.671666Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n",
      "(32, 60, 14, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "# 一个完成了的generator\n",
    "def data_generator(data,label,class17label,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    class17label: series\n",
    "    \"\"\"\n",
    "    class17label=np.asarray(class17label)\n",
    "    length=len(data)\n",
    "    seq_length=len(data[0])\n",
    "    half_seq_length=int(seq_length/2)\n",
    "    \n",
    "    # index2label\n",
    "    index2label=dict(zip(range(length),class17label))\n",
    "    \n",
    "    label2index={}\n",
    "#     print(class17label)\n",
    "    for i in range(length):\n",
    "#         print(class17label[i],label2index.get(class17label[i],[]))\n",
    "        label2index[class17label[i]]=label2index.get(class17label[i],[])\n",
    "        label2index[class17label[i]].append(i)\n",
    "\n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        if random.choice([0,1,1,1,1,1,1]):\n",
    "            # minxup\n",
    "            #one specific index -> label -> all the index belong to this\n",
    "            choice_index=[random.choice(label2index[index2label[x]]) for x in inds]   # get the random choice seq(waiting for concat)\n",
    "            # 1st 前1/2 seq_length 点原始  后1/2 seq_length 点随机\n",
    "            res_x_orig=data[inds,:half_seq_length]\n",
    "            res_x=data[choice_index,half_seq_length:]\n",
    "\n",
    "    #         print(inds)\n",
    "    #         print(data.shape,res_x_orig.shape,res_x.shape,np.concatenate((res_x_orig,res_x),axis=1).shape)\n",
    "            yield np.concatenate((res_x_orig,res_x),axis=1),\\\n",
    "                    [label[0][inds],label[1][inds],label[2][inds]]\n",
    "        else:\n",
    "        \n",
    "            yield data[inds],[label[0][inds],label[1][inds],label[2][inds]]\n",
    "            \n",
    "    \n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x,[y,y,y],y,32):\n",
    "    print(a.shape,b[0].shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:15.720324Z",
     "start_time": "2020-08-07T01:22:15.637112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ConvBNRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "#                activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def ConvRelu(X,filters,kernal_size=(3,3)):\n",
    "    X = Conv2D(filters=filters,\n",
    "               kernel_size=kernal_size,\n",
    "               activation='relu',\n",
    "               use_bias=False,\n",
    "               padding='same')(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def squeeze_excitation_layer(x, out_dim,ratio=8):\n",
    "    '''\n",
    "    SE module performs inter-channel weighting.\n",
    "    '''\n",
    "    squeeze = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    excitation = Dense(units=out_dim // ratio)(squeeze)\n",
    "    excitation = Activation('relu')(excitation)\n",
    "    excitation = Dense(units=out_dim)(excitation)\n",
    "    excitation = Activation('sigmoid')(excitation)\n",
    "    excitation = Reshape((1,1,out_dim))(excitation)\n",
    "    scale = multiply([x,excitation])\n",
    "    return scale\n",
    "\n",
    "# def SE_Residual(X):\n",
    "#     A = \n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "#     X =  Add()([X,A])\n",
    "    \n",
    "\n",
    "def lenet5(input):\n",
    "    A = ConvBNRelu(input,64,kernal_size=(3,3))\n",
    "#     B = ConvBNRelu(input,16,kernal_size=(5,1))\n",
    "#     C = ConvBNRelu(input,16,kernal_size=(7,1))\n",
    "#     ABC = layers.Concatenate()([A,B,C])\n",
    "    X = ConvBNRelu(A,128)\n",
    "#     X = squeeze_excitation_layer(X,128)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    X = AveragePooling2D()(X)\n",
    "    \n",
    "    X = ConvBNRelu(X,256)\n",
    "    X = Dropout(0.3)(X)\n",
    "#     X = squeeze_excitation_layer(X,256)\n",
    "    X = ConvBNRelu(X,512)   \n",
    "    X = Dropout(0.5)(X)\n",
    "#     X = squeeze_excitation_layer(X,512)\n",
    "#     X = GlobalMaxPooling2D()(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "#     X = BatchNormalization()(X)\n",
    "    return X\n",
    "import tensorflow as tf\n",
    "def Net(sample_num):\n",
    "    input1 = Input(shape=(sample_num, FEATURE_NUM, 1))\n",
    "    part = tf.split(input1,axis=2, num_or_size_splits = [6, 2, 6])\n",
    "#     res = tf.split(c, axis = 3, num_or_size_splits = [2, 2, 4])\n",
    "    \n",
    "    \n",
    "    X1 = Concatenate(axis=-2)([part[0],part[1]])\n",
    "    X1 = lenet5(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dense(128, activation='relu')(X1)\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dropout(0.2)(X1)\n",
    "\n",
    "    X2 = Concatenate(axis=-2)([part[0],part[2]])\n",
    "    X2 = lenet5(X2)    \n",
    "    X2 = BatchNormalization()(X2)\n",
    "#     X = Dense(512, activation='relu')(X)\n",
    "#     X = BatchNormalization()(X)\n",
    "    X2 = Dense(128, activation='relu')(X2)\n",
    "    X2 = BatchNormalization()(X2)\n",
    "    X2 = Dropout(0.2)(X2)\n",
    "    \n",
    "    X = Concatenate(axis=-1)([X1,X2])\n",
    "    \n",
    "#     X = Dense(256)(X)    \n",
    "    \n",
    "    output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "    X = Dense(64)(X)\n",
    "    output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(19, activation='softmax',name='19class')(X) #小类\n",
    "    \n",
    "    \n",
    "    return Model([input1], [output1,output2,output3])\n",
    "\n",
    "# model = Net(60)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:18.453148Z",
     "start_time": "2020-08-07T01:22:16.283660Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=['A', 'B', 'C', 'D'], y=0         A\n",
      "1         A\n",
      "2         A\n",
      "3         A\n",
      "4         A\n",
      "         ..\n",
      "425354    C\n",
      "425355    C\n",
      "425356    C\n",
      "425357    C\n",
      "425358    C\n",
      "Name: behavior_id, Length: 425359, dtype: object as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0, 1, 2, 3, 4, 5, 6], y=0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "425354    6\n",
      "425355    6\n",
      "425356    6\n",
      "425357    6\n",
      "425358    6\n",
      "Name: behavior_id, Length: 425359, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18], y=0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "425354    18\n",
      "425355    18\n",
      "425356    18\n",
      "425357    18\n",
      "425358    18\n",
      "Name: behavior_id, Length: 425359, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 0.7453285438934641,\n",
       "  1: 0.7866762590992484,\n",
       "  2: 0.8681079382183908,\n",
       "  3: 4.251549256356949},\n",
       " {0: 1.757195321956318,\n",
       "  1: 0.4815479398086302,\n",
       "  2: 0.9616022823865589,\n",
       "  3: 1.1412232173040497,\n",
       "  4: 2.4294567179182565,\n",
       "  5: 1.2367567913331454,\n",
       "  6: 0.8210787010495146},\n",
       " {0: 1.6146639588513296,\n",
       "  1: 0.5658077637797579,\n",
       "  2: 0.9428223116223914,\n",
       "  3: 1.1544614165363905,\n",
       "  4: 0.895063001338305,\n",
       "  5: 1.120093850476494,\n",
       "  6: 0.5129293816036677,\n",
       "  7: 1.6147804233607677,\n",
       "  8: 1.1629774436090226,\n",
       "  9: 1.3287818013695207,\n",
       "  10: 1.639495846903968,\n",
       "  11: 0.8571276001942526,\n",
       "  12: 0.5209381219191084,\n",
       "  13: 1.3164363042146117,\n",
       "  14: 3.1705588145409553,\n",
       "  15: 0.8020390423628304,\n",
       "  16: 1.108447580802777,\n",
       "  17: 1.4649467209444893,\n",
       "  18: 1.12076674790857})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个输出    \n",
    "mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "# 每一个大类输出 4\n",
    "new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id'])\n",
    "classweights1=compute_class_weight(\"balanced\",['A','B','C','D'],\\\n",
    "                                   pd.read_csv(root_path+'sensor_train.csv')['behavior_id'].apply(lambda x:mapping[x][0]))\n",
    "classweights1=pd.DataFrame(classweights1)[0].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "classweights2=compute_class_weight(\"balanced\",list(range(7)),\\\n",
    "                                   pd.read_csv(root_path+'sensor_train.csv')['behavior_id'].apply(lambda x:int(mapping[x][2])))\n",
    "classweights2=pd.DataFrame(classweights2)[0].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id'])\n",
    "classweights3=compute_class_weight(\"balanced\",np.array(range(19)), pd.read_csv(root_path+'sensor_train.csv')['behavior_id'])\n",
    "classweights3=pd.DataFrame(classweights3)[0].to_dict()\n",
    "classweights1,classweights2,classweights3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:28.734100Z",
     "start_time": "2020-08-07T01:22:18.455113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 60, 6, 1), ( 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 8, 1)     0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 12, 1)    0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 60, 8, 64)    576         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 12, 64)   576         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 60, 8, 64)    256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 60, 12, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 60, 8, 64)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 12, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 8, 128)   73728       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 60, 12, 128)  73728       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 8, 128)   512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 60, 12, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 60, 8, 128)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 60, 12, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 60, 8, 128)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 60, 12, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 30, 4, 128)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 30, 6, 128)   0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 4, 256)   294912      average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 30, 6, 256)   294912      average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 4, 256)   1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 30, 6, 256)   1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 30, 4, 256)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 30, 6, 256)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 4, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 30, 6, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 30, 4, 512)   1179648     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 6, 512)   1179648     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 30, 4, 512)   2048        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 30, 6, 512)   2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 30, 4, 512)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 30, 6, 512)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30, 4, 512)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 30, 6, 512)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 512)          2048        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512)          2048        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           dropout_3[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "4class (Dense)                  (None, 4)            1028        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "7class (Dense)                  (None, 7)            455         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 19)           1235        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,265,182\n",
      "Trainable params: 3,258,782\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 182 steps, validate on 1459 samples\n",
      "Epoch 1/1000\n",
      "  1/182 [..............................] - ETA: 22:17WARNING:tensorflow:Reduce LR on plateau conditioned on metric `19class_acc` which is not available. Available metrics are: lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_19class_acc` which is not available. Available metrics are: \n",
      "WARNING:tensorflow:Can save best model only with val_19class_acc available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9ae2895ad422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplateau3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                          class_weight=[classweights1,classweights2,classweights3])\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/zq/miniconda3/envs/TF2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "    \n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((7500, 19))\n",
    "proba_oof = np.zeros((7292,19))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "# 每一个大类输出 4\n",
    "new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# 每一个大类输出 \n",
    "new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "y_3 = to_categorical(y, num_classes=19)\n",
    "# y_3=y\n",
    "\n",
    "\n",
    "for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "    4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "    8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "    12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "    16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "    \n",
    "    model = Net(60)\n",
    "    model.summary()\n",
    "    model.compile(loss=[custom_loss,custom_loss,custom_loss],loss_weights=[3,7,21],\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=[\"acc\"])#'',localscore\n",
    "\n",
    "    plateau3 = ReduceLROnPlateau(monitor=\"19class_acc\",\n",
    "                                verbose=1,\n",
    "                                mode='max',\n",
    "                                factor=0.5,\n",
    "                                patience=18)\n",
    "    early_stopping = EarlyStopping(monitor=\"val_19class_acc\",\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   patience=60)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(f'Conv2d_multiloss_mixup_fold{fold}.h5',\n",
    "                                 monitor=\"val_19class_acc\",\n",
    "                                 verbose=0,\n",
    "                                 mode='max',\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    train_res = model.fit(data_generator(train[xx], [y_1[xx], y_2[xx], y_3[xx]],y[xx],32),\n",
    "              epochs=1000,\n",
    "              steps_per_epoch=len(xx) // 32,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(train[yy], [y_1[yy], y_2[yy],y_3[yy]]),\n",
    "              callbacks=[plateau3, early_stopping, checkpoint],\n",
    "                         class_weight=[classweights1,classweights2,classweights3])\n",
    "\n",
    "    history.append(train_res)\n",
    "    \n",
    "    model.load_weights(f'Conv2d_multiloss_mixup_fold{fold}.h5')\n",
    "    proba_t += model.predict(test, verbose=0, batch_size=1024)[2] / fold_num \n",
    "    proba_oof[yy] += model.predict(train[yy],verbose=0,batch_size=1024) [2]\n",
    "\n",
    "    oof_y = np.argmax(proba_oof[yy], axis=1)\n",
    "    acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "    print(acc)\n",
    "    oof_score.append(acc)\n",
    "    scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "    oof_comm.append(scores)   \n",
    "    print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:28.737018Z",
     "start_time": "2020-08-07T01:22:16.047Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "joblib.dump(oof_dict,\"0730_generator_one_sixth_orig_mixup_%.5f_dict.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:01.606884Z",
     "start_time": "2020-08-07T01:21:26.497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t, axis=1)\n",
    "oof_y = np.argmax(proba_oof, axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../../../zp/jiaozibei/data/'\n",
    "sub = pd.read_csv(data_path+'提交结果示例.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "# sns.barplot(vc.index, vc.values)\n",
    "# plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "# sns.barplot(vc.index, vc.values)\n",
    "# plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "# sns.barplot(vc.index, vc.values)\n",
    "# plt.show()\n",
    "sub.to_csv('0729_generator_one_third_orig_mixup_%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T01:22:33.989471Z",
     "start_time": "2020-08-07T01:22:33.944708Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# %matplotlib inline\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_confusion_matrix(cm,classes,title='Confusion Matrix'):\n",
    "\n",
    "#     plt.figure(figsize=(12, 9), dpi=100)\n",
    "#     np.set_printoptions(precision=2)\n",
    "    \n",
    "#     sns.heatmap(cm,annot=True)\n",
    "#     plt.title(title)\n",
    "#     plt.xticks(ticks=range(19),labels=classes)\n",
    "#     plt.yticks(ticks=range(19),labels=classes)\n",
    "    \n",
    "#     plt.ylabel('Actual label')\n",
    "#     plt.xlabel('Predict label')\n",
    "#     plt.show()\n",
    "    \n",
    "# # classes表示不同类别的名称，比如这有6个类别\n",
    "# num2detail_mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "#         4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "#         8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "#         12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "#         16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "\n",
    "# classes = [num2detail_mapping[int(i)]for i in range(19)]\n",
    "# print(classes)\n",
    "# # 获取混淆矩阵\n",
    "# cm = confusion_matrix(train_y, oof_y,normalize='true')\n",
    "# cm = np.round(cm,2)\n",
    "# plot_confusion_matrix(cm,classes, title='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
