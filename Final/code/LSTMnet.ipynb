{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:11.670321Z",
     "start_time": "2020-08-21T14:44:10.203118Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:12.188266Z",
     "start_time": "2020-08-21T14:44:11.672589Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 选择比较好的模型\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "sample_num = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:12.866839Z",
     "start_time": "2020-08-21T14:44:12.349957Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path  = '../data/'\n",
    "train = pd.read_csv(root_path+'sensor_train_final.csv')\n",
    "test = pd.read_csv(root_path+'sensor_test_final.csv')\n",
    "sub = pd.read_csv(root_path+'submit_example.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "# data_test['fragment_id'] += 100000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:12.944961Z",
     "start_time": "2020-08-21T14:44:12.868674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg'],\n",
      "      dtype='object')\n",
      "Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n",
      "       'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_features(df):\n",
    "    print(df.columns)\n",
    "    df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n",
    "    df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n",
    "#     df['thetax']=np.arctan(df.acc_xg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetay']=np.arctan(df.acc_yg/\n",
    "#                            np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg))*180/np.pi\n",
    "#     df['thetaz']=np.arctan(df.acc_zg/\n",
    "#                            np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg))*180/np.pi\n",
    "\n",
    "#     df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n",
    "#     df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5    \n",
    "    \n",
    "    df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + \n",
    "                 (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "train=add_features(train)\n",
    "test=add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:14.378642Z",
     "start_time": "2020-08-21T14:44:12.946386Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "# group1 = [\"acc_x\",\"acc_y\",\"acc_z\",\"acc\",\"acc_xg\",\"acc_yg\",\"acc_zg\",\"accg\"]\n",
    "\n",
    "\n",
    "test['fragment_id'] += 15000\n",
    "data = pd.concat([train, test], sort=False)\n",
    "ss_tool = StandardScaler()\n",
    "data[group1] = ss_tool.fit_transform(data[group1])\n",
    "\n",
    "\n",
    "train = data[data[\"behavior_id\"].isna()==False].reset_index(drop=True)\n",
    "test = data[data[\"behavior_id\"].isna()==True].reset_index(drop=True)\n",
    "test['fragment_id'] -= 15000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:14.415681Z",
     "start_time": "2020-08-21T14:44:14.380495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_x', 'acc_y', 'acc_z', 'acc_xg', 'acc_yg', 'acc_zg', 'acc', 'accg', 'g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = [x for x in train.columns if x not in ['fragment_id', 'time_point','behavior_id']]\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:14.446271Z",
     "start_time": "2020-08-21T14:44:14.417224Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_NUM=len(group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:44:14.475278Z",
     "start_time": "2020-08-21T14:44:14.448417Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.zeros((15000, sample_num, FEATURE_NUM, 1))\n",
    "t = np.zeros((16000, sample_num, FEATURE_NUM, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:45:59.844316Z",
     "start_time": "2020-08-21T14:44:14.476693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:46<00:00, 323.82it/s]\n",
      "100%|██████████| 16000/16000 [00:49<00:00, 324.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def df2array(df,num,x):\n",
    "    for i in tqdm(range(num)):\n",
    "        tmp = df[df.fragment_id == i][:sample_num]\n",
    "        length=len(tmp)\n",
    "        if length<50:\n",
    "            new_tmp=tmp.copy()\n",
    "            while len(tmp)<60:\n",
    "                new_tmp['time_point']=tmp['time_point'].max()+new_tmp['time_point']\n",
    "                tmp=pd.concat([tmp,new_tmp],ignore_index=True)\n",
    "        x[i,:,:,0] = resample(tmp[group1], sample_num, np.array(tmp.time_point))[0].reshape(sample_num,FEATURE_NUM)\n",
    "    return x\n",
    "x=df2array(train,15000,x)\n",
    "t=df2array(test,16000,t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:45:59.932384Z",
     "start_time": "2020-08-21T14:45:59.846370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "(32, 60, 9, 1) (32,)\n",
      "CPU times: user 26.2 ms, sys: 2.02 ms, total: 28.2 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 一个完成了的generator\n",
    "def data_generator(data,label,class17label,batch_size):\n",
    "    \"\"\"\n",
    "    data:array  (7292, 60, 14, 1)\n",
    "    label:array (7292,)\n",
    "    class17label: series\n",
    "    \"\"\"\n",
    "    class17label=np.asarray(class17label)\n",
    "    length=len(data)\n",
    "    seq_length=len(data[0])\n",
    "    half_seq_length=int(seq_length/2)\n",
    "    \n",
    "    # index2label\n",
    "    index2label=dict(zip(range(length),class17label))\n",
    "    \n",
    "    label2index={}\n",
    "#     print(class17label)\n",
    "    for i in range(length):\n",
    "#         print(class17label[i],label2index.get(class17label[i],[]))\n",
    "        label2index[class17label[i]]=label2index.get(class17label[i],[])\n",
    "        label2index[class17label[i]].append(i)\n",
    "\n",
    "    count=0\n",
    "    np.random.seed(seed)# 保证结果可重复\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if count==0 or (count + 1) * batch_size > length:  # 如果是第一个或者最后一个batch\n",
    "            count=0\n",
    "            shuffle_index = list(range(length))\n",
    "            np.random.shuffle(shuffle_index)   ## 对索引进行打乱\n",
    "        \n",
    "        start = count * batch_size  ## batch的起始点\n",
    "        end = (count + 1) * batch_size ## batch的终点\n",
    "        inds=shuffle_index[start:end]\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        if random.choice([0,1,1,1]):\n",
    "            # minxup\n",
    "            #one specific index -> label -> all the index belong to this\n",
    "            choice_index=[random.choice(label2index[index2label[x]]) for x in inds]   # get the random choice seq(waiting for concat)\n",
    "            # 1st 前1/2 seq_length 点原始  后1/2 seq_length 点随机\n",
    "            res_x_orig=data[inds,:half_seq_length]   #原始\n",
    "            res_x=data[choice_index,half_seq_length:]   #需要加入的\n",
    "\n",
    "    #         print(inds)\n",
    "    #         print(data.shape,res_x_orig.shape,res_x.shape,np.concatenate((res_x_orig,res_x),axis=1).shape)\n",
    "#             yield np.concatenate((res_x_orig,res_x),axis=1),\\\n",
    "#                     [label[0][inds],label[1][inds],label[2][inds]]\n",
    "            yield np.concatenate((res_x_orig,res_x),axis=1),label[inds]\n",
    "        else:\n",
    "        \n",
    "            yield data[inds],label[inds]\n",
    "            \n",
    "    \n",
    "\n",
    "count=0\n",
    "for a,b in data_generator(x,y,y,32):\n",
    "    print(a.shape,b.shape)\n",
    "    count+=1\n",
    "    if count==20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM,LayerNormalization,Dropout,GlobalAveragePooling1D,Input,Concatenate,BatchNormalization,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from cos_dense_attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T14:46:04.286916Z",
     "start_time": "2020-08-21T14:45:59.967707Z"
    },
    "code_folding": [
     21,
     36
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 60, 3), (Non 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 3)        0           tf_op_layer_split_1[0][2]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "                                                                 tf_op_layer_split_1[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_1[0][0]        \n",
      "                                                                 tf_op_layer_split_1[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_1[0][1]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 60, 32)       5376        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 60, 16)       1280        tf_op_layer_split_1[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 60, 16)       1280        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 60, 32)       4736        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 60, 32)       4736        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 60, 32)       64          lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 60, 16)       32          lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 60, 16)       32          lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 60, 16)       32          lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 60, 32)       64          lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 60, 32)       64          lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 60, 32)       0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 60, 16)       0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 60, 16)       0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 60, 16)       0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 60, 32)       0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 60, 32)       0           layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 60, 32)       8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 60, 16)       2112        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 60, 16)       2112        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 60, 16)       2112        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 60, 32)       8320        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 60, 32)       8320        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 60, 32)       64          lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 60, 16)       32          lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 60, 16)       32          lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 60, 16)       32          lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 60, 32)       64          lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 60, 32)       64          lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60, 32)       0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60, 16)       0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 60, 16)       0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 60, 16)       0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 60, 32)       0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 60, 32)       0           layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 60, 32)       8320        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 60, 16)       2112        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 60, 16)       2112        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 60, 16)       2112        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 60, 32)       8320        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 60, 32)       8320        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 60, 32)       0           lstm_5[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 60, 16)       0           lstm_8[0][0]                     \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 60, 16)       0           lstm_11[0][0]                    \n",
      "                                                                 lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 60, 16)       0           lstm_14[0][0]                    \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 60, 32)       0           lstm_17[0][0]                    \n",
      "                                                                 lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 60, 32)       0           lstm_20[0][0]                    \n",
      "                                                                 lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 60, 32)       64          attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 60, 16)       32          attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 60, 16)       32          attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 60, 16)       32          attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 60, 32)       64          attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 60, 32)       64          attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 60, 32)       0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 60, 16)       0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 60, 16)       0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 60, 16)       0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 60, 32)       0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 60, 32)       0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 16)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 16)           0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 16)           0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 144)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 144)          576         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 144)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          37120       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def LSTM_A(input,INPUT_SIZE = 8,CELL_SIZE = 64):\n",
    "    TIME_STEPS = 60\n",
    "    OUTPUT_SIZE = 19\n",
    "    \n",
    "    activateion_fun = 'tanh'\n",
    "#     inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,INPUT_SIZE), return_sequences=True, activation=activateion_fun)(input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)# here change from 02 to 05\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True,activation=activateion_fun)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "#     x = Attention()([x,x])\n",
    "    x = Dropout(0.5)(x)  # here change from 03 to 05\n",
    "    x = LSTM(CELL_SIZE, input_shape = (TIME_STEPS,CELL_SIZE), return_sequences=True)(x)\n",
    "    x = Attention()([x,x])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalAveragePooling1D()(x)   \n",
    "    return x\n",
    "\n",
    "import tensorflow as tf\n",
    "def LSTM_Model():\n",
    "    \n",
    "    TIME_STEPS = 60\n",
    "    INPUT_SIZE = len(group1)\n",
    "    OUTPUT_SIZE = 19\n",
    "    activateion_fun = 'tanh'\n",
    "    inputs = Input(shape=[TIME_STEPS,INPUT_SIZE])\n",
    "    part = tf.split(inputs,axis=2, num_or_size_splits = [3,3,1,1,1])\n",
    "    A = LSTM_A(inputs,CELL_SIZE = 32)\n",
    "    A1 = LSTM_A(part[0],3,CELL_SIZE = 16)\n",
    "    A2 = LSTM_A(part[1],3,CELL_SIZE = 16)\n",
    "    A3 = LSTM_A(Concatenate()([part[2],part[3],part[4]]),3,CELL_SIZE = 16)\n",
    "    A4 = LSTM_A(Concatenate()([part[0],part[2]]),4,CELL_SIZE = 32)\n",
    "    A5 = LSTM_A(Concatenate()([part[1],part[3]]),4,CELL_SIZE = 32)\n",
    "    \n",
    "#     A4 = LSTM_A(part[3],6,CELL_SIZE = 16)    \n",
    "#     B = LSTM_B(inputs,INPUT_SIZE=9)\n",
    "#     C = LSTM_C(inputs,CELL_SIZE=46)\n",
    "    x = Concatenate()([A,A1,A2,A3,A4,A5])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)   # add\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    X = BatchNormalization()(x)\n",
    "#     output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n",
    "    print(X.shape)\n",
    "#     output2 = Dense(128)(X)\n",
    "#     output2 = Dense(64)(X)\n",
    "    X = Dense(64)(X)\n",
    "#     output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n",
    "#     X = Dense(64)(X)\n",
    "#     X = Dense(32)(X)\n",
    "#     X = Concatenate(axis=-1)([X,output1,output2])\n",
    "    X = Dense(64)(X)\n",
    "    output3 = Dense(20, activation='softmax',name='19class')(X) #小类\n",
    "    print(output3.shape)\n",
    "    return Model([inputs], output3)\n",
    "\n",
    "LSTM_Model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:25:33.904101Z",
     "start_time": "2020-08-21T14:46:04.288976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_2 (TensorFlow [(None, 60, 3), (Non 0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 60, 3)        0           tf_op_layer_split_2[0][2]        \n",
      "                                                                 tf_op_layer_split_2[0][3]        \n",
      "                                                                 tf_op_layer_split_2[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_2[0][0]        \n",
      "                                                                 tf_op_layer_split_2[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_2[0][1]        \n",
      "                                                                 tf_op_layer_split_2[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 60, 32)       5376        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_2[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  (None, 60, 16)       1280        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 60, 32)       4736        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, 60, 32)       4736        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 60, 32)       64          lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 60, 16)       32          lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 60, 16)       32          lstm_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 60, 16)       32          lstm_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 60, 32)       64          lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 60, 32)       64          lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 60, 32)       0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 60, 16)       0           layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 60, 16)       0           layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 60, 16)       0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 60, 32)       0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 60, 32)       0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 60, 32)       8320        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  (None, 60, 16)       2112        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  (None, 60, 16)       2112        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 60, 16)       2112        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, 60, 32)       8320        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (None, 60, 32)       8320        dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 60, 32)       64          lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 60, 16)       32          lstm_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 60, 16)       32          lstm_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 60, 16)       32          lstm_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 60, 32)       64          lstm_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 60, 32)       64          lstm_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 60, 32)       0           layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 60, 16)       0           layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 60, 16)       0           layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 60, 16)       0           layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 60, 32)       0           layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 60, 32)       0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 60, 32)       8320        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                  (None, 60, 16)       2112        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 60, 16)       2112        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 60, 16)       2112        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 60, 32)       8320        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (None, 60, 32)       8320        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 60, 32)       0           lstm_23[0][0]                    \n",
      "                                                                 lstm_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 60, 16)       0           lstm_26[0][0]                    \n",
      "                                                                 lstm_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 60, 16)       0           lstm_29[0][0]                    \n",
      "                                                                 lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 60, 16)       0           lstm_32[0][0]                    \n",
      "                                                                 lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 60, 32)       0           lstm_35[0][0]                    \n",
      "                                                                 lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 60, 32)       0           lstm_38[0][0]                    \n",
      "                                                                 lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 60, 32)       64          attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 60, 16)       32          attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 60, 16)       32          attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 60, 16)       32          attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 60, 32)       64          attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 60, 32)       64          attention_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 32)       0           layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 60, 16)       0           layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 60, 16)       0           layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 60, 16)       0           layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 60, 32)       0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 60, 32)       0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 16)           0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 16)           0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 16)           0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 32)           0           dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 32)           0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 144)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 144)          576         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 144)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          37120       dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16448       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 40s 3ms/sample - loss: 2.0943 - acc: 0.3931 - val_loss: 1.6759 - val_acc: 0.4907\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 1.6203 - acc: 0.5098 - val_loss: 1.4630 - val_acc: 0.5790\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 1.4918 - acc: 0.5624 - val_loss: 1.3422 - val_acc: 0.6107\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 1.4071 - acc: 0.5894 - val_loss: 1.2678 - val_acc: 0.6470\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 1.3464 - acc: 0.6148 - val_loss: 1.2215 - val_acc: 0.6557\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 1.2951 - acc: 0.6325 - val_loss: 1.1897 - val_acc: 0.6657\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 1.2514 - acc: 0.6499 - val_loss: 1.1534 - val_acc: 0.6923\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 1.2111 - acc: 0.6679 - val_loss: 1.1357 - val_acc: 0.6883\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 1.1692 - acc: 0.6858 - val_loss: 1.1037 - val_acc: 0.7083\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 1.1496 - acc: 0.6899 - val_loss: 1.0815 - val_acc: 0.7267\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.1203 - acc: 0.6990 - val_loss: 1.0622 - val_acc: 0.7317\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 1.1024 - acc: 0.7139 - val_loss: 1.0677 - val_acc: 0.7207\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 1.0792 - acc: 0.7207 - val_loss: 1.0779 - val_acc: 0.7260\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 1.0718 - acc: 0.7248 - val_loss: 1.0430 - val_acc: 0.7333\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 1.0496 - acc: 0.7305 - val_loss: 1.0367 - val_acc: 0.7463\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 1.0470 - acc: 0.7350 - val_loss: 1.0341 - val_acc: 0.7440\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 1.0213 - acc: 0.7416 - val_loss: 0.9947 - val_acc: 0.7533\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 1.0030 - acc: 0.7431 - val_loss: 0.9995 - val_acc: 0.7597\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 1.0005 - acc: 0.7484 - val_loss: 1.0042 - val_acc: 0.7577\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.9822 - acc: 0.7584 - val_loss: 0.9799 - val_acc: 0.7613\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.9722 - acc: 0.7623 - val_loss: 0.9945 - val_acc: 0.7620\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.9601 - acc: 0.7650 - val_loss: 0.9925 - val_acc: 0.7583\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.9502 - acc: 0.7701 - val_loss: 0.9748 - val_acc: 0.7717\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.9470 - acc: 0.7700 - val_loss: 0.9747 - val_acc: 0.7750\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.9351 - acc: 0.7754 - val_loss: 0.9924 - val_acc: 0.7653\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.9263 - acc: 0.7826 - val_loss: 0.9661 - val_acc: 0.7727\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.9035 - acc: 0.7880 - val_loss: 0.9668 - val_acc: 0.7773\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.9148 - acc: 0.7853 - val_loss: 0.9792 - val_acc: 0.7667\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.8925 - acc: 0.7937 - val_loss: 0.9758 - val_acc: 0.7687\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.8873 - acc: 0.7976 - val_loss: 0.9635 - val_acc: 0.7800\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.8785 - acc: 0.8014 - val_loss: 0.9457 - val_acc: 0.7940\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.8756 - acc: 0.7992 - val_loss: 0.9418 - val_acc: 0.7883\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.8582 - acc: 0.8035 - val_loss: 0.9652 - val_acc: 0.7797\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.8607 - acc: 0.8093 - val_loss: 0.9552 - val_acc: 0.7817\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.8507 - acc: 0.8120 - val_loss: 0.9559 - val_acc: 0.7833\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.8555 - acc: 0.8070 - val_loss: 0.9405 - val_acc: 0.7890\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.8417 - acc: 0.8164 - val_loss: 0.9736 - val_acc: 0.7737\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.8429 - acc: 0.8153 - val_loss: 0.9570 - val_acc: 0.7863\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.8315 - acc: 0.8198 - val_loss: 0.9380 - val_acc: 0.7947\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.8188 - acc: 0.8245 - val_loss: 0.9482 - val_acc: 0.7843\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.8263 - acc: 0.8225 - val_loss: 0.9460 - val_acc: 0.7893\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.8132 - acc: 0.8264 - val_loss: 0.9339 - val_acc: 0.7910\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.8103 - acc: 0.8260 - val_loss: 0.9344 - val_acc: 0.7927\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.8055 - acc: 0.8264 - val_loss: 0.9395 - val_acc: 0.7910\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.7955 - acc: 0.8347 - val_loss: 0.9295 - val_acc: 0.7990\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.7915 - acc: 0.8328 - val_loss: 0.9301 - val_acc: 0.8003\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.7876 - acc: 0.8348 - val_loss: 0.9259 - val_acc: 0.7987\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.7900 - acc: 0.8415 - val_loss: 0.9305 - val_acc: 0.7987\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.7705 - acc: 0.8478 - val_loss: 0.9287 - val_acc: 0.7997\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.7835 - acc: 0.8399 - val_loss: 0.9344 - val_acc: 0.7993\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.7802 - acc: 0.8393 - val_loss: 0.9304 - val_acc: 0.7960\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.7718 - acc: 0.8447 - val_loss: 0.9406 - val_acc: 0.7903\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.7640 - acc: 0.8433 - val_loss: 0.9447 - val_acc: 0.7933\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.7652 - acc: 0.8442 - val_loss: 0.9263 - val_acc: 0.7963\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.7527 - acc: 0.8514 - val_loss: 0.9209 - val_acc: 0.8053\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.7511 - acc: 0.8547 - val_loss: 0.9128 - val_acc: 0.8077\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.7532 - acc: 0.8550 - val_loss: 0.9241 - val_acc: 0.8043\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.7446 - acc: 0.8578 - val_loss: 0.9218 - val_acc: 0.8033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.7447 - acc: 0.8565 - val_loss: 0.9262 - val_acc: 0.8047\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.7374 - acc: 0.8569 - val_loss: 0.9172 - val_acc: 0.8030\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.7387 - acc: 0.8553 - val_loss: 0.9271 - val_acc: 0.8027\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 409us/sample - loss: 0.7295 - acc: 0.8637 - val_loss: 0.9379 - val_acc: 0.8003\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.7313 - acc: 0.8610 - val_loss: 0.9218 - val_acc: 0.8117\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.7254 - acc: 0.8640 - val_loss: 0.9253 - val_acc: 0.8023\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.7273 - acc: 0.8610 - val_loss: 0.9126 - val_acc: 0.8110\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 6s 493us/sample - loss: 0.7233 - acc: 0.8620 - val_loss: 0.9346 - val_acc: 0.8007\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.7192 - acc: 0.8679 - val_loss: 0.9424 - val_acc: 0.8013\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.7170 - acc: 0.8686 - val_loss: 0.9242 - val_acc: 0.8053\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 6s 493us/sample - loss: 0.7144 - acc: 0.8665 - val_loss: 0.9422 - val_acc: 0.8043\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.7082 - acc: 0.8718 - val_loss: 0.9358 - val_acc: 0.8073\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.7148 - acc: 0.8656 - val_loss: 0.9390 - val_acc: 0.8083\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.7083 - acc: 0.8699 - val_loss: 0.9420 - val_acc: 0.8027\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.7061 - acc: 0.8722 - val_loss: 0.9205 - val_acc: 0.8027\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.7001 - acc: 0.8740 - val_loss: 0.9217 - val_acc: 0.8100\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.7032 - acc: 0.8735 - val_loss: 0.9339 - val_acc: 0.8067\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 6s 510us/sample - loss: 0.6979 - acc: 0.8761 - val_loss: 0.9285 - val_acc: 0.8123\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.6861 - acc: 0.8798 - val_loss: 0.9357 - val_acc: 0.8067\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 513us/sample - loss: 0.6958 - acc: 0.8767 - val_loss: 0.9305 - val_acc: 0.8100\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6894 - acc: 0.8747 - val_loss: 0.9236 - val_acc: 0.8083\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 6s 534us/sample - loss: 0.6842 - acc: 0.8804 - val_loss: 0.9095 - val_acc: 0.8160\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.6786 - acc: 0.8840 - val_loss: 0.9193 - val_acc: 0.8077\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.6767 - acc: 0.8848 - val_loss: 0.9294 - val_acc: 0.8050\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.6730 - acc: 0.8881 - val_loss: 0.9361 - val_acc: 0.8037\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.6713 - acc: 0.8832 - val_loss: 0.9444 - val_acc: 0.8030\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.6768 - acc: 0.8863 - val_loss: 0.9426 - val_acc: 0.8010\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6764 - acc: 0.8830 - val_loss: 0.9339 - val_acc: 0.8030\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.6692 - acc: 0.8893 - val_loss: 0.9315 - val_acc: 0.8003\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 504us/sample - loss: 0.6654 - acc: 0.8907 - val_loss: 0.9424 - val_acc: 0.8007\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.6598 - acc: 0.8935 - val_loss: 0.9397 - val_acc: 0.8047\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.6572 - acc: 0.8941 - val_loss: 0.9161 - val_acc: 0.8093\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 6s 489us/sample - loss: 0.6569 - acc: 0.8946 - val_loss: 0.9289 - val_acc: 0.8110\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 6s 506us/sample - loss: 0.6647 - acc: 0.8878 - val_loss: 0.9429 - val_acc: 0.8023\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.6599 - acc: 0.8910 - val_loss: 0.9354 - val_acc: 0.8063\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.6568 - acc: 0.8942 - val_loss: 0.9286 - val_acc: 0.8100\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 6s 517us/sample - loss: 0.6550 - acc: 0.8923 - val_loss: 0.9239 - val_acc: 0.8110\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.6480 - acc: 0.8956 - val_loss: 0.9253 - val_acc: 0.8103\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.6528 - acc: 0.8947 - val_loss: 0.9178 - val_acc: 0.8087\n",
      "Epoch 98/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.8956\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.6516 - acc: 0.8950 - val_loss: 0.9520 - val_acc: 0.7980\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.6264 - acc: 0.9096 - val_loss: 0.9213 - val_acc: 0.8070\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 6s 497us/sample - loss: 0.6176 - acc: 0.9114 - val_loss: 0.9199 - val_acc: 0.8117\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6126 - acc: 0.9118 - val_loss: 0.9144 - val_acc: 0.8157\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.6145 - acc: 0.9131 - val_loss: 0.9128 - val_acc: 0.8163\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.6122 - acc: 0.9112 - val_loss: 0.9127 - val_acc: 0.8140\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 6s 510us/sample - loss: 0.6113 - acc: 0.9140 - val_loss: 0.9225 - val_acc: 0.8127\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.6170 - acc: 0.9123 - val_loss: 0.9087 - val_acc: 0.8173\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 6s 494us/sample - loss: 0.6029 - acc: 0.9180 - val_loss: 0.9161 - val_acc: 0.8120\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.6045 - acc: 0.9162 - val_loss: 0.9132 - val_acc: 0.8127\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.6095 - acc: 0.9110 - val_loss: 0.9193 - val_acc: 0.8160\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 431us/sample - loss: 0.6055 - acc: 0.9142 - val_loss: 0.9087 - val_acc: 0.8110\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.6116 - acc: 0.9138 - val_loss: 0.9370 - val_acc: 0.8050\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.6061 - acc: 0.9151 - val_loss: 0.9224 - val_acc: 0.8190\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.6087 - acc: 0.9122 - val_loss: 0.9335 - val_acc: 0.8107\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.6021 - acc: 0.9164 - val_loss: 0.9250 - val_acc: 0.8160\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.6038 - acc: 0.9175 - val_loss: 0.9234 - val_acc: 0.8157\n",
      "Epoch 115/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.5987 - acc: 0.9187 - val_loss: 0.9130 - val_acc: 0.8133\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5923 - acc: 0.9210 - val_loss: 0.9072 - val_acc: 0.8137\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.6019 - acc: 0.9180 - val_loss: 0.9240 - val_acc: 0.8120\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5989 - acc: 0.9172 - val_loss: 0.9286 - val_acc: 0.8117\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 6s 502us/sample - loss: 0.5979 - acc: 0.9189 - val_loss: 0.9255 - val_acc: 0.8127\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5987 - acc: 0.9212 - val_loss: 0.9166 - val_acc: 0.8163\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5924 - acc: 0.9215 - val_loss: 0.9186 - val_acc: 0.8143\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 6s 507us/sample - loss: 0.5906 - acc: 0.9265 - val_loss: 0.9222 - val_acc: 0.8210\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.5978 - acc: 0.9216 - val_loss: 0.9448 - val_acc: 0.8067\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.6020 - acc: 0.9168 - val_loss: 0.9158 - val_acc: 0.8160\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.5945 - acc: 0.9219 - val_loss: 0.9239 - val_acc: 0.8197\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 6s 493us/sample - loss: 0.5907 - acc: 0.9197 - val_loss: 0.9252 - val_acc: 0.8147\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5926 - acc: 0.9217 - val_loss: 0.9214 - val_acc: 0.8147\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.5920 - acc: 0.9195 - val_loss: 0.9208 - val_acc: 0.8150\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.5930 - acc: 0.9233 - val_loss: 0.9293 - val_acc: 0.8147\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.5932 - acc: 0.9199 - val_loss: 0.9178 - val_acc: 0.8200\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5924 - acc: 0.9186 - val_loss: 0.9328 - val_acc: 0.8147\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5801 - acc: 0.9284 - val_loss: 0.9286 - val_acc: 0.8180\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.5917 - acc: 0.9214 - val_loss: 0.9272 - val_acc: 0.8173\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5849 - acc: 0.9248 - val_loss: 0.9205 - val_acc: 0.8200\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 6s 505us/sample - loss: 0.5812 - acc: 0.9236 - val_loss: 0.9244 - val_acc: 0.8240\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5894 - acc: 0.9233 - val_loss: 0.9198 - val_acc: 0.8173\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5818 - acc: 0.9258 - val_loss: 0.9122 - val_acc: 0.8207\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.5861 - acc: 0.9219 - val_loss: 0.9281 - val_acc: 0.8183\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5792 - acc: 0.9254 - val_loss: 0.9276 - val_acc: 0.8133\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.5789 - acc: 0.9288 - val_loss: 0.9315 - val_acc: 0.8160\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.5852 - acc: 0.9237 - val_loss: 0.9203 - val_acc: 0.8207\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.5759 - acc: 0.9298 - val_loss: 0.9203 - val_acc: 0.8193\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 6s 537us/sample - loss: 0.5797 - acc: 0.9266 - val_loss: 0.9159 - val_acc: 0.8227\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5789 - acc: 0.9258 - val_loss: 0.9198 - val_acc: 0.8213\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.5738 - acc: 0.9297 - val_loss: 0.9250 - val_acc: 0.8210\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5754 - acc: 0.9290 - val_loss: 0.9303 - val_acc: 0.8130\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 7s 551us/sample - loss: 0.5820 - acc: 0.9270 - val_loss: 0.9164 - val_acc: 0.8187\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5759 - acc: 0.9302 - val_loss: 0.9117 - val_acc: 0.8223\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5762 - acc: 0.9298 - val_loss: 0.9301 - val_acc: 0.8147\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.5738 - acc: 0.9307 - val_loss: 0.9293 - val_acc: 0.8157\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.5751 - acc: 0.9287 - val_loss: 0.9213 - val_acc: 0.8180\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5799 - acc: 0.9238 - val_loss: 0.9223 - val_acc: 0.8170\n",
      "Epoch 153/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.9280\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5783 - acc: 0.9276 - val_loss: 0.9282 - val_acc: 0.8180\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 6s 540us/sample - loss: 0.5650 - acc: 0.9322 - val_loss: 0.9150 - val_acc: 0.8203\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5557 - acc: 0.9390 - val_loss: 0.9198 - val_acc: 0.8173\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.5567 - acc: 0.9358 - val_loss: 0.9198 - val_acc: 0.8137\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5578 - acc: 0.9361 - val_loss: 0.9183 - val_acc: 0.8143\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5540 - acc: 0.9395 - val_loss: 0.9199 - val_acc: 0.8203\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5530 - acc: 0.9381 - val_loss: 0.9144 - val_acc: 0.8207\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5503 - acc: 0.9391 - val_loss: 0.9184 - val_acc: 0.8177\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 6s 508us/sample - loss: 0.5515 - acc: 0.9406 - val_loss: 0.9287 - val_acc: 0.8177\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.5505 - acc: 0.9402 - val_loss: 0.9268 - val_acc: 0.8167\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.5479 - acc: 0.9427 - val_loss: 0.9243 - val_acc: 0.8213\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 6s 499us/sample - loss: 0.5553 - acc: 0.9371 - val_loss: 0.9225 - val_acc: 0.8183\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.5579 - acc: 0.9361 - val_loss: 0.9237 - val_acc: 0.8193\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 6s 499us/sample - loss: 0.5469 - acc: 0.9408 - val_loss: 0.9251 - val_acc: 0.8163\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5508 - acc: 0.9425 - val_loss: 0.9287 - val_acc: 0.8180\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 6s 505us/sample - loss: 0.5492 - acc: 0.9402 - val_loss: 0.9206 - val_acc: 0.8183\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 7s 575us/sample - loss: 0.5504 - acc: 0.9386 - val_loss: 0.9242 - val_acc: 0.8153\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5466 - acc: 0.9415 - val_loss: 0.9195 - val_acc: 0.8207\n",
      "Epoch 171/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.9407\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.5510 - acc: 0.9401 - val_loss: 0.9205 - val_acc: 0.8190\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5439 - acc: 0.9423 - val_loss: 0.9184 - val_acc: 0.8187\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.5411 - acc: 0.9461 - val_loss: 0.9198 - val_acc: 0.8183\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.5402 - acc: 0.9446 - val_loss: 0.9165 - val_acc: 0.8217\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.5441 - acc: 0.9435 - val_loss: 0.9140 - val_acc: 0.8230\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.5416 - acc: 0.9453 - val_loss: 0.9202 - val_acc: 0.8203\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5419 - acc: 0.9469 - val_loss: 0.9193 - val_acc: 0.8213\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5413 - acc: 0.9444 - val_loss: 0.9158 - val_acc: 0.8227\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.5437 - acc: 0.9441 - val_loss: 0.9181 - val_acc: 0.8230\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5375 - acc: 0.9471 - val_loss: 0.9185 - val_acc: 0.8227\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5424 - acc: 0.9417 - val_loss: 0.9191 - val_acc: 0.8223\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.5457 - acc: 0.9407 - val_loss: 0.9151 - val_acc: 0.8233\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.5376 - acc: 0.9433 - val_loss: 0.9173 - val_acc: 0.8243\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.5405 - acc: 0.9441 - val_loss: 0.9187 - val_acc: 0.8243\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 7s 543us/sample - loss: 0.5423 - acc: 0.9422 - val_loss: 0.9159 - val_acc: 0.8253\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5391 - acc: 0.9447 - val_loss: 0.9173 - val_acc: 0.8243\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 6s 500us/sample - loss: 0.5437 - acc: 0.9452 - val_loss: 0.9199 - val_acc: 0.8197\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.5374 - acc: 0.9455 - val_loss: 0.9180 - val_acc: 0.8220\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.5363 - acc: 0.9473 - val_loss: 0.9140 - val_acc: 0.8267\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.5351 - acc: 0.9464 - val_loss: 0.9182 - val_acc: 0.8247\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5369 - acc: 0.9475 - val_loss: 0.9150 - val_acc: 0.8267\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5397 - acc: 0.9460 - val_loss: 0.9138 - val_acc: 0.8260\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.5371 - acc: 0.9471 - val_loss: 0.9162 - val_acc: 0.8237\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5393 - acc: 0.9453 - val_loss: 0.9186 - val_acc: 0.8213\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5349 - acc: 0.9478 - val_loss: 0.9186 - val_acc: 0.8200\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5370 - acc: 0.9442 - val_loss: 0.9160 - val_acc: 0.8253\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.5338 - acc: 0.9479 - val_loss: 0.9160 - val_acc: 0.8227\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.5320 - acc: 0.9492 - val_loss: 0.9164 - val_acc: 0.8243\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.5372 - acc: 0.9453 - val_loss: 0.9168 - val_acc: 0.8240\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.5332 - acc: 0.9465 - val_loss: 0.9128 - val_acc: 0.8230\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5342 - acc: 0.9482 - val_loss: 0.9134 - val_acc: 0.8210\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5369 - acc: 0.9444 - val_loss: 0.9102 - val_acc: 0.8253\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5350 - acc: 0.9488 - val_loss: 0.9143 - val_acc: 0.8243\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.5350 - acc: 0.9482 - val_loss: 0.9165 - val_acc: 0.8183\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5329 - acc: 0.9474 - val_loss: 0.9156 - val_acc: 0.8227\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5394 - acc: 0.9441 - val_loss: 0.9191 - val_acc: 0.8200\n",
      "Epoch 207/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.9451\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5369 - acc: 0.9450 - val_loss: 0.9250 - val_acc: 0.8187\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5346 - acc: 0.9460 - val_loss: 0.9164 - val_acc: 0.8243\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5269 - acc: 0.9511 - val_loss: 0.9162 - val_acc: 0.8253\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 410us/sample - loss: 0.5300 - acc: 0.9492 - val_loss: 0.9188 - val_acc: 0.8230\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.5328 - acc: 0.9470 - val_loss: 0.9189 - val_acc: 0.8230\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.5324 - acc: 0.9475 - val_loss: 0.9190 - val_acc: 0.8247\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5298 - acc: 0.9488 - val_loss: 0.9192 - val_acc: 0.8243\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5255 - acc: 0.9500 - val_loss: 0.9201 - val_acc: 0.8197\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5254 - acc: 0.9515 - val_loss: 0.9192 - val_acc: 0.8243\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5322 - acc: 0.9464 - val_loss: 0.9181 - val_acc: 0.8230\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.5311 - acc: 0.9492 - val_loss: 0.9146 - val_acc: 0.8263\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.5265 - acc: 0.9518 - val_loss: 0.9160 - val_acc: 0.8267\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.5336 - acc: 0.9476 - val_loss: 0.9164 - val_acc: 0.8247\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5283 - acc: 0.9497 - val_loss: 0.9211 - val_acc: 0.8203\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5257 - acc: 0.9523 - val_loss: 0.9151 - val_acc: 0.8237\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5261 - acc: 0.9517 - val_loss: 0.9183 - val_acc: 0.8223\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.5268 - acc: 0.9507 - val_loss: 0.9170 - val_acc: 0.8210\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.5281 - acc: 0.9492 - val_loss: 0.9169 - val_acc: 0.8217\n",
      "Epoch 225/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.9495\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5300 - acc: 0.9495 - val_loss: 0.9173 - val_acc: 0.8237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5271 - acc: 0.9522 - val_loss: 0.9173 - val_acc: 0.8240\n",
      "Epoch 227/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.5295 - acc: 0.9498 - val_loss: 0.9162 - val_acc: 0.8240\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.5311 - acc: 0.9502 - val_loss: 0.9163 - val_acc: 0.8227\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5209 - acc: 0.9527 - val_loss: 0.9170 - val_acc: 0.8237\n",
      "Epoch 230/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.5300 - acc: 0.9483 - val_loss: 0.9182 - val_acc: 0.8237\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5246 - acc: 0.9519 - val_loss: 0.9185 - val_acc: 0.8217\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5282 - acc: 0.9484 - val_loss: 0.9164 - val_acc: 0.8240\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.5264 - acc: 0.9515 - val_loss: 0.9172 - val_acc: 0.8223\n",
      "Epoch 234/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5268 - acc: 0.9520 - val_loss: 0.9173 - val_acc: 0.8237\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.5279 - acc: 0.9500 - val_loss: 0.9176 - val_acc: 0.8233\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5259 - acc: 0.9515 - val_loss: 0.9193 - val_acc: 0.8230\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 5s 375us/sample - loss: 0.5309 - acc: 0.9480 - val_loss: 0.9204 - val_acc: 0.8213\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5288 - acc: 0.9467 - val_loss: 0.9215 - val_acc: 0.8223\n",
      "Epoch 239/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.5308 - acc: 0.9476 - val_loss: 0.9219 - val_acc: 0.8227\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5224 - acc: 0.9522 - val_loss: 0.9196 - val_acc: 0.8223\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.5259 - acc: 0.9526 - val_loss: 0.9186 - val_acc: 0.8253\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.5253 - acc: 0.9512 - val_loss: 0.9190 - val_acc: 0.8247\n",
      "Epoch 243/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.9534\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5241 - acc: 0.9535 - val_loss: 0.9197 - val_acc: 0.8243\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.5227 - acc: 0.9536 - val_loss: 0.9191 - val_acc: 0.8237\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5256 - acc: 0.9531 - val_loss: 0.9185 - val_acc: 0.8230\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5284 - acc: 0.9491 - val_loss: 0.9186 - val_acc: 0.8217\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5280 - acc: 0.9490 - val_loss: 0.9187 - val_acc: 0.8240\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5289 - acc: 0.9498 - val_loss: 0.9185 - val_acc: 0.8230\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5237 - acc: 0.9528 - val_loss: 0.9188 - val_acc: 0.8230\n",
      "Epoch 00249: early stopping\n",
      "0.827\n",
      "0.84408\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_3 (TensorFlow [(None, 60, 3), (Non 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 60, 3)        0           tf_op_layer_split_3[0][2]        \n",
      "                                                                 tf_op_layer_split_3[0][3]        \n",
      "                                                                 tf_op_layer_split_3[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 60, 4)        0           tf_op_layer_split_3[0][0]        \n",
      "                                                                 tf_op_layer_split_3[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_3[0][1]        \n",
      "                                                                 tf_op_layer_split_3[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, 60, 32)       5376        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_3[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                  (None, 60, 16)       1280        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                  (None, 60, 32)       4736        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                  (None, 60, 32)       4736        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 60, 32)       64          lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, 60, 16)       32          lstm_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, 60, 16)       32          lstm_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, 60, 16)       32          lstm_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, 60, 32)       64          lstm_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_53 (LayerNo (None, 60, 32)       64          lstm_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 60, 32)       0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 60, 16)       0           layer_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 60, 16)       0           layer_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 60, 16)       0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 60, 32)       0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 60, 32)       0           layer_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, 60, 32)       8320        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 60, 16)       2112        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 60, 16)       2112        dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_49 (LSTM)                  (None, 60, 16)       2112        dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_52 (LSTM)                  (None, 60, 32)       8320        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                  (None, 60, 32)       8320        dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 60, 32)       64          lstm_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, 60, 16)       32          lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, 60, 16)       32          lstm_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, 60, 16)       32          lstm_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_51 (LayerNo (None, 60, 32)       64          lstm_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_54 (LayerNo (None, 60, 32)       64          lstm_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 60, 32)       0           layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 60, 16)       0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 60, 16)       0           layer_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 60, 16)       0           layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 60, 32)       0           layer_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 60, 32)       0           layer_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, 60, 32)       8320        dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 60, 16)       2112        dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                  (None, 60, 16)       2112        dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_50 (LSTM)                  (None, 60, 16)       2112        dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_53 (LSTM)                  (None, 60, 32)       8320        dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_56 (LSTM)                  (None, 60, 32)       8320        dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 60, 32)       0           lstm_41[0][0]                    \n",
      "                                                                 lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 60, 16)       0           lstm_44[0][0]                    \n",
      "                                                                 lstm_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 60, 16)       0           lstm_47[0][0]                    \n",
      "                                                                 lstm_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 60, 16)       0           lstm_50[0][0]                    \n",
      "                                                                 lstm_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 60, 32)       0           lstm_53[0][0]                    \n",
      "                                                                 lstm_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 60, 32)       0           lstm_56[0][0]                    \n",
      "                                                                 lstm_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, 60, 32)       64          attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, 60, 16)       32          attention_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, 60, 16)       32          attention_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, 60, 16)       32          attention_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_52 (LayerNo (None, 60, 32)       64          attention_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_55 (LayerNo (None, 60, 32)       64          attention_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 60, 32)       0           layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 60, 16)       0           layer_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 60, 16)       0           layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 60, 16)       0           layer_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 60, 32)       0           layer_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 60, 32)       0           layer_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 16)           0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 16)           0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 16)           0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 32)           0           dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 144)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 144)          576         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 144)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          37120       dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           16448       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 38s 3ms/sample - loss: 2.0945 - acc: 0.3934 - val_loss: 1.7578 - val_acc: 0.4523\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 1.6295 - acc: 0.5096 - val_loss: 1.4893 - val_acc: 0.5537\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 1.4943 - acc: 0.5592 - val_loss: 1.3687 - val_acc: 0.5990\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 1.4138 - acc: 0.5826 - val_loss: 1.2950 - val_acc: 0.6307\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 1.3433 - acc: 0.6118 - val_loss: 1.2254 - val_acc: 0.6540\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 1.2943 - acc: 0.6337 - val_loss: 1.1885 - val_acc: 0.6627\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 1.2598 - acc: 0.6438 - val_loss: 1.1672 - val_acc: 0.6867\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 1.2235 - acc: 0.6579 - val_loss: 1.1594 - val_acc: 0.6850\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.1921 - acc: 0.6762 - val_loss: 1.1299 - val_acc: 0.7027\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 1.1612 - acc: 0.6871 - val_loss: 1.0774 - val_acc: 0.7257\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 1.1324 - acc: 0.6993 - val_loss: 1.1062 - val_acc: 0.7137\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 1.1280 - acc: 0.7013 - val_loss: 1.0903 - val_acc: 0.7113\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 1.0986 - acc: 0.7068 - val_loss: 1.0835 - val_acc: 0.7203\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 1.0857 - acc: 0.7163 - val_loss: 1.0379 - val_acc: 0.7350\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 1.0552 - acc: 0.7259 - val_loss: 1.0213 - val_acc: 0.7507\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 1.0496 - acc: 0.7282 - val_loss: 1.0461 - val_acc: 0.7383\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 1.0270 - acc: 0.7418 - val_loss: 1.0316 - val_acc: 0.7437\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 1.0164 - acc: 0.7426 - val_loss: 0.9924 - val_acc: 0.7557\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 1.0028 - acc: 0.7481 - val_loss: 1.0100 - val_acc: 0.7523\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 1.0024 - acc: 0.7492 - val_loss: 0.9977 - val_acc: 0.7450\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.9918 - acc: 0.7523 - val_loss: 0.9905 - val_acc: 0.7533\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.9832 - acc: 0.7538 - val_loss: 0.9700 - val_acc: 0.7690\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.9672 - acc: 0.7632 - val_loss: 0.9842 - val_acc: 0.7683\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.9449 - acc: 0.7682 - val_loss: 0.9792 - val_acc: 0.7683\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.9467 - acc: 0.7707 - val_loss: 0.9581 - val_acc: 0.7760\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.9444 - acc: 0.7732 - val_loss: 0.9630 - val_acc: 0.7750\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.9301 - acc: 0.7820 - val_loss: 0.9745 - val_acc: 0.7720\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.9254 - acc: 0.7773 - val_loss: 0.9680 - val_acc: 0.7643\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.9159 - acc: 0.7803 - val_loss: 0.9661 - val_acc: 0.7673\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.9001 - acc: 0.7887 - val_loss: 0.9362 - val_acc: 0.7857\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.8902 - acc: 0.7970 - val_loss: 0.9329 - val_acc: 0.7833\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.8801 - acc: 0.7978 - val_loss: 0.9545 - val_acc: 0.7747\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.8765 - acc: 0.7968 - val_loss: 0.9465 - val_acc: 0.7717\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.8731 - acc: 0.8013 - val_loss: 0.9260 - val_acc: 0.7920\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.8773 - acc: 0.7996 - val_loss: 0.9399 - val_acc: 0.7777\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.8613 - acc: 0.8067 - val_loss: 0.9272 - val_acc: 0.7867\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.8496 - acc: 0.8115 - val_loss: 0.9332 - val_acc: 0.7833\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.8481 - acc: 0.8112 - val_loss: 0.9306 - val_acc: 0.7857\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 422us/sample - loss: 0.8447 - acc: 0.8138 - val_loss: 0.9293 - val_acc: 0.7823\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.8393 - acc: 0.8148 - val_loss: 0.9185 - val_acc: 0.7890\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.8299 - acc: 0.8171 - val_loss: 0.9185 - val_acc: 0.7917\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.8280 - acc: 0.8194 - val_loss: 0.9200 - val_acc: 0.7907\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.8242 - acc: 0.8190 - val_loss: 0.9241 - val_acc: 0.7933\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.8205 - acc: 0.8264 - val_loss: 0.9243 - val_acc: 0.7920\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.8130 - acc: 0.8257 - val_loss: 0.9283 - val_acc: 0.7920\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.7989 - acc: 0.8301 - val_loss: 0.9308 - val_acc: 0.7873\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.8058 - acc: 0.8281 - val_loss: 0.9049 - val_acc: 0.7930\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.7958 - acc: 0.8317 - val_loss: 0.8969 - val_acc: 0.8057\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.7870 - acc: 0.8365 - val_loss: 0.9127 - val_acc: 0.7947\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.7928 - acc: 0.8335 - val_loss: 0.9212 - val_acc: 0.8037\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.7825 - acc: 0.8390 - val_loss: 0.9180 - val_acc: 0.7963\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.7819 - acc: 0.8390 - val_loss: 0.8983 - val_acc: 0.8007\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.7718 - acc: 0.8441 - val_loss: 0.9224 - val_acc: 0.7973\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.7796 - acc: 0.8360 - val_loss: 0.9002 - val_acc: 0.8040\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.7645 - acc: 0.8482 - val_loss: 0.8986 - val_acc: 0.8123\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.7607 - acc: 0.8473 - val_loss: 0.9199 - val_acc: 0.7967\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.7612 - acc: 0.8445 - val_loss: 0.9053 - val_acc: 0.8043\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.7592 - acc: 0.8462 - val_loss: 0.9168 - val_acc: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.7509 - acc: 0.8537 - val_loss: 0.8961 - val_acc: 0.8133\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.7532 - acc: 0.8497 - val_loss: 0.9047 - val_acc: 0.8077\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.7537 - acc: 0.8490 - val_loss: 0.9030 - val_acc: 0.8120\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.7444 - acc: 0.8568 - val_loss: 0.9194 - val_acc: 0.8097\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 404us/sample - loss: 0.7417 - acc: 0.8558 - val_loss: 0.9219 - val_acc: 0.8067\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.7405 - acc: 0.8553 - val_loss: 0.9042 - val_acc: 0.8113\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.7386 - acc: 0.8554 - val_loss: 0.9169 - val_acc: 0.8023\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.7397 - acc: 0.8557 - val_loss: 0.9055 - val_acc: 0.8057\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.7299 - acc: 0.8581 - val_loss: 0.9008 - val_acc: 0.8093\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.7236 - acc: 0.8634 - val_loss: 0.9148 - val_acc: 0.8050\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.7299 - acc: 0.8571 - val_loss: 0.9032 - val_acc: 0.8060\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.7191 - acc: 0.8663 - val_loss: 0.9119 - val_acc: 0.8120\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.7097 - acc: 0.8683 - val_loss: 0.8994 - val_acc: 0.8170\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.7180 - acc: 0.8648 - val_loss: 0.8896 - val_acc: 0.8120\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.7088 - acc: 0.8696 - val_loss: 0.8900 - val_acc: 0.8123\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.7138 - acc: 0.8675 - val_loss: 0.9016 - val_acc: 0.8087\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.7138 - acc: 0.8666 - val_loss: 0.9084 - val_acc: 0.8113\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.6985 - acc: 0.8706 - val_loss: 0.9166 - val_acc: 0.8033\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.7006 - acc: 0.8700 - val_loss: 0.8874 - val_acc: 0.8200\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 458us/sample - loss: 0.7011 - acc: 0.8699 - val_loss: 0.9094 - val_acc: 0.8060\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.7038 - acc: 0.8717 - val_loss: 0.9047 - val_acc: 0.8067\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.6979 - acc: 0.8711 - val_loss: 0.8962 - val_acc: 0.8140\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.6849 - acc: 0.8829 - val_loss: 0.8991 - val_acc: 0.8133\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.6933 - acc: 0.8758 - val_loss: 0.9183 - val_acc: 0.8017\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 0.6916 - acc: 0.8737 - val_loss: 0.9064 - val_acc: 0.8077\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.6786 - acc: 0.8814 - val_loss: 0.9158 - val_acc: 0.8117\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.6799 - acc: 0.8818 - val_loss: 0.9269 - val_acc: 0.8057\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.6827 - acc: 0.8788 - val_loss: 0.9090 - val_acc: 0.8070\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.6719 - acc: 0.8862 - val_loss: 0.9121 - val_acc: 0.8093\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.6759 - acc: 0.8841 - val_loss: 0.9140 - val_acc: 0.8110\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.6727 - acc: 0.8892 - val_loss: 0.9134 - val_acc: 0.8057\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.6787 - acc: 0.8805 - val_loss: 0.9199 - val_acc: 0.8110\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.6687 - acc: 0.8843 - val_loss: 0.9084 - val_acc: 0.8070\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.6690 - acc: 0.8878 - val_loss: 0.9078 - val_acc: 0.8133\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.6714 - acc: 0.8842 - val_loss: 0.9112 - val_acc: 0.8107\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.6681 - acc: 0.8887 - val_loss: 0.9085 - val_acc: 0.8120\n",
      "Epoch 95/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.8873\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.6642 - acc: 0.8866 - val_loss: 0.9037 - val_acc: 0.8150\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.6415 - acc: 0.8999 - val_loss: 0.8919 - val_acc: 0.8193\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.6315 - acc: 0.9044 - val_loss: 0.8831 - val_acc: 0.8180\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.6320 - acc: 0.9021 - val_loss: 0.8859 - val_acc: 0.8200\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.6263 - acc: 0.9056 - val_loss: 0.8815 - val_acc: 0.8187\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.6168 - acc: 0.9105 - val_loss: 0.8904 - val_acc: 0.8190\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.6170 - acc: 0.9089 - val_loss: 0.8963 - val_acc: 0.8187\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.6231 - acc: 0.9058 - val_loss: 0.8838 - val_acc: 0.8267\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.6197 - acc: 0.9104 - val_loss: 0.9007 - val_acc: 0.8150\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.6178 - acc: 0.9128 - val_loss: 0.8989 - val_acc: 0.8217\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.6228 - acc: 0.9072 - val_loss: 0.8856 - val_acc: 0.8230\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.6105 - acc: 0.9104 - val_loss: 0.8966 - val_acc: 0.8203\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.6156 - acc: 0.9103 - val_loss: 0.8946 - val_acc: 0.8207\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.6136 - acc: 0.9101 - val_loss: 0.8888 - val_acc: 0.8247\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.6109 - acc: 0.9131 - val_loss: 0.8847 - val_acc: 0.8220\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.6086 - acc: 0.9151 - val_loss: 0.9122 - val_acc: 0.8203\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.6053 - acc: 0.9156 - val_loss: 0.8914 - val_acc: 0.8223\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.6040 - acc: 0.9151 - val_loss: 0.8984 - val_acc: 0.8207\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.6111 - acc: 0.9128 - val_loss: 0.8972 - val_acc: 0.8163\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.6092 - acc: 0.9155 - val_loss: 0.8857 - val_acc: 0.8180\n",
      "Epoch 115/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.6053 - acc: 0.9158 - val_loss: 0.8993 - val_acc: 0.8187\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.6081 - acc: 0.9123 - val_loss: 0.8944 - val_acc: 0.8193\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.6056 - acc: 0.9147 - val_loss: 0.8852 - val_acc: 0.8230\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.6034 - acc: 0.9159 - val_loss: 0.8899 - val_acc: 0.8217\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.6018 - acc: 0.9155 - val_loss: 0.8985 - val_acc: 0.8223\n",
      "Epoch 120/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.9132\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.6070 - acc: 0.9129 - val_loss: 0.8927 - val_acc: 0.8170\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.5933 - acc: 0.9216 - val_loss: 0.8873 - val_acc: 0.8283\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5884 - acc: 0.9232 - val_loss: 0.8929 - val_acc: 0.8240\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5850 - acc: 0.9205 - val_loss: 0.8880 - val_acc: 0.8230\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5872 - acc: 0.9227 - val_loss: 0.8899 - val_acc: 0.8250\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5850 - acc: 0.9219 - val_loss: 0.8867 - val_acc: 0.8263\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5825 - acc: 0.9256 - val_loss: 0.8884 - val_acc: 0.8213\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5858 - acc: 0.9217 - val_loss: 0.8898 - val_acc: 0.8233\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5794 - acc: 0.9254 - val_loss: 0.8861 - val_acc: 0.8263\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.5812 - acc: 0.9269 - val_loss: 0.8906 - val_acc: 0.8273\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5843 - acc: 0.9260 - val_loss: 0.8922 - val_acc: 0.8257\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.5783 - acc: 0.9284 - val_loss: 0.8904 - val_acc: 0.8233\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.5767 - acc: 0.9299 - val_loss: 0.8918 - val_acc: 0.8273\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.5792 - acc: 0.9262 - val_loss: 0.9000 - val_acc: 0.8183\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5781 - acc: 0.9271 - val_loss: 0.8956 - val_acc: 0.8207\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5741 - acc: 0.9286 - val_loss: 0.8999 - val_acc: 0.8203\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.5822 - acc: 0.9276 - val_loss: 0.9010 - val_acc: 0.8200\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5777 - acc: 0.9292 - val_loss: 0.8913 - val_acc: 0.8237\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.5779 - acc: 0.9265 - val_loss: 0.8915 - val_acc: 0.8253\n",
      "Epoch 139/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.9287\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.5737 - acc: 0.9286 - val_loss: 0.8918 - val_acc: 0.8260\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5711 - acc: 0.9321 - val_loss: 0.8904 - val_acc: 0.8243\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.5693 - acc: 0.9310 - val_loss: 0.8834 - val_acc: 0.8267\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.5671 - acc: 0.9323 - val_loss: 0.8856 - val_acc: 0.8277\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5683 - acc: 0.9314 - val_loss: 0.8882 - val_acc: 0.8280\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5707 - acc: 0.9305 - val_loss: 0.8837 - val_acc: 0.8297\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.5673 - acc: 0.9305 - val_loss: 0.8866 - val_acc: 0.8260\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5685 - acc: 0.9313 - val_loss: 0.8870 - val_acc: 0.8280\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.5682 - acc: 0.9320 - val_loss: 0.8836 - val_acc: 0.8300\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5655 - acc: 0.9320 - val_loss: 0.8868 - val_acc: 0.8277\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.5629 - acc: 0.9334 - val_loss: 0.8894 - val_acc: 0.8250\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5663 - acc: 0.9328 - val_loss: 0.8898 - val_acc: 0.8253\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.5628 - acc: 0.9366 - val_loss: 0.8837 - val_acc: 0.8253\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.5670 - acc: 0.9316 - val_loss: 0.8880 - val_acc: 0.8260\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5653 - acc: 0.9342 - val_loss: 0.8856 - val_acc: 0.8253\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5651 - acc: 0.9351 - val_loss: 0.8917 - val_acc: 0.8217\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5631 - acc: 0.9331 - val_loss: 0.8833 - val_acc: 0.8240\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5662 - acc: 0.9295 - val_loss: 0.8912 - val_acc: 0.8243\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.5600 - acc: 0.9344 - val_loss: 0.8916 - val_acc: 0.8253\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.5638 - acc: 0.9343 - val_loss: 0.8920 - val_acc: 0.8243\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5586 - acc: 0.9358 - val_loss: 0.8954 - val_acc: 0.8227\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5675 - acc: 0.9309 - val_loss: 0.8877 - val_acc: 0.8247\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.5640 - acc: 0.9327 - val_loss: 0.8897 - val_acc: 0.8267\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5641 - acc: 0.9313 - val_loss: 0.8816 - val_acc: 0.8270\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5628 - acc: 0.9352 - val_loss: 0.8862 - val_acc: 0.8283\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.5623 - acc: 0.9331 - val_loss: 0.8880 - val_acc: 0.8243\n",
      "Epoch 165/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.9326\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5622 - acc: 0.9328 - val_loss: 0.8851 - val_acc: 0.8277\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.5614 - acc: 0.9348 - val_loss: 0.8862 - val_acc: 0.8257\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5568 - acc: 0.9383 - val_loss: 0.8888 - val_acc: 0.8250\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 5s 426us/sample - loss: 0.5594 - acc: 0.9365 - val_loss: 0.8889 - val_acc: 0.8247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.5599 - acc: 0.9352 - val_loss: 0.8871 - val_acc: 0.8277\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5628 - acc: 0.9326 - val_loss: 0.8867 - val_acc: 0.8253\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5587 - acc: 0.9364 - val_loss: 0.8866 - val_acc: 0.8243\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5586 - acc: 0.9361 - val_loss: 0.8883 - val_acc: 0.8257\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5573 - acc: 0.9383 - val_loss: 0.8886 - val_acc: 0.8263\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5545 - acc: 0.9395 - val_loss: 0.8886 - val_acc: 0.8270\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 408us/sample - loss: 0.5537 - acc: 0.9380 - val_loss: 0.8919 - val_acc: 0.8263\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.5547 - acc: 0.9376 - val_loss: 0.8900 - val_acc: 0.8300\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5570 - acc: 0.9375 - val_loss: 0.8871 - val_acc: 0.8280\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5584 - acc: 0.9353 - val_loss: 0.8871 - val_acc: 0.8270\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.5551 - acc: 0.9401 - val_loss: 0.8862 - val_acc: 0.8290\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.5550 - acc: 0.9388 - val_loss: 0.8867 - val_acc: 0.8283\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5565 - acc: 0.9348 - val_loss: 0.8866 - val_acc: 0.8287\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5547 - acc: 0.9372 - val_loss: 0.8864 - val_acc: 0.8273\n",
      "Epoch 183/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.9395\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5533 - acc: 0.9394 - val_loss: 0.8871 - val_acc: 0.8250\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.5547 - acc: 0.9388 - val_loss: 0.8889 - val_acc: 0.8257\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.5556 - acc: 0.9363 - val_loss: 0.8873 - val_acc: 0.8270\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 0.5551 - acc: 0.9359 - val_loss: 0.8878 - val_acc: 0.8263\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.5584 - acc: 0.9356 - val_loss: 0.8880 - val_acc: 0.8263\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.5534 - acc: 0.9383 - val_loss: 0.8871 - val_acc: 0.8273\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.5515 - acc: 0.9385 - val_loss: 0.8883 - val_acc: 0.8283\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5544 - acc: 0.9391 - val_loss: 0.8891 - val_acc: 0.8253\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5533 - acc: 0.9380 - val_loss: 0.8891 - val_acc: 0.8263\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.5540 - acc: 0.9393 - val_loss: 0.8874 - val_acc: 0.8270\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.5537 - acc: 0.9368 - val_loss: 0.8902 - val_acc: 0.8260\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 0.5514 - acc: 0.9369 - val_loss: 0.8905 - val_acc: 0.8260\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.5511 - acc: 0.9385 - val_loss: 0.8900 - val_acc: 0.8267\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5487 - acc: 0.9415 - val_loss: 0.8891 - val_acc: 0.8287\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.5522 - acc: 0.9414 - val_loss: 0.8878 - val_acc: 0.8293\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5533 - acc: 0.9375 - val_loss: 0.8878 - val_acc: 0.8263\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.5570 - acc: 0.9358 - val_loss: 0.8881 - val_acc: 0.8280\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.5477 - acc: 0.9436 - val_loss: 0.8889 - val_acc: 0.8257\n",
      "Epoch 201/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.9385\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.5542 - acc: 0.9388 - val_loss: 0.8881 - val_acc: 0.8267\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.5535 - acc: 0.9378 - val_loss: 0.8875 - val_acc: 0.8263\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5520 - acc: 0.9385 - val_loss: 0.8869 - val_acc: 0.8270\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5514 - acc: 0.9390 - val_loss: 0.8875 - val_acc: 0.8277\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5549 - acc: 0.9377 - val_loss: 0.8884 - val_acc: 0.8277\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.5504 - acc: 0.9413 - val_loss: 0.8878 - val_acc: 0.8300\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.5490 - acc: 0.9413 - val_loss: 0.8879 - val_acc: 0.8277\n",
      "Epoch 00207: early stopping\n",
      "0.83\n",
      "0.84806\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_4 (TensorFlow [(None, 60, 3), (Non 0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_4[0][2]        \n",
      "                                                                 tf_op_layer_split_4[0][3]        \n",
      "                                                                 tf_op_layer_split_4[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_4[0][0]        \n",
      "                                                                 tf_op_layer_split_4[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_4[0][1]        \n",
      "                                                                 tf_op_layer_split_4[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_57 (LSTM)                  (None, 60, 32)       5376        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_60 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_63 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_4[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_66 (LSTM)                  (None, 60, 16)       1280        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_69 (LSTM)                  (None, 60, 32)       4736        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_72 (LSTM)                  (None, 60, 32)       4736        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (None, 60, 32)       64          lstm_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (None, 60, 16)       32          lstm_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, 60, 16)       32          lstm_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_65 (LayerNo (None, 60, 16)       32          lstm_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_68 (LayerNo (None, 60, 32)       64          lstm_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, 60, 32)       64          lstm_72[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 60, 32)       0           layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 60, 16)       0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 60, 16)       0           layer_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 60, 16)       0           layer_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 60, 32)       0           layer_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 60, 32)       0           layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_58 (LSTM)                  (None, 60, 32)       8320        dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  (None, 60, 16)       2112        dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_64 (LSTM)                  (None, 60, 16)       2112        dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_67 (LSTM)                  (None, 60, 16)       2112        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_70 (LSTM)                  (None, 60, 32)       8320        dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_73 (LSTM)                  (None, 60, 32)       8320        dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (None, 60, 32)       64          lstm_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_60 (LayerNo (None, 60, 16)       32          lstm_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_63 (LayerNo (None, 60, 16)       32          lstm_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_66 (LayerNo (None, 60, 16)       32          lstm_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_69 (LayerNo (None, 60, 32)       64          lstm_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_72 (LayerNo (None, 60, 32)       64          lstm_73[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 60, 32)       0           layer_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 60, 16)       0           layer_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 60, 16)       0           layer_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 60, 16)       0           layer_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 60, 32)       0           layer_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 60, 32)       0           layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_59 (LSTM)                  (None, 60, 32)       8320        dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_62 (LSTM)                  (None, 60, 16)       2112        dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_65 (LSTM)                  (None, 60, 16)       2112        dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_68 (LSTM)                  (None, 60, 16)       2112        dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (None, 60, 32)       8320        dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_74 (LSTM)                  (None, 60, 32)       8320        dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 60, 32)       0           lstm_59[0][0]                    \n",
      "                                                                 lstm_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 60, 16)       0           lstm_62[0][0]                    \n",
      "                                                                 lstm_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 60, 16)       0           lstm_65[0][0]                    \n",
      "                                                                 lstm_65[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 60, 16)       0           lstm_68[0][0]                    \n",
      "                                                                 lstm_68[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 60, 32)       0           lstm_71[0][0]                    \n",
      "                                                                 lstm_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 60, 32)       0           lstm_74[0][0]                    \n",
      "                                                                 lstm_74[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (None, 60, 32)       64          attention_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_61 (LayerNo (None, 60, 16)       32          attention_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_64 (LayerNo (None, 60, 16)       32          attention_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_67 (LayerNo (None, 60, 16)       32          attention_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_70 (LayerNo (None, 60, 32)       64          attention_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, 60, 32)       64          attention_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 60, 32)       0           layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 60, 16)       0           layer_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 60, 16)       0           layer_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 60, 16)       0           layer_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 60, 32)       0           layer_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 60, 32)       0           layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 16)           0           dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 16)           0           dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 16)           0           dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 144)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 144)          576         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 144)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          37120       dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256)          1024        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           16448       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 38s 3ms/sample - loss: 2.1089 - acc: 0.3907 - val_loss: 1.7109 - val_acc: 0.5000\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 1.6172 - acc: 0.5117 - val_loss: 1.5000 - val_acc: 0.5573\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 1.4787 - acc: 0.5595 - val_loss: 1.3740 - val_acc: 0.6070\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 1.4026 - acc: 0.5853 - val_loss: 1.2913 - val_acc: 0.6403\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 1.3254 - acc: 0.6172 - val_loss: 1.2703 - val_acc: 0.6343\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 1.2806 - acc: 0.6342 - val_loss: 1.2487 - val_acc: 0.6423\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 1.2353 - acc: 0.6557 - val_loss: 1.1848 - val_acc: 0.6797\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 1.2014 - acc: 0.6701 - val_loss: 1.1650 - val_acc: 0.6790\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 1.1826 - acc: 0.6768 - val_loss: 1.1628 - val_acc: 0.6950\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 1.1404 - acc: 0.6952 - val_loss: 1.1295 - val_acc: 0.7067\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 445us/sample - loss: 1.1176 - acc: 0.7050 - val_loss: 1.1360 - val_acc: 0.7050\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 1.0990 - acc: 0.7100 - val_loss: 1.1078 - val_acc: 0.7173\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 1.0847 - acc: 0.7155 - val_loss: 1.1110 - val_acc: 0.7150\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 1.0669 - acc: 0.7224 - val_loss: 1.0875 - val_acc: 0.7240\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 1.0517 - acc: 0.7286 - val_loss: 1.0628 - val_acc: 0.7353\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 1.0398 - acc: 0.7339 - val_loss: 1.0383 - val_acc: 0.7493\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 428us/sample - loss: 1.0163 - acc: 0.7453 - val_loss: 1.0463 - val_acc: 0.7367\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 1.0052 - acc: 0.7454 - val_loss: 1.0657 - val_acc: 0.7433\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.9981 - acc: 0.7501 - val_loss: 1.0512 - val_acc: 0.7353\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.9885 - acc: 0.7503 - val_loss: 1.0232 - val_acc: 0.7453\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.9722 - acc: 0.7599 - val_loss: 1.0142 - val_acc: 0.7540\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.9547 - acc: 0.7696 - val_loss: 1.0093 - val_acc: 0.7553\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.9547 - acc: 0.7688 - val_loss: 1.0157 - val_acc: 0.7513\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.9493 - acc: 0.7697 - val_loss: 1.0081 - val_acc: 0.7527\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 440us/sample - loss: 0.9338 - acc: 0.7760 - val_loss: 1.0061 - val_acc: 0.7553\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.9289 - acc: 0.7715 - val_loss: 1.0213 - val_acc: 0.7493\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.9200 - acc: 0.7777 - val_loss: 0.9994 - val_acc: 0.7637\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.9072 - acc: 0.7889 - val_loss: 1.0046 - val_acc: 0.7633\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.9037 - acc: 0.7859 - val_loss: 0.9990 - val_acc: 0.7620\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.8988 - acc: 0.7933 - val_loss: 0.9986 - val_acc: 0.7653\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.8858 - acc: 0.7965 - val_loss: 0.9851 - val_acc: 0.7623\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 4s 360us/sample - loss: 0.8752 - acc: 0.8014 - val_loss: 0.9972 - val_acc: 0.7707\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.8742 - acc: 0.8005 - val_loss: 0.9794 - val_acc: 0.7677\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 4s 342us/sample - loss: 0.8721 - acc: 0.8004 - val_loss: 0.9719 - val_acc: 0.7670\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 4s 362us/sample - loss: 0.8509 - acc: 0.8091 - val_loss: 0.9649 - val_acc: 0.7720\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.8486 - acc: 0.8123 - val_loss: 0.9694 - val_acc: 0.7790\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.8427 - acc: 0.8127 - val_loss: 0.9841 - val_acc: 0.7740\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.8384 - acc: 0.8167 - val_loss: 0.9931 - val_acc: 0.7640\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.8475 - acc: 0.8090 - val_loss: 0.9819 - val_acc: 0.7743\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.8349 - acc: 0.8163 - val_loss: 0.9860 - val_acc: 0.7760\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.8238 - acc: 0.8232 - val_loss: 0.9831 - val_acc: 0.7730\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.8324 - acc: 0.8205 - val_loss: 0.9670 - val_acc: 0.7743\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 4s 362us/sample - loss: 0.8072 - acc: 0.8263 - val_loss: 0.9942 - val_acc: 0.7810\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.8214 - acc: 0.8195 - val_loss: 0.9868 - val_acc: 0.7727\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.8066 - acc: 0.8232 - val_loss: 0.9769 - val_acc: 0.7833\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.8038 - acc: 0.8299 - val_loss: 0.9481 - val_acc: 0.7867\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.8037 - acc: 0.8232 - val_loss: 0.9525 - val_acc: 0.7873\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.7861 - acc: 0.8366 - val_loss: 0.9515 - val_acc: 0.7830\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.7867 - acc: 0.8361 - val_loss: 0.9639 - val_acc: 0.7857\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.7743 - acc: 0.8439 - val_loss: 0.9644 - val_acc: 0.7833\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.7825 - acc: 0.8365 - val_loss: 0.9588 - val_acc: 0.7867\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 4s 363us/sample - loss: 0.7701 - acc: 0.8431 - val_loss: 0.9666 - val_acc: 0.7893\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 4s 356us/sample - loss: 0.7644 - acc: 0.8470 - val_loss: 0.9631 - val_acc: 0.7877\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.7642 - acc: 0.8434 - val_loss: 0.9674 - val_acc: 0.7750\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.7669 - acc: 0.8461 - val_loss: 0.9507 - val_acc: 0.7897\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.7648 - acc: 0.8466 - val_loss: 0.9420 - val_acc: 0.7900\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 4s 356us/sample - loss: 0.7548 - acc: 0.8486 - val_loss: 0.9807 - val_acc: 0.7820\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.7456 - acc: 0.8513 - val_loss: 0.9515 - val_acc: 0.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.7428 - acc: 0.8538 - val_loss: 0.9496 - val_acc: 0.7903\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.7418 - acc: 0.8568 - val_loss: 0.9637 - val_acc: 0.7843\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.7406 - acc: 0.8581 - val_loss: 0.9592 - val_acc: 0.7873\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 4s 360us/sample - loss: 0.7296 - acc: 0.8618 - val_loss: 0.9502 - val_acc: 0.7973\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.7338 - acc: 0.8586 - val_loss: 0.9457 - val_acc: 0.7877\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.7368 - acc: 0.8582 - val_loss: 0.9743 - val_acc: 0.7820\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.7304 - acc: 0.8574 - val_loss: 0.9444 - val_acc: 0.7927\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.7212 - acc: 0.8643 - val_loss: 0.9593 - val_acc: 0.7913\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.7211 - acc: 0.8618 - val_loss: 0.9489 - val_acc: 0.7977\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.7220 - acc: 0.8627 - val_loss: 0.9453 - val_acc: 0.7933\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.7188 - acc: 0.8652 - val_loss: 0.9470 - val_acc: 0.7913\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.7083 - acc: 0.8730 - val_loss: 0.9486 - val_acc: 0.7943\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.7084 - acc: 0.8714 - val_loss: 0.9411 - val_acc: 0.8003\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.7085 - acc: 0.8681 - val_loss: 0.9598 - val_acc: 0.7923\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.7079 - acc: 0.8698 - val_loss: 0.9501 - val_acc: 0.7917\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.7011 - acc: 0.8720 - val_loss: 0.9465 - val_acc: 0.7937\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6959 - acc: 0.8737 - val_loss: 0.9657 - val_acc: 0.7887\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.6939 - acc: 0.8786 - val_loss: 0.9293 - val_acc: 0.7933\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 4s 344us/sample - loss: 0.6904 - acc: 0.8769 - val_loss: 0.9426 - val_acc: 0.7880\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.6906 - acc: 0.8802 - val_loss: 0.9375 - val_acc: 0.8010\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.6861 - acc: 0.8804 - val_loss: 0.9483 - val_acc: 0.7953\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6833 - acc: 0.8829 - val_loss: 0.9533 - val_acc: 0.7990\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 4s 344us/sample - loss: 0.6863 - acc: 0.8797 - val_loss: 0.9445 - val_acc: 0.8000\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.6826 - acc: 0.8825 - val_loss: 0.9555 - val_acc: 0.7953\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6755 - acc: 0.8869 - val_loss: 0.9563 - val_acc: 0.7927\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6796 - acc: 0.8812 - val_loss: 0.9373 - val_acc: 0.7983\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.6736 - acc: 0.8869 - val_loss: 0.9625 - val_acc: 0.7910\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 4s 342us/sample - loss: 0.6732 - acc: 0.8884 - val_loss: 0.9645 - val_acc: 0.7957\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.6741 - acc: 0.8856 - val_loss: 0.9444 - val_acc: 0.8000\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6749 - acc: 0.8817 - val_loss: 0.9485 - val_acc: 0.7983\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.6673 - acc: 0.8891 - val_loss: 0.9432 - val_acc: 0.7913\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6677 - acc: 0.8867 - val_loss: 0.9579 - val_acc: 0.7943\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 4s 362us/sample - loss: 0.6612 - acc: 0.8913 - val_loss: 0.9254 - val_acc: 0.8033\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6638 - acc: 0.8910 - val_loss: 0.9345 - val_acc: 0.8013\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 4s 353us/sample - loss: 0.6646 - acc: 0.8900 - val_loss: 0.9590 - val_acc: 0.7927\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 4s 341us/sample - loss: 0.6574 - acc: 0.8917 - val_loss: 0.9594 - val_acc: 0.7973\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6509 - acc: 0.8934 - val_loss: 0.9399 - val_acc: 0.8020\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.6506 - acc: 0.8949 - val_loss: 0.9322 - val_acc: 0.7993\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6537 - acc: 0.8939 - val_loss: 0.9595 - val_acc: 0.7960\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6579 - acc: 0.8941 - val_loss: 0.9568 - val_acc: 0.7947\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6511 - acc: 0.8963 - val_loss: 0.9494 - val_acc: 0.7983\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.6452 - acc: 0.8975 - val_loss: 0.9418 - val_acc: 0.8020\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6386 - acc: 0.9017 - val_loss: 0.9547 - val_acc: 0.7890\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.6538 - acc: 0.8927 - val_loss: 0.9506 - val_acc: 0.7950\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6437 - acc: 0.8992 - val_loss: 0.9585 - val_acc: 0.7973\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.6387 - acc: 0.9015 - val_loss: 0.9419 - val_acc: 0.8037\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6368 - acc: 0.9007 - val_loss: 0.9604 - val_acc: 0.7943\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.6366 - acc: 0.8997 - val_loss: 0.9355 - val_acc: 0.8037\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.6308 - acc: 0.9040 - val_loss: 0.9659 - val_acc: 0.7920\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.6417 - acc: 0.8991 - val_loss: 0.9967 - val_acc: 0.7827\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.6358 - acc: 0.9015 - val_loss: 0.9681 - val_acc: 0.7977\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6315 - acc: 0.9017 - val_loss: 0.9706 - val_acc: 0.7923\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6389 - acc: 0.9010 - val_loss: 0.9593 - val_acc: 0.7953\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.6247 - acc: 0.9074 - val_loss: 0.9388 - val_acc: 0.8017\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.6224 - acc: 0.9065 - val_loss: 0.9494 - val_acc: 0.7950\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6294 - acc: 0.9071 - val_loss: 0.9498 - val_acc: 0.7987\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6270 - acc: 0.9065 - val_loss: 0.9587 - val_acc: 0.8023\n",
      "Epoch 116/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.6174 - acc: 0.9120 - val_loss: 0.9446 - val_acc: 0.8043\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 4s 355us/sample - loss: 0.6260 - acc: 0.9059 - val_loss: 0.9716 - val_acc: 0.7953\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.6295 - acc: 0.9049 - val_loss: 0.9557 - val_acc: 0.7973\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6138 - acc: 0.9108 - val_loss: 0.9716 - val_acc: 0.7953\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.6124 - acc: 0.9112 - val_loss: 0.9621 - val_acc: 0.8050\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6151 - acc: 0.9118 - val_loss: 0.9491 - val_acc: 0.8043\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6104 - acc: 0.9131 - val_loss: 0.9490 - val_acc: 0.7997\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.6112 - acc: 0.9133 - val_loss: 0.9472 - val_acc: 0.8033\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 4s 344us/sample - loss: 0.6095 - acc: 0.9125 - val_loss: 0.9711 - val_acc: 0.7977\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.6095 - acc: 0.9144 - val_loss: 0.9520 - val_acc: 0.8020\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.6102 - acc: 0.9123 - val_loss: 0.9833 - val_acc: 0.7957\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.6149 - acc: 0.9104 - val_loss: 0.9525 - val_acc: 0.7990\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 4s 353us/sample - loss: 0.6054 - acc: 0.9149 - val_loss: 0.9587 - val_acc: 0.7943\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.6048 - acc: 0.9148 - val_loss: 0.9547 - val_acc: 0.7957\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.6117 - acc: 0.9133 - val_loss: 0.9429 - val_acc: 0.8017\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 4s 342us/sample - loss: 0.6090 - acc: 0.9129 - val_loss: 0.9598 - val_acc: 0.8010\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 4s 338us/sample - loss: 0.6038 - acc: 0.9133 - val_loss: 0.9572 - val_acc: 0.8037\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 4s 341us/sample - loss: 0.5997 - acc: 0.9201 - val_loss: 0.9470 - val_acc: 0.7960\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 4s 363us/sample - loss: 0.5961 - acc: 0.9195 - val_loss: 0.9528 - val_acc: 0.8053\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.6007 - acc: 0.9162 - val_loss: 0.9554 - val_acc: 0.7957\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 4s 341us/sample - loss: 0.5972 - acc: 0.9202 - val_loss: 0.9393 - val_acc: 0.8030\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.5977 - acc: 0.9183 - val_loss: 0.9491 - val_acc: 0.8000\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 4s 364us/sample - loss: 0.5938 - acc: 0.9190 - val_loss: 0.9443 - val_acc: 0.8087\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.5951 - acc: 0.9182 - val_loss: 0.9554 - val_acc: 0.7957\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.5970 - acc: 0.9176 - val_loss: 0.9595 - val_acc: 0.7990\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5885 - acc: 0.9220 - val_loss: 0.9644 - val_acc: 0.7993\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.5924 - acc: 0.9167 - val_loss: 0.9473 - val_acc: 0.8033\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.5929 - acc: 0.9184 - val_loss: 0.9533 - val_acc: 0.8023\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.5918 - acc: 0.9202 - val_loss: 0.9677 - val_acc: 0.7980\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 4s 341us/sample - loss: 0.5905 - acc: 0.9197 - val_loss: 0.9431 - val_acc: 0.8083\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 4s 340us/sample - loss: 0.5875 - acc: 0.9213 - val_loss: 0.9466 - val_acc: 0.8037\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.5835 - acc: 0.9243 - val_loss: 0.9610 - val_acc: 0.8030\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5842 - acc: 0.9237 - val_loss: 0.9498 - val_acc: 0.8033\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.5913 - acc: 0.9198 - val_loss: 0.9476 - val_acc: 0.8027\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 4s 366us/sample - loss: 0.5816 - acc: 0.9243 - val_loss: 0.9745 - val_acc: 0.7983\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5872 - acc: 0.9226 - val_loss: 0.9604 - val_acc: 0.8010\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.5805 - acc: 0.9243 - val_loss: 0.9593 - val_acc: 0.8017\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.5832 - acc: 0.9242 - val_loss: 0.9613 - val_acc: 0.7960\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5813 - acc: 0.9264 - val_loss: 0.9673 - val_acc: 0.8027\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5793 - acc: 0.9293 - val_loss: 0.9596 - val_acc: 0.8063\n",
      "Epoch 156/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.9281\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5761 - acc: 0.9284 - val_loss: 0.9593 - val_acc: 0.8003\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 4s 368us/sample - loss: 0.5619 - acc: 0.9340 - val_loss: 0.9449 - val_acc: 0.8037\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5507 - acc: 0.9416 - val_loss: 0.9471 - val_acc: 0.8067\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5466 - acc: 0.9419 - val_loss: 0.9421 - val_acc: 0.8097\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.5467 - acc: 0.9421 - val_loss: 0.9576 - val_acc: 0.8037\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.5436 - acc: 0.9422 - val_loss: 0.9394 - val_acc: 0.8103\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5399 - acc: 0.9446 - val_loss: 0.9436 - val_acc: 0.8007\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5474 - acc: 0.9399 - val_loss: 0.9382 - val_acc: 0.8077\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5438 - acc: 0.9425 - val_loss: 0.9373 - val_acc: 0.8043\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5440 - acc: 0.9421 - val_loss: 0.9382 - val_acc: 0.8080\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.5421 - acc: 0.9441 - val_loss: 0.9308 - val_acc: 0.8100\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5413 - acc: 0.9451 - val_loss: 0.9403 - val_acc: 0.8093\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5418 - acc: 0.9427 - val_loss: 0.9363 - val_acc: 0.8043\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.5429 - acc: 0.9430 - val_loss: 0.9360 - val_acc: 0.8093\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5395 - acc: 0.9438 - val_loss: 0.9349 - val_acc: 0.8083\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5401 - acc: 0.9434 - val_loss: 0.9487 - val_acc: 0.8030\n",
      "Epoch 172/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 4s 353us/sample - loss: 0.5380 - acc: 0.9475 - val_loss: 0.9416 - val_acc: 0.8097\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 4s 356us/sample - loss: 0.5385 - acc: 0.9463 - val_loss: 0.9425 - val_acc: 0.8067\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 4s 362us/sample - loss: 0.5398 - acc: 0.9437 - val_loss: 0.9458 - val_acc: 0.8033\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5394 - acc: 0.9464 - val_loss: 0.9416 - val_acc: 0.8083\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5367 - acc: 0.9448 - val_loss: 0.9374 - val_acc: 0.8077\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5376 - acc: 0.9457 - val_loss: 0.9479 - val_acc: 0.8033\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5415 - acc: 0.9411 - val_loss: 0.9432 - val_acc: 0.8047\n",
      "Epoch 179/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.9417\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5399 - acc: 0.9417 - val_loss: 0.9426 - val_acc: 0.8090\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5263 - acc: 0.9516 - val_loss: 0.9261 - val_acc: 0.8093\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5239 - acc: 0.9514 - val_loss: 0.9399 - val_acc: 0.8100\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5224 - acc: 0.9513 - val_loss: 0.9373 - val_acc: 0.8073\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.5249 - acc: 0.9498 - val_loss: 0.9354 - val_acc: 0.8100\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5218 - acc: 0.9542 - val_loss: 0.9411 - val_acc: 0.8063\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.5239 - acc: 0.9506 - val_loss: 0.9370 - val_acc: 0.8097\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.5185 - acc: 0.9536 - val_loss: 0.9268 - val_acc: 0.8123\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5186 - acc: 0.9538 - val_loss: 0.9361 - val_acc: 0.8093\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5216 - acc: 0.9522 - val_loss: 0.9368 - val_acc: 0.8097\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5185 - acc: 0.9537 - val_loss: 0.9385 - val_acc: 0.8090\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5179 - acc: 0.9532 - val_loss: 0.9322 - val_acc: 0.8093\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.5160 - acc: 0.9566 - val_loss: 0.9321 - val_acc: 0.8100\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.5192 - acc: 0.9563 - val_loss: 0.9333 - val_acc: 0.8090\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.5199 - acc: 0.9515 - val_loss: 0.9390 - val_acc: 0.8137\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5147 - acc: 0.9548 - val_loss: 0.9316 - val_acc: 0.8073\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5171 - acc: 0.9552 - val_loss: 0.9318 - val_acc: 0.8103\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 4s 361us/sample - loss: 0.5138 - acc: 0.9548 - val_loss: 0.9403 - val_acc: 0.8107\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 4s 361us/sample - loss: 0.5236 - acc: 0.9519 - val_loss: 0.9460 - val_acc: 0.8080\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5184 - acc: 0.9527 - val_loss: 0.9374 - val_acc: 0.8093\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5167 - acc: 0.9559 - val_loss: 0.9314 - val_acc: 0.8103\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5202 - acc: 0.9538 - val_loss: 0.9385 - val_acc: 0.8093\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5156 - acc: 0.9553 - val_loss: 0.9385 - val_acc: 0.8080\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5163 - acc: 0.9556 - val_loss: 0.9422 - val_acc: 0.8103\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5177 - acc: 0.9532 - val_loss: 0.9460 - val_acc: 0.8080\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5164 - acc: 0.9553 - val_loss: 0.9414 - val_acc: 0.8110\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5156 - acc: 0.9561 - val_loss: 0.9492 - val_acc: 0.8087\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.5138 - acc: 0.9562 - val_loss: 0.9496 - val_acc: 0.8110\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.5168 - acc: 0.9553 - val_loss: 0.9381 - val_acc: 0.8140\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5124 - acc: 0.9567 - val_loss: 0.9453 - val_acc: 0.8130\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5141 - acc: 0.9555 - val_loss: 0.9336 - val_acc: 0.8090\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.5141 - acc: 0.9561 - val_loss: 0.9431 - val_acc: 0.8130\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5191 - acc: 0.9531 - val_loss: 0.9392 - val_acc: 0.8093\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5089 - acc: 0.9577 - val_loss: 0.9391 - val_acc: 0.8100\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5162 - acc: 0.9542 - val_loss: 0.9425 - val_acc: 0.8087\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5145 - acc: 0.9533 - val_loss: 0.9417 - val_acc: 0.8047\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.5151 - acc: 0.9553 - val_loss: 0.9446 - val_acc: 0.8067\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5170 - acc: 0.9534 - val_loss: 0.9476 - val_acc: 0.8117\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.5078 - acc: 0.9590 - val_loss: 0.9491 - val_acc: 0.8037\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.5110 - acc: 0.9580 - val_loss: 0.9418 - val_acc: 0.8070\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.5135 - acc: 0.9552 - val_loss: 0.9397 - val_acc: 0.8057\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5128 - acc: 0.9572 - val_loss: 0.9345 - val_acc: 0.8083\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.5110 - acc: 0.9577 - val_loss: 0.9396 - val_acc: 0.8087\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5148 - acc: 0.9551 - val_loss: 0.9426 - val_acc: 0.8080\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.5106 - acc: 0.9561 - val_loss: 0.9461 - val_acc: 0.8063\n",
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5140 - acc: 0.9538 - val_loss: 0.9465 - val_acc: 0.8090\n",
      "Epoch 225/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.9570\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.5094 - acc: 0.9572 - val_loss: 0.9444 - val_acc: 0.8093\n",
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5062 - acc: 0.9603 - val_loss: 0.9427 - val_acc: 0.8060\n",
      "Epoch 227/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5009 - acc: 0.9620 - val_loss: 0.9416 - val_acc: 0.8107\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.5060 - acc: 0.9599 - val_loss: 0.9372 - val_acc: 0.8103\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5055 - acc: 0.9589 - val_loss: 0.9366 - val_acc: 0.8087\n",
      "Epoch 230/400\n",
      "12000/12000 [==============================] - 4s 365us/sample - loss: 0.5035 - acc: 0.9596 - val_loss: 0.9394 - val_acc: 0.8110\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.5054 - acc: 0.9590 - val_loss: 0.9372 - val_acc: 0.8107\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5013 - acc: 0.9617 - val_loss: 0.9370 - val_acc: 0.8100\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.5039 - acc: 0.9622 - val_loss: 0.9399 - val_acc: 0.8097\n",
      "Epoch 234/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5060 - acc: 0.9571 - val_loss: 0.9348 - val_acc: 0.8107\n",
      "Epoch 235/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5016 - acc: 0.9595 - val_loss: 0.9362 - val_acc: 0.8133\n",
      "Epoch 236/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5001 - acc: 0.9604 - val_loss: 0.9375 - val_acc: 0.8120\n",
      "Epoch 237/400\n",
      "12000/12000 [==============================] - 4s 364us/sample - loss: 0.5031 - acc: 0.9613 - val_loss: 0.9464 - val_acc: 0.8060\n",
      "Epoch 238/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.5004 - acc: 0.9626 - val_loss: 0.9418 - val_acc: 0.8083\n",
      "Epoch 239/400\n",
      "12000/12000 [==============================] - 5s 377us/sample - loss: 0.5030 - acc: 0.9609 - val_loss: 0.9410 - val_acc: 0.8103\n",
      "Epoch 240/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.5072 - acc: 0.9582 - val_loss: 0.9386 - val_acc: 0.8100\n",
      "Epoch 241/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.4981 - acc: 0.9632 - val_loss: 0.9402 - val_acc: 0.8077\n",
      "Epoch 242/400\n",
      "12000/12000 [==============================] - 5s 375us/sample - loss: 0.5069 - acc: 0.9582 - val_loss: 0.9464 - val_acc: 0.8080\n",
      "Epoch 243/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.9604\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5031 - acc: 0.9603 - val_loss: 0.9440 - val_acc: 0.8070\n",
      "Epoch 244/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.5026 - acc: 0.9613 - val_loss: 0.9392 - val_acc: 0.8093\n",
      "Epoch 245/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5008 - acc: 0.9638 - val_loss: 0.9360 - val_acc: 0.8097\n",
      "Epoch 246/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5017 - acc: 0.9614 - val_loss: 0.9408 - val_acc: 0.8043\n",
      "Epoch 247/400\n",
      "12000/12000 [==============================] - 4s 361us/sample - loss: 0.5033 - acc: 0.9601 - val_loss: 0.9397 - val_acc: 0.8053\n",
      "Epoch 248/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.4957 - acc: 0.9642 - val_loss: 0.9422 - val_acc: 0.8057\n",
      "Epoch 249/400\n",
      "12000/12000 [==============================] - 4s 356us/sample - loss: 0.4971 - acc: 0.9640 - val_loss: 0.9440 - val_acc: 0.8043\n",
      "Epoch 250/400\n",
      "12000/12000 [==============================] - 4s 369us/sample - loss: 0.5006 - acc: 0.9603 - val_loss: 0.9369 - val_acc: 0.8093\n",
      "Epoch 251/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5008 - acc: 0.9636 - val_loss: 0.9383 - val_acc: 0.8073\n",
      "Epoch 252/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.4944 - acc: 0.9648 - val_loss: 0.9386 - val_acc: 0.8093\n",
      "Epoch 253/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.4972 - acc: 0.9635 - val_loss: 0.9400 - val_acc: 0.8090\n",
      "Epoch 254/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.4993 - acc: 0.9611 - val_loss: 0.9376 - val_acc: 0.8107\n",
      "Epoch 255/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.5004 - acc: 0.9626 - val_loss: 0.9372 - val_acc: 0.8097\n",
      "Epoch 256/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5008 - acc: 0.9600 - val_loss: 0.9390 - val_acc: 0.8080\n",
      "Epoch 257/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.4962 - acc: 0.9631 - val_loss: 0.9367 - val_acc: 0.8087\n",
      "Epoch 258/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.4965 - acc: 0.9649 - val_loss: 0.9405 - val_acc: 0.8080\n",
      "Epoch 259/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5006 - acc: 0.9626 - val_loss: 0.9406 - val_acc: 0.8063\n",
      "Epoch 260/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5035 - acc: 0.9583 - val_loss: 0.9377 - val_acc: 0.8063\n",
      "Epoch 261/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.9654\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.4966 - acc: 0.9651 - val_loss: 0.9395 - val_acc: 0.8063\n",
      "Epoch 262/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.4982 - acc: 0.9623 - val_loss: 0.9388 - val_acc: 0.8060\n",
      "Epoch 263/400\n",
      "12000/12000 [==============================] - 4s 375us/sample - loss: 0.4930 - acc: 0.9669 - val_loss: 0.9384 - val_acc: 0.8063\n",
      "Epoch 264/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.4985 - acc: 0.9612 - val_loss: 0.9388 - val_acc: 0.8093\n",
      "Epoch 265/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.4937 - acc: 0.9653 - val_loss: 0.9388 - val_acc: 0.8067\n",
      "Epoch 266/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.4954 - acc: 0.9651 - val_loss: 0.9391 - val_acc: 0.8070\n",
      "Epoch 267/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.4966 - acc: 0.9641 - val_loss: 0.9392 - val_acc: 0.8067\n",
      "Epoch 00267: early stopping\n",
      "0.814\n",
      "0.83452\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_5 (TensorFlow [(None, 60, 3), (Non 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_5[0][2]        \n",
      "                                                                 tf_op_layer_split_5[0][3]        \n",
      "                                                                 tf_op_layer_split_5[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_5[0][0]        \n",
      "                                                                 tf_op_layer_split_5[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_5[0][1]        \n",
      "                                                                 tf_op_layer_split_5[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_75 (LSTM)                  (None, 60, 32)       5376        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_78 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_5[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_84 (LSTM)                  (None, 60, 16)       1280        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_87 (LSTM)                  (None, 60, 32)       4736        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_90 (LSTM)                  (None, 60, 32)       4736        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, 60, 32)       64          lstm_75[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, 60, 16)       32          lstm_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, 60, 16)       32          lstm_81[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, 60, 16)       32          lstm_84[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 60, 32)       64          lstm_87[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 60, 32)       64          lstm_90[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 60, 32)       0           layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 60, 16)       0           layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 60, 16)       0           layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 60, 16)       0           layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 60, 32)       0           layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 60, 32)       0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_76 (LSTM)                  (None, 60, 32)       8320        dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_79 (LSTM)                  (None, 60, 16)       2112        dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                  (None, 60, 16)       2112        dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_85 (LSTM)                  (None, 60, 16)       2112        dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_88 (LSTM)                  (None, 60, 32)       8320        dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_91 (LSTM)                  (None, 60, 32)       8320        dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, 60, 32)       64          lstm_76[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, 60, 16)       32          lstm_79[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, 60, 16)       32          lstm_82[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, 60, 16)       32          lstm_85[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 60, 32)       64          lstm_88[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, 60, 32)       64          lstm_91[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 60, 32)       0           layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 60, 16)       0           layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 60, 16)       0           layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 60, 16)       0           layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 60, 32)       0           layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 60, 32)       0           layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_77 (LSTM)                  (None, 60, 32)       8320        dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_80 (LSTM)                  (None, 60, 16)       2112        dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_83 (LSTM)                  (None, 60, 16)       2112        dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_86 (LSTM)                  (None, 60, 16)       2112        dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_89 (LSTM)                  (None, 60, 32)       8320        dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_92 (LSTM)                  (None, 60, 32)       8320        dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 60, 32)       0           lstm_77[0][0]                    \n",
      "                                                                 lstm_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_25 (Attention)        (None, 60, 16)       0           lstm_80[0][0]                    \n",
      "                                                                 lstm_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_26 (Attention)        (None, 60, 16)       0           lstm_83[0][0]                    \n",
      "                                                                 lstm_83[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_27 (Attention)        (None, 60, 16)       0           lstm_86[0][0]                    \n",
      "                                                                 lstm_86[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_28 (Attention)        (None, 60, 32)       0           lstm_89[0][0]                    \n",
      "                                                                 lstm_89[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_29 (Attention)        (None, 60, 32)       0           lstm_92[0][0]                    \n",
      "                                                                 lstm_92[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, 60, 32)       64          attention_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, 60, 16)       32          attention_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, 60, 16)       32          attention_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 60, 16)       32          attention_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 60, 32)       64          attention_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_91 (LayerNo (None, 60, 32)       64          attention_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 60, 32)       0           layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 60, 16)       0           layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 60, 16)       0           layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 60, 16)       0           layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 60, 32)       0           layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 60, 32)       0           layer_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 16)           0           dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 16)           0           dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 16)           0           dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 32)           0           dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 32)           0           dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 144)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "                                                                 global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 144)          576         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 144)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          37120       dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           16448       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           4160        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 37s 3ms/sample - loss: 2.0901 - acc: 0.3977 - val_loss: 1.7114 - val_acc: 0.4870\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 1.6413 - acc: 0.5013 - val_loss: 1.5217 - val_acc: 0.5513\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 1.5109 - acc: 0.5482 - val_loss: 1.3748 - val_acc: 0.6057\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 1.4358 - acc: 0.5753 - val_loss: 1.3147 - val_acc: 0.6393\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.3603 - acc: 0.6044 - val_loss: 1.2467 - val_acc: 0.6540\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 1.3108 - acc: 0.6293 - val_loss: 1.2226 - val_acc: 0.6707\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 1.2623 - acc: 0.6488 - val_loss: 1.1764 - val_acc: 0.6777\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 1.2206 - acc: 0.6618 - val_loss: 1.1564 - val_acc: 0.6890\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 1.1929 - acc: 0.6738 - val_loss: 1.1301 - val_acc: 0.7093\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 1.1625 - acc: 0.6883 - val_loss: 1.1332 - val_acc: 0.7003\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 1.1439 - acc: 0.6908 - val_loss: 1.0943 - val_acc: 0.7130\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 1.1241 - acc: 0.6982 - val_loss: 1.1136 - val_acc: 0.7017\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 1.1074 - acc: 0.7052 - val_loss: 1.1044 - val_acc: 0.7123\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 1.0879 - acc: 0.7132 - val_loss: 1.0515 - val_acc: 0.7330\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 1.0731 - acc: 0.7212 - val_loss: 1.0482 - val_acc: 0.7333\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 5s 405us/sample - loss: 1.0600 - acc: 0.7237 - val_loss: 1.0470 - val_acc: 0.7337\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 1.0373 - acc: 0.7370 - val_loss: 1.0256 - val_acc: 0.7527\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 1.0290 - acc: 0.7390 - val_loss: 1.0237 - val_acc: 0.7407\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 1.0122 - acc: 0.7468 - val_loss: 1.0096 - val_acc: 0.7533\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 1.0017 - acc: 0.7493 - val_loss: 0.9862 - val_acc: 0.7593\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.9839 - acc: 0.7562 - val_loss: 0.9988 - val_acc: 0.7567\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.9774 - acc: 0.7608 - val_loss: 1.0023 - val_acc: 0.7583\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.9700 - acc: 0.7592 - val_loss: 1.0076 - val_acc: 0.7667\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 380us/sample - loss: 0.9648 - acc: 0.7617 - val_loss: 0.9893 - val_acc: 0.7620\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 5s 395us/sample - loss: 0.9471 - acc: 0.7688 - val_loss: 0.9755 - val_acc: 0.7707\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.9450 - acc: 0.7718 - val_loss: 0.9608 - val_acc: 0.7760\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.9348 - acc: 0.7744 - val_loss: 0.9851 - val_acc: 0.7610\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.9377 - acc: 0.7680 - val_loss: 0.9711 - val_acc: 0.7720\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.9160 - acc: 0.7802 - val_loss: 0.9815 - val_acc: 0.7660\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.9087 - acc: 0.7856 - val_loss: 0.9519 - val_acc: 0.7813\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.9056 - acc: 0.7892 - val_loss: 0.9599 - val_acc: 0.7797\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.8971 - acc: 0.7890 - val_loss: 0.9524 - val_acc: 0.7873\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.8844 - acc: 0.7926 - val_loss: 0.9454 - val_acc: 0.7813\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.8853 - acc: 0.7956 - val_loss: 0.9592 - val_acc: 0.7727\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.8798 - acc: 0.7949 - val_loss: 0.9455 - val_acc: 0.7817\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.8749 - acc: 0.8031 - val_loss: 0.9559 - val_acc: 0.7733\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.8627 - acc: 0.8031 - val_loss: 0.9284 - val_acc: 0.7897\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.8570 - acc: 0.8071 - val_loss: 0.9641 - val_acc: 0.7767\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.8459 - acc: 0.8134 - val_loss: 0.9306 - val_acc: 0.7937\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.8484 - acc: 0.8084 - val_loss: 0.9398 - val_acc: 0.7863\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.8525 - acc: 0.8077 - val_loss: 0.9354 - val_acc: 0.7897\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.8378 - acc: 0.8147 - val_loss: 0.9390 - val_acc: 0.7887\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 0.8275 - acc: 0.8205 - val_loss: 0.9292 - val_acc: 0.7917\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 6s 495us/sample - loss: 0.8216 - acc: 0.8197 - val_loss: 0.9237 - val_acc: 0.7950\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.8147 - acc: 0.8226 - val_loss: 0.9311 - val_acc: 0.7913\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.8141 - acc: 0.8246 - val_loss: 0.9415 - val_acc: 0.7933\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 0.8161 - acc: 0.8247 - val_loss: 0.9310 - val_acc: 0.7953\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 6s 502us/sample - loss: 0.8118 - acc: 0.8264 - val_loss: 0.9252 - val_acc: 0.7923\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 6s 510us/sample - loss: 0.8024 - acc: 0.8281 - val_loss: 0.9102 - val_acc: 0.7980\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.7967 - acc: 0.8320 - val_loss: 0.9385 - val_acc: 0.7880\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 5s 397us/sample - loss: 0.7963 - acc: 0.8309 - val_loss: 0.9073 - val_acc: 0.7980\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.7931 - acc: 0.8332 - val_loss: 0.9105 - val_acc: 0.7977\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 0.7830 - acc: 0.8364 - val_loss: 0.9225 - val_acc: 0.8053\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 6s 491us/sample - loss: 0.7818 - acc: 0.8340 - val_loss: 0.9374 - val_acc: 0.7927\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.7801 - acc: 0.8396 - val_loss: 0.9100 - val_acc: 0.8017\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.7717 - acc: 0.8436 - val_loss: 0.9163 - val_acc: 0.7933\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.7759 - acc: 0.8372 - val_loss: 0.9142 - val_acc: 0.8020\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.7763 - acc: 0.8391 - val_loss: 0.9144 - val_acc: 0.8017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 6s 489us/sample - loss: 0.7626 - acc: 0.8463 - val_loss: 0.9294 - val_acc: 0.7937\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.7662 - acc: 0.8443 - val_loss: 0.9074 - val_acc: 0.8007\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.7552 - acc: 0.8508 - val_loss: 0.9223 - val_acc: 0.8000\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 418us/sample - loss: 0.7522 - acc: 0.8520 - val_loss: 0.9148 - val_acc: 0.8000\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.7527 - acc: 0.8500 - val_loss: 0.9208 - val_acc: 0.8050\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 6s 528us/sample - loss: 0.7373 - acc: 0.8528 - val_loss: 0.9060 - val_acc: 0.8103\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.7419 - acc: 0.8514 - val_loss: 0.9117 - val_acc: 0.8037\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 0.7430 - acc: 0.8512 - val_loss: 0.8954 - val_acc: 0.8113\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.7417 - acc: 0.8533 - val_loss: 0.9248 - val_acc: 0.8003\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.7375 - acc: 0.8541 - val_loss: 0.9213 - val_acc: 0.8050\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.7322 - acc: 0.8580 - val_loss: 0.9043 - val_acc: 0.8043\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 6s 493us/sample - loss: 0.7286 - acc: 0.8584 - val_loss: 0.9140 - val_acc: 0.8013\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.7223 - acc: 0.8652 - val_loss: 0.9028 - val_acc: 0.8113\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 6s 504us/sample - loss: 0.7249 - acc: 0.8604 - val_loss: 0.8983 - val_acc: 0.8090\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.7195 - acc: 0.8642 - val_loss: 0.9204 - val_acc: 0.8047\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 5s 419us/sample - loss: 0.7121 - acc: 0.8682 - val_loss: 0.9085 - val_acc: 0.8087\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.7086 - acc: 0.8710 - val_loss: 0.9278 - val_acc: 0.7980\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 6s 500us/sample - loss: 0.7023 - acc: 0.8702 - val_loss: 0.9081 - val_acc: 0.8110\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 6s 504us/sample - loss: 0.7046 - acc: 0.8699 - val_loss: 0.9035 - val_acc: 0.8173\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 499us/sample - loss: 0.7051 - acc: 0.8687 - val_loss: 0.9109 - val_acc: 0.8107\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 438us/sample - loss: 0.6982 - acc: 0.8763 - val_loss: 0.8995 - val_acc: 0.8047\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.7056 - acc: 0.8679 - val_loss: 0.9115 - val_acc: 0.8053\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 6s 489us/sample - loss: 0.7031 - acc: 0.8682 - val_loss: 0.9009 - val_acc: 0.8117\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.6907 - acc: 0.8784 - val_loss: 0.9063 - val_acc: 0.8063\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 6s 518us/sample - loss: 0.6909 - acc: 0.8784 - val_loss: 0.9111 - val_acc: 0.8103\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.6845 - acc: 0.8794 - val_loss: 0.9143 - val_acc: 0.8080\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.6828 - acc: 0.8819 - val_loss: 0.9234 - val_acc: 0.8033\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.6847 - acc: 0.8805 - val_loss: 0.9191 - val_acc: 0.8110\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.6779 - acc: 0.8820 - val_loss: 0.8961 - val_acc: 0.8137\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6877 - acc: 0.8814 - val_loss: 0.9132 - val_acc: 0.8133\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.6783 - acc: 0.8832 - val_loss: 0.9067 - val_acc: 0.8140\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.6767 - acc: 0.8831 - val_loss: 0.9076 - val_acc: 0.8153\n",
      "Epoch 91/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.6734 - acc: 0.8872 - val_loss: 0.9041 - val_acc: 0.8180\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 429us/sample - loss: 0.6710 - acc: 0.8867 - val_loss: 0.9027 - val_acc: 0.8113\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.6763 - acc: 0.8833 - val_loss: 0.9045 - val_acc: 0.8167\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.6629 - acc: 0.8863 - val_loss: 0.9068 - val_acc: 0.8153\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 6s 481us/sample - loss: 0.6645 - acc: 0.8875 - val_loss: 0.9103 - val_acc: 0.8107\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.6694 - acc: 0.8841 - val_loss: 0.9028 - val_acc: 0.8137\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.6646 - acc: 0.8869 - val_loss: 0.9061 - val_acc: 0.8067\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.6623 - acc: 0.8891 - val_loss: 0.9142 - val_acc: 0.8120\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.6574 - acc: 0.8935 - val_loss: 0.9160 - val_acc: 0.8100\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 6s 501us/sample - loss: 0.6590 - acc: 0.8922 - val_loss: 0.9081 - val_acc: 0.8153\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.6568 - acc: 0.8923 - val_loss: 0.9010 - val_acc: 0.8137\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.6536 - acc: 0.8948 - val_loss: 0.9136 - val_acc: 0.8063\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 449us/sample - loss: 0.6586 - acc: 0.8879 - val_loss: 0.9127 - val_acc: 0.8060\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 5s 416us/sample - loss: 0.6478 - acc: 0.8963 - val_loss: 0.9102 - val_acc: 0.8080\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.6459 - acc: 0.8979 - val_loss: 0.9092 - val_acc: 0.8150\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.6448 - acc: 0.8961 - val_loss: 0.9190 - val_acc: 0.8063\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 6s 494us/sample - loss: 0.6431 - acc: 0.8991 - val_loss: 0.9079 - val_acc: 0.8150\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 6s 517us/sample - loss: 0.6439 - acc: 0.8976 - val_loss: 0.9000 - val_acc: 0.8210\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 454us/sample - loss: 0.6407 - acc: 0.9007 - val_loss: 0.9158 - val_acc: 0.8130\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 437us/sample - loss: 0.6354 - acc: 0.9003 - val_loss: 0.9242 - val_acc: 0.8143\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.6405 - acc: 0.9004 - val_loss: 0.8937 - val_acc: 0.8193\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 6s 500us/sample - loss: 0.6368 - acc: 0.9003 - val_loss: 0.9107 - val_acc: 0.8157\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.6456 - acc: 0.8961 - val_loss: 0.9030 - val_acc: 0.8163\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.6378 - acc: 0.9003 - val_loss: 0.9062 - val_acc: 0.8167\n",
      "Epoch 115/400\n",
      "12000/12000 [==============================] - 5s 417us/sample - loss: 0.6305 - acc: 0.9052 - val_loss: 0.9126 - val_acc: 0.8150\n",
      "Epoch 116/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.6305 - acc: 0.9050 - val_loss: 0.9041 - val_acc: 0.8187\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 6s 488us/sample - loss: 0.6252 - acc: 0.9059 - val_loss: 0.8992 - val_acc: 0.8190\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.6298 - acc: 0.9071 - val_loss: 0.9142 - val_acc: 0.8080\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 6s 516us/sample - loss: 0.6243 - acc: 0.9054 - val_loss: 0.8968 - val_acc: 0.8240\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.6309 - acc: 0.9028 - val_loss: 0.8950 - val_acc: 0.8183\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 414us/sample - loss: 0.6201 - acc: 0.9107 - val_loss: 0.9203 - val_acc: 0.8113\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.6276 - acc: 0.9039 - val_loss: 0.9131 - val_acc: 0.8120\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 6s 483us/sample - loss: 0.6267 - acc: 0.9082 - val_loss: 0.9148 - val_acc: 0.8113\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 0.6177 - acc: 0.9066 - val_loss: 0.8978 - val_acc: 0.8190\n",
      "Epoch 125/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.6240 - acc: 0.9067 - val_loss: 0.9131 - val_acc: 0.8143\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.6249 - acc: 0.9099 - val_loss: 0.9096 - val_acc: 0.8117\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.6236 - acc: 0.9067 - val_loss: 0.9045 - val_acc: 0.8197\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 5s 415us/sample - loss: 0.6181 - acc: 0.9082 - val_loss: 0.9162 - val_acc: 0.8113\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.6166 - acc: 0.9103 - val_loss: 0.9033 - val_acc: 0.8163\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.6130 - acc: 0.9112 - val_loss: 0.9184 - val_acc: 0.8140\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.6146 - acc: 0.9125 - val_loss: 0.9222 - val_acc: 0.8133\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.6206 - acc: 0.9062 - val_loss: 0.9172 - val_acc: 0.8183\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 0.6189 - acc: 0.9081 - val_loss: 0.9075 - val_acc: 0.8190\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 443us/sample - loss: 0.6076 - acc: 0.9116 - val_loss: 0.9132 - val_acc: 0.8090\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 6s 527us/sample - loss: 0.6072 - acc: 0.9133 - val_loss: 0.9168 - val_acc: 0.8213\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 0.6076 - acc: 0.9106 - val_loss: 0.9038 - val_acc: 0.8207\n",
      "Epoch 137/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.9119\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 6s 527us/sample - loss: 0.6096 - acc: 0.9123 - val_loss: 0.9220 - val_acc: 0.8067\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 6s 485us/sample - loss: 0.5863 - acc: 0.9218 - val_loss: 0.9054 - val_acc: 0.8170\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5780 - acc: 0.9274 - val_loss: 0.8976 - val_acc: 0.8193\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 6s 527us/sample - loss: 0.5768 - acc: 0.9290 - val_loss: 0.9040 - val_acc: 0.8173\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 6s 521us/sample - loss: 0.5714 - acc: 0.9334 - val_loss: 0.8990 - val_acc: 0.8173\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 6s 527us/sample - loss: 0.5683 - acc: 0.9312 - val_loss: 0.9041 - val_acc: 0.8227\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 6s 506us/sample - loss: 0.5676 - acc: 0.9315 - val_loss: 0.9042 - val_acc: 0.8177\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 6s 496us/sample - loss: 0.5688 - acc: 0.9317 - val_loss: 0.8944 - val_acc: 0.8247\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 6s 524us/sample - loss: 0.5704 - acc: 0.9318 - val_loss: 0.9105 - val_acc: 0.8227\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 6s 528us/sample - loss: 0.5707 - acc: 0.9309 - val_loss: 0.9003 - val_acc: 0.8193\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 6s 530us/sample - loss: 0.5699 - acc: 0.9317 - val_loss: 0.9132 - val_acc: 0.8193\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 6s 516us/sample - loss: 0.5692 - acc: 0.9306 - val_loss: 0.9080 - val_acc: 0.8167\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5708 - acc: 0.9296 - val_loss: 0.8969 - val_acc: 0.8183\n",
      "Epoch 150/400\n",
      "12000/12000 [==============================] - 6s 515us/sample - loss: 0.5691 - acc: 0.9308 - val_loss: 0.9058 - val_acc: 0.8203\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 6s 525us/sample - loss: 0.5675 - acc: 0.9326 - val_loss: 0.8996 - val_acc: 0.8193\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 6s 520us/sample - loss: 0.5715 - acc: 0.9307 - val_loss: 0.9073 - val_acc: 0.8193\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 6s 522us/sample - loss: 0.5633 - acc: 0.9337 - val_loss: 0.9124 - val_acc: 0.8187\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.5642 - acc: 0.9327 - val_loss: 0.9129 - val_acc: 0.8153\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 6s 503us/sample - loss: 0.5619 - acc: 0.9336 - val_loss: 0.9039 - val_acc: 0.8170\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 6s 510us/sample - loss: 0.5663 - acc: 0.9336 - val_loss: 0.9126 - val_acc: 0.8173\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 6s 506us/sample - loss: 0.5620 - acc: 0.9333 - val_loss: 0.9035 - val_acc: 0.8277\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5628 - acc: 0.9330 - val_loss: 0.9052 - val_acc: 0.8163\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 5s 402us/sample - loss: 0.5652 - acc: 0.9329 - val_loss: 0.9076 - val_acc: 0.8140\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5589 - acc: 0.9355 - val_loss: 0.9037 - val_acc: 0.8213\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5587 - acc: 0.9367 - val_loss: 0.9011 - val_acc: 0.8213\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.5611 - acc: 0.9333 - val_loss: 0.8963 - val_acc: 0.8227\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.5568 - acc: 0.9367 - val_loss: 0.8970 - val_acc: 0.8223\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5587 - acc: 0.9360 - val_loss: 0.9063 - val_acc: 0.8177\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 4s 340us/sample - loss: 0.5597 - acc: 0.9349 - val_loss: 0.9069 - val_acc: 0.8137\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.5635 - acc: 0.9321 - val_loss: 0.9086 - val_acc: 0.8173\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 4s 344us/sample - loss: 0.5610 - acc: 0.9334 - val_loss: 0.9098 - val_acc: 0.8197\n",
      "Epoch 168/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.5644 - acc: 0.9327 - val_loss: 0.9137 - val_acc: 0.8177\n",
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5546 - acc: 0.9388 - val_loss: 0.9023 - val_acc: 0.8247\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5504 - acc: 0.9425 - val_loss: 0.9122 - val_acc: 0.8230\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5594 - acc: 0.9353 - val_loss: 0.9112 - val_acc: 0.8173\n",
      "Epoch 172/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.5544 - acc: 0.9386 - val_loss: 0.9083 - val_acc: 0.8233\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5581 - acc: 0.9355 - val_loss: 0.9127 - val_acc: 0.8197\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 4s 353us/sample - loss: 0.5499 - acc: 0.9416 - val_loss: 0.9133 - val_acc: 0.8160\n",
      "Epoch 175/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.9377\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 4s 344us/sample - loss: 0.5533 - acc: 0.9374 - val_loss: 0.9188 - val_acc: 0.8173\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 4s 345us/sample - loss: 0.5450 - acc: 0.9423 - val_loss: 0.9017 - val_acc: 0.8190\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.5363 - acc: 0.9465 - val_loss: 0.9039 - val_acc: 0.8223\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 4s 352us/sample - loss: 0.5400 - acc: 0.9442 - val_loss: 0.9021 - val_acc: 0.8240\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.5347 - acc: 0.9484 - val_loss: 0.9032 - val_acc: 0.8257\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5365 - acc: 0.9449 - val_loss: 0.9064 - val_acc: 0.8203\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.5369 - acc: 0.9461 - val_loss: 0.9079 - val_acc: 0.8230\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5407 - acc: 0.9439 - val_loss: 0.9050 - val_acc: 0.8207\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.5426 - acc: 0.9430 - val_loss: 0.9028 - val_acc: 0.8213\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5360 - acc: 0.9435 - val_loss: 0.9063 - val_acc: 0.8227\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 4s 342us/sample - loss: 0.5340 - acc: 0.9463 - val_loss: 0.9025 - val_acc: 0.8217\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5341 - acc: 0.9465 - val_loss: 0.9043 - val_acc: 0.8243\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5380 - acc: 0.9450 - val_loss: 0.9111 - val_acc: 0.8190\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5343 - acc: 0.9441 - val_loss: 0.9021 - val_acc: 0.8223\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 4s 343us/sample - loss: 0.5368 - acc: 0.9436 - val_loss: 0.9036 - val_acc: 0.8207\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.5355 - acc: 0.9463 - val_loss: 0.9021 - val_acc: 0.8217\n",
      "Epoch 191/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5338 - acc: 0.9467 - val_loss: 0.9020 - val_acc: 0.8217\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.5367 - acc: 0.9446 - val_loss: 0.9025 - val_acc: 0.8217\n",
      "Epoch 193/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.9478\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5350 - acc: 0.9475 - val_loss: 0.9045 - val_acc: 0.8217\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 4s 347us/sample - loss: 0.5229 - acc: 0.9513 - val_loss: 0.8994 - val_acc: 0.8217\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 4s 346us/sample - loss: 0.5247 - acc: 0.9489 - val_loss: 0.9028 - val_acc: 0.8217\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 4s 340us/sample - loss: 0.5228 - acc: 0.9543 - val_loss: 0.9003 - val_acc: 0.8217\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 4s 357us/sample - loss: 0.5280 - acc: 0.9484 - val_loss: 0.8987 - val_acc: 0.8227\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.5251 - acc: 0.9506 - val_loss: 0.9048 - val_acc: 0.8233\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5244 - acc: 0.9509 - val_loss: 0.9037 - val_acc: 0.8220\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 4s 372us/sample - loss: 0.5249 - acc: 0.9523 - val_loss: 0.8998 - val_acc: 0.8237\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.5275 - acc: 0.9493 - val_loss: 0.9044 - val_acc: 0.8207\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.5243 - acc: 0.9560 - val_loss: 0.9035 - val_acc: 0.8193\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.5264 - acc: 0.9499 - val_loss: 0.8989 - val_acc: 0.8237\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 4s 348us/sample - loss: 0.5267 - acc: 0.9471 - val_loss: 0.9001 - val_acc: 0.8237\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 4s 351us/sample - loss: 0.5211 - acc: 0.9521 - val_loss: 0.8995 - val_acc: 0.8213\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.5232 - acc: 0.9521 - val_loss: 0.9050 - val_acc: 0.8227\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 4s 353us/sample - loss: 0.5272 - acc: 0.9487 - val_loss: 0.9040 - val_acc: 0.8213\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5222 - acc: 0.9528 - val_loss: 0.8995 - val_acc: 0.8237\n",
      "Epoch 209/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5223 - acc: 0.9515 - val_loss: 0.9018 - val_acc: 0.8257\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5234 - acc: 0.9513 - val_loss: 0.9057 - val_acc: 0.8250\n",
      "Epoch 211/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.9495\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5257 - acc: 0.9493 - val_loss: 0.9085 - val_acc: 0.8210\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 4s 358us/sample - loss: 0.5191 - acc: 0.9547 - val_loss: 0.9052 - val_acc: 0.8220\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5222 - acc: 0.9527 - val_loss: 0.9024 - val_acc: 0.8197\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5211 - acc: 0.9529 - val_loss: 0.9067 - val_acc: 0.8173\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 4s 359us/sample - loss: 0.5212 - acc: 0.9528 - val_loss: 0.9047 - val_acc: 0.8210\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.5215 - acc: 0.9532 - val_loss: 0.9036 - val_acc: 0.8203\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 4s 350us/sample - loss: 0.5214 - acc: 0.9529 - val_loss: 0.9032 - val_acc: 0.8223\n",
      "Epoch 00217: early stopping\n",
      "0.828\n",
      "0.8463\n",
      "(15000, 60, 9, 1)\n",
      "(None, 256)\n",
      "(None, 20)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 60, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_6 (TensorFlow [(None, 60, 3), (Non 0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 60, 3)        0           tf_op_layer_split_6[0][2]        \n",
      "                                                                 tf_op_layer_split_6[0][3]        \n",
      "                                                                 tf_op_layer_split_6[0][4]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_6[0][0]        \n",
      "                                                                 tf_op_layer_split_6[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 60, 4)        0           tf_op_layer_split_6[0][1]        \n",
      "                                                                 tf_op_layer_split_6[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_93 (LSTM)                  (None, 60, 32)       5376        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_96 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_99 (LSTM)                  (None, 60, 16)       1280        tf_op_layer_split_6[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_102 (LSTM)                 (None, 60, 16)       1280        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_105 (LSTM)                 (None, 60, 32)       4736        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_108 (LSTM)                 (None, 60, 32)       4736        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_92 (LayerNo (None, 60, 32)       64          lstm_93[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, 60, 16)       32          lstm_96[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_98 (LayerNo (None, 60, 16)       32          lstm_99[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_101 (LayerN (None, 60, 16)       32          lstm_102[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_104 (LayerN (None, 60, 32)       64          lstm_105[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_107 (LayerN (None, 60, 32)       64          lstm_108[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 60, 32)       0           layer_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 60, 16)       0           layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 60, 16)       0           layer_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 60, 16)       0           layer_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 60, 32)       0           layer_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 60, 32)       0           layer_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_94 (LSTM)                  (None, 60, 32)       8320        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_97 (LSTM)                  (None, 60, 16)       2112        dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_100 (LSTM)                 (None, 60, 16)       2112        dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_103 (LSTM)                 (None, 60, 16)       2112        dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_106 (LSTM)                 (None, 60, 32)       8320        dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_109 (LSTM)                 (None, 60, 32)       8320        dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_93 (LayerNo (None, 60, 32)       64          lstm_94[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_96 (LayerNo (None, 60, 16)       32          lstm_97[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_99 (LayerNo (None, 60, 16)       32          lstm_100[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_102 (LayerN (None, 60, 16)       32          lstm_103[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_105 (LayerN (None, 60, 32)       64          lstm_106[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_108 (LayerN (None, 60, 32)       64          lstm_109[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 60, 32)       0           layer_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 60, 16)       0           layer_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 60, 16)       0           layer_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 60, 16)       0           layer_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 60, 32)       0           layer_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 60, 32)       0           layer_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_95 (LSTM)                  (None, 60, 32)       8320        dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_98 (LSTM)                  (None, 60, 16)       2112        dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_101 (LSTM)                 (None, 60, 16)       2112        dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_104 (LSTM)                 (None, 60, 16)       2112        dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_107 (LSTM)                 (None, 60, 32)       8320        dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_110 (LSTM)                 (None, 60, 32)       8320        dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_30 (Attention)        (None, 60, 32)       0           lstm_95[0][0]                    \n",
      "                                                                 lstm_95[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_31 (Attention)        (None, 60, 16)       0           lstm_98[0][0]                    \n",
      "                                                                 lstm_98[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_32 (Attention)        (None, 60, 16)       0           lstm_101[0][0]                   \n",
      "                                                                 lstm_101[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_33 (Attention)        (None, 60, 16)       0           lstm_104[0][0]                   \n",
      "                                                                 lstm_104[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_34 (Attention)        (None, 60, 32)       0           lstm_107[0][0]                   \n",
      "                                                                 lstm_107[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_35 (Attention)        (None, 60, 32)       0           lstm_110[0][0]                   \n",
      "                                                                 lstm_110[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_94 (LayerNo (None, 60, 32)       64          attention_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_97 (LayerNo (None, 60, 16)       32          attention_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_100 (LayerN (None, 60, 16)       32          attention_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_103 (LayerN (None, 60, 16)       32          attention_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_106 (LayerN (None, 60, 32)       64          attention_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_109 (LayerN (None, 60, 32)       64          attention_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 60, 32)       0           layer_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 60, 16)       0           layer_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 60, 16)       0           layer_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 60, 16)       0           layer_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 60, 32)       0           layer_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 60, 32)       0           layer_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 32)           0           dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 16)           0           dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 16)           0           dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 16)           0           dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 32)           0           dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 32)           0           dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 144)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "                                                                 global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 144)          576         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 144)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          37120       dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256)          1024        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           16448       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           4160        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "19class (Dense)                 (None, 20)           1300        dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 142,772\n",
      "Trainable params: 141,972\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 40s 3ms/sample - loss: 2.0920 - acc: 0.3884 - val_loss: 1.7132 - val_acc: 0.4853\n",
      "Epoch 2/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 1.6176 - acc: 0.5094 - val_loss: 1.4998 - val_acc: 0.5537\n",
      "Epoch 3/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 1.4900 - acc: 0.5622 - val_loss: 1.3739 - val_acc: 0.5903\n",
      "Epoch 4/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 1.4084 - acc: 0.5918 - val_loss: 1.3044 - val_acc: 0.6280\n",
      "Epoch 5/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 1.3494 - acc: 0.6124 - val_loss: 1.2596 - val_acc: 0.6503\n",
      "Epoch 6/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 1.2921 - acc: 0.6366 - val_loss: 1.2014 - val_acc: 0.6630\n",
      "Epoch 7/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 1.2332 - acc: 0.6542 - val_loss: 1.1746 - val_acc: 0.6770\n",
      "Epoch 8/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 1.2085 - acc: 0.6694 - val_loss: 1.1590 - val_acc: 0.6797\n",
      "Epoch 9/400\n",
      "12000/12000 [==============================] - 6s 490us/sample - loss: 1.1882 - acc: 0.6735 - val_loss: 1.1514 - val_acc: 0.6887\n",
      "Epoch 10/400\n",
      "12000/12000 [==============================] - 6s 487us/sample - loss: 1.1535 - acc: 0.6877 - val_loss: 1.0914 - val_acc: 0.7100\n",
      "Epoch 11/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 1.1290 - acc: 0.6957 - val_loss: 1.0892 - val_acc: 0.7170\n",
      "Epoch 12/400\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 1.1020 - acc: 0.7092 - val_loss: 1.0994 - val_acc: 0.7127\n",
      "Epoch 13/400\n",
      "12000/12000 [==============================] - 5s 411us/sample - loss: 1.0807 - acc: 0.7161 - val_loss: 1.0487 - val_acc: 0.7313\n",
      "Epoch 14/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 1.0634 - acc: 0.7235 - val_loss: 1.0556 - val_acc: 0.7273\n",
      "Epoch 15/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 1.0526 - acc: 0.7302 - val_loss: 1.0322 - val_acc: 0.7410\n",
      "Epoch 16/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 1.0390 - acc: 0.7321 - val_loss: 1.0362 - val_acc: 0.7373\n",
      "Epoch 17/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 1.0245 - acc: 0.7363 - val_loss: 1.0251 - val_acc: 0.7370\n",
      "Epoch 18/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 1.0041 - acc: 0.7492 - val_loss: 0.9943 - val_acc: 0.7533\n",
      "Epoch 19/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.9982 - acc: 0.7459 - val_loss: 0.9984 - val_acc: 0.7553\n",
      "Epoch 20/400\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.9852 - acc: 0.7528 - val_loss: 0.9851 - val_acc: 0.7593\n",
      "Epoch 21/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.9793 - acc: 0.7592 - val_loss: 1.0046 - val_acc: 0.7457\n",
      "Epoch 22/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.9618 - acc: 0.7673 - val_loss: 0.9917 - val_acc: 0.7533\n",
      "Epoch 23/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.9559 - acc: 0.7712 - val_loss: 0.9846 - val_acc: 0.7533\n",
      "Epoch 24/400\n",
      "12000/12000 [==============================] - 5s 436us/sample - loss: 0.9582 - acc: 0.7681 - val_loss: 0.9697 - val_acc: 0.7593\n",
      "Epoch 25/400\n",
      "12000/12000 [==============================] - 6s 506us/sample - loss: 0.9259 - acc: 0.7743 - val_loss: 0.9889 - val_acc: 0.7510\n",
      "Epoch 26/400\n",
      "12000/12000 [==============================] - 6s 529us/sample - loss: 0.9337 - acc: 0.7733 - val_loss: 0.9589 - val_acc: 0.7657\n",
      "Epoch 27/400\n",
      "12000/12000 [==============================] - 6s 492us/sample - loss: 0.9141 - acc: 0.7841 - val_loss: 0.9706 - val_acc: 0.7627\n",
      "Epoch 28/400\n",
      "12000/12000 [==============================] - 6s 521us/sample - loss: 0.9108 - acc: 0.7844 - val_loss: 0.9531 - val_acc: 0.7703\n",
      "Epoch 29/400\n",
      "12000/12000 [==============================] - 5s 441us/sample - loss: 0.9023 - acc: 0.7893 - val_loss: 0.9759 - val_acc: 0.7623\n",
      "Epoch 30/400\n",
      "12000/12000 [==============================] - 6s 522us/sample - loss: 0.8964 - acc: 0.7947 - val_loss: 0.9464 - val_acc: 0.7733\n",
      "Epoch 31/400\n",
      "12000/12000 [==============================] - 7s 547us/sample - loss: 0.8921 - acc: 0.7922 - val_loss: 0.9514 - val_acc: 0.7753\n",
      "Epoch 32/400\n",
      "12000/12000 [==============================] - 6s 526us/sample - loss: 0.8802 - acc: 0.7960 - val_loss: 0.9452 - val_acc: 0.7690\n",
      "Epoch 33/400\n",
      "12000/12000 [==============================] - 6s 520us/sample - loss: 0.8708 - acc: 0.7976 - val_loss: 0.9496 - val_acc: 0.7737\n",
      "Epoch 34/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.8722 - acc: 0.8012 - val_loss: 0.9375 - val_acc: 0.7793\n",
      "Epoch 35/400\n",
      "12000/12000 [==============================] - 6s 509us/sample - loss: 0.8593 - acc: 0.8086 - val_loss: 0.9462 - val_acc: 0.7783\n",
      "Epoch 36/400\n",
      "12000/12000 [==============================] - 6s 532us/sample - loss: 0.8558 - acc: 0.8079 - val_loss: 0.9481 - val_acc: 0.7827\n",
      "Epoch 37/400\n",
      "12000/12000 [==============================] - 7s 543us/sample - loss: 0.8428 - acc: 0.8147 - val_loss: 0.9269 - val_acc: 0.7867\n",
      "Epoch 38/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.8352 - acc: 0.8201 - val_loss: 0.9378 - val_acc: 0.7757\n",
      "Epoch 39/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.8321 - acc: 0.8213 - val_loss: 0.9268 - val_acc: 0.7837\n",
      "Epoch 40/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.8332 - acc: 0.8192 - val_loss: 0.9148 - val_acc: 0.7890\n",
      "Epoch 41/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.8248 - acc: 0.8195 - val_loss: 0.9258 - val_acc: 0.7853\n",
      "Epoch 42/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.8144 - acc: 0.8227 - val_loss: 0.9181 - val_acc: 0.7917\n",
      "Epoch 43/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.8250 - acc: 0.8224 - val_loss: 0.9190 - val_acc: 0.7897\n",
      "Epoch 44/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.8078 - acc: 0.8264 - val_loss: 0.9073 - val_acc: 0.7997\n",
      "Epoch 45/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.8036 - acc: 0.8294 - val_loss: 0.9160 - val_acc: 0.7887\n",
      "Epoch 46/400\n",
      "12000/12000 [==============================] - 5s 444us/sample - loss: 0.7995 - acc: 0.8302 - val_loss: 0.9241 - val_acc: 0.7910\n",
      "Epoch 47/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.7948 - acc: 0.8332 - val_loss: 0.9238 - val_acc: 0.7907\n",
      "Epoch 48/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.7924 - acc: 0.8393 - val_loss: 0.9214 - val_acc: 0.7877\n",
      "Epoch 49/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.7901 - acc: 0.8377 - val_loss: 0.9216 - val_acc: 0.7940\n",
      "Epoch 50/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.7825 - acc: 0.8393 - val_loss: 0.9360 - val_acc: 0.7893\n",
      "Epoch 51/400\n",
      "12000/12000 [==============================] - 4s 367us/sample - loss: 0.7746 - acc: 0.8397 - val_loss: 0.9086 - val_acc: 0.7983\n",
      "Epoch 52/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.7667 - acc: 0.8477 - val_loss: 0.8955 - val_acc: 0.8057\n",
      "Epoch 53/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.7663 - acc: 0.8469 - val_loss: 0.9155 - val_acc: 0.7907\n",
      "Epoch 54/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.7634 - acc: 0.8462 - val_loss: 0.9264 - val_acc: 0.8000\n",
      "Epoch 55/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.7660 - acc: 0.8429 - val_loss: 0.9151 - val_acc: 0.7970\n",
      "Epoch 56/400\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.7593 - acc: 0.8493 - val_loss: 0.9146 - val_acc: 0.7937\n",
      "Epoch 57/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.7640 - acc: 0.8424 - val_loss: 0.9047 - val_acc: 0.8013\n",
      "Epoch 58/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.7472 - acc: 0.8526 - val_loss: 0.9040 - val_acc: 0.8010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.7495 - acc: 0.8490 - val_loss: 0.9022 - val_acc: 0.7990\n",
      "Epoch 60/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.7413 - acc: 0.8537 - val_loss: 0.9127 - val_acc: 0.8063\n",
      "Epoch 61/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.7453 - acc: 0.8536 - val_loss: 0.9081 - val_acc: 0.7963\n",
      "Epoch 62/400\n",
      "12000/12000 [==============================] - 5s 400us/sample - loss: 0.7460 - acc: 0.8520 - val_loss: 0.9014 - val_acc: 0.7983\n",
      "Epoch 63/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.7365 - acc: 0.8572 - val_loss: 0.8931 - val_acc: 0.8127\n",
      "Epoch 64/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.7266 - acc: 0.8603 - val_loss: 0.9075 - val_acc: 0.7983\n",
      "Epoch 65/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.7258 - acc: 0.8632 - val_loss: 0.9319 - val_acc: 0.8007\n",
      "Epoch 66/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.7211 - acc: 0.8648 - val_loss: 0.9094 - val_acc: 0.8073\n",
      "Epoch 67/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.7180 - acc: 0.8646 - val_loss: 0.9111 - val_acc: 0.8043\n",
      "Epoch 68/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.7186 - acc: 0.8636 - val_loss: 0.9119 - val_acc: 0.8040\n",
      "Epoch 69/400\n",
      "12000/12000 [==============================] - 5s 394us/sample - loss: 0.7113 - acc: 0.8683 - val_loss: 0.9137 - val_acc: 0.8037\n",
      "Epoch 70/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.7077 - acc: 0.8692 - val_loss: 0.8956 - val_acc: 0.8047\n",
      "Epoch 71/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.7041 - acc: 0.8707 - val_loss: 0.8985 - val_acc: 0.8007\n",
      "Epoch 72/400\n",
      "12000/12000 [==============================] - 6s 482us/sample - loss: 0.7019 - acc: 0.8709 - val_loss: 0.9212 - val_acc: 0.8033\n",
      "Epoch 73/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.7021 - acc: 0.8744 - val_loss: 0.8918 - val_acc: 0.8157\n",
      "Epoch 74/400\n",
      "12000/12000 [==============================] - 4s 368us/sample - loss: 0.6960 - acc: 0.8742 - val_loss: 0.9333 - val_acc: 0.7970\n",
      "Epoch 75/400\n",
      "12000/12000 [==============================] - 5s 389us/sample - loss: 0.6999 - acc: 0.8785 - val_loss: 0.8937 - val_acc: 0.8077\n",
      "Epoch 76/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.6897 - acc: 0.8763 - val_loss: 0.9290 - val_acc: 0.7967\n",
      "Epoch 77/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.6970 - acc: 0.8732 - val_loss: 0.9175 - val_acc: 0.7947\n",
      "Epoch 78/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.6944 - acc: 0.8735 - val_loss: 0.9046 - val_acc: 0.8073\n",
      "Epoch 79/400\n",
      "12000/12000 [==============================] - 5s 452us/sample - loss: 0.6888 - acc: 0.8764 - val_loss: 0.8952 - val_acc: 0.8060\n",
      "Epoch 80/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.6891 - acc: 0.8779 - val_loss: 0.9014 - val_acc: 0.8080\n",
      "Epoch 81/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.6809 - acc: 0.8805 - val_loss: 0.9217 - val_acc: 0.8050\n",
      "Epoch 82/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.6775 - acc: 0.8822 - val_loss: 0.9080 - val_acc: 0.8027\n",
      "Epoch 83/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.6907 - acc: 0.8779 - val_loss: 0.9062 - val_acc: 0.8077\n",
      "Epoch 84/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.6742 - acc: 0.8823 - val_loss: 0.8991 - val_acc: 0.8093\n",
      "Epoch 85/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6708 - acc: 0.8863 - val_loss: 0.8870 - val_acc: 0.8117\n",
      "Epoch 86/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.6720 - acc: 0.8860 - val_loss: 0.8973 - val_acc: 0.8087\n",
      "Epoch 87/400\n",
      "12000/12000 [==============================] - 5s 403us/sample - loss: 0.6663 - acc: 0.8888 - val_loss: 0.8920 - val_acc: 0.8080\n",
      "Epoch 88/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6644 - acc: 0.8860 - val_loss: 0.9068 - val_acc: 0.8097\n",
      "Epoch 89/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6692 - acc: 0.8844 - val_loss: 0.8900 - val_acc: 0.8070\n",
      "Epoch 90/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.6606 - acc: 0.8921 - val_loss: 0.8969 - val_acc: 0.8120\n",
      "Epoch 91/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.8861\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.6655 - acc: 0.8863 - val_loss: 0.8897 - val_acc: 0.8057\n",
      "Epoch 92/400\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 0.6328 - acc: 0.9025 - val_loss: 0.8750 - val_acc: 0.8187\n",
      "Epoch 93/400\n",
      "12000/12000 [==============================] - 5s 401us/sample - loss: 0.6246 - acc: 0.9087 - val_loss: 0.8890 - val_acc: 0.8127\n",
      "Epoch 94/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.6234 - acc: 0.9068 - val_loss: 0.8744 - val_acc: 0.8147\n",
      "Epoch 95/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.6210 - acc: 0.9091 - val_loss: 0.8813 - val_acc: 0.8140\n",
      "Epoch 96/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.6266 - acc: 0.9059 - val_loss: 0.8823 - val_acc: 0.8090\n",
      "Epoch 97/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.6087 - acc: 0.9124 - val_loss: 0.8871 - val_acc: 0.8077\n",
      "Epoch 98/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.6166 - acc: 0.9088 - val_loss: 0.8783 - val_acc: 0.8153\n",
      "Epoch 99/400\n",
      "12000/12000 [==============================] - 5s 413us/sample - loss: 0.6162 - acc: 0.9102 - val_loss: 0.8839 - val_acc: 0.8120\n",
      "Epoch 100/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.6150 - acc: 0.9100 - val_loss: 0.8806 - val_acc: 0.8120\n",
      "Epoch 101/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6129 - acc: 0.9124 - val_loss: 0.8785 - val_acc: 0.8143\n",
      "Epoch 102/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.6083 - acc: 0.9120 - val_loss: 0.8853 - val_acc: 0.8150\n",
      "Epoch 103/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.6159 - acc: 0.9093 - val_loss: 0.8892 - val_acc: 0.8123\n",
      "Epoch 104/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.6117 - acc: 0.9109 - val_loss: 0.8953 - val_acc: 0.8180\n",
      "Epoch 105/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.6111 - acc: 0.9122 - val_loss: 0.8900 - val_acc: 0.8123\n",
      "Epoch 106/400\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.6061 - acc: 0.9122 - val_loss: 0.8824 - val_acc: 0.8200\n",
      "Epoch 107/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.6119 - acc: 0.9131 - val_loss: 0.8828 - val_acc: 0.8227\n",
      "Epoch 108/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.6140 - acc: 0.9128 - val_loss: 0.8775 - val_acc: 0.8170\n",
      "Epoch 109/400\n",
      "12000/12000 [==============================] - 5s 433us/sample - loss: 0.6068 - acc: 0.9128 - val_loss: 0.8977 - val_acc: 0.8163\n",
      "Epoch 110/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.6138 - acc: 0.9115 - val_loss: 0.8809 - val_acc: 0.8163\n",
      "Epoch 111/400\n",
      "12000/12000 [==============================] - 5s 435us/sample - loss: 0.6036 - acc: 0.9155 - val_loss: 0.8826 - val_acc: 0.8157\n",
      "Epoch 112/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.6098 - acc: 0.9121 - val_loss: 0.8800 - val_acc: 0.8217\n",
      "Epoch 113/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.6015 - acc: 0.9172 - val_loss: 0.8955 - val_acc: 0.8167\n",
      "Epoch 114/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.5998 - acc: 0.9156 - val_loss: 0.8982 - val_acc: 0.8137\n",
      "Epoch 115/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.5952 - acc: 0.9217 - val_loss: 0.9061 - val_acc: 0.8103\n",
      "Epoch 116/400\n",
      "12000/12000 [==============================] - 4s 371us/sample - loss: 0.5959 - acc: 0.9202 - val_loss: 0.9026 - val_acc: 0.8160\n",
      "Epoch 117/400\n",
      "12000/12000 [==============================] - 5s 446us/sample - loss: 0.5959 - acc: 0.9193 - val_loss: 0.8819 - val_acc: 0.8207\n",
      "Epoch 118/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5992 - acc: 0.9176 - val_loss: 0.8928 - val_acc: 0.8153\n",
      "Epoch 119/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.5983 - acc: 0.9192 - val_loss: 0.8948 - val_acc: 0.8147\n",
      "Epoch 120/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5980 - acc: 0.9181 - val_loss: 0.8890 - val_acc: 0.8173\n",
      "Epoch 121/400\n",
      "12000/12000 [==============================] - 5s 424us/sample - loss: 0.5954 - acc: 0.9201 - val_loss: 0.8904 - val_acc: 0.8173\n",
      "Epoch 122/400\n",
      "12000/12000 [==============================] - 5s 387us/sample - loss: 0.5958 - acc: 0.9183 - val_loss: 0.8945 - val_acc: 0.8197\n",
      "Epoch 123/400\n",
      "12000/12000 [==============================] - 5s 447us/sample - loss: 0.5984 - acc: 0.9184 - val_loss: 0.8980 - val_acc: 0.8173\n",
      "Epoch 124/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5934 - acc: 0.9189 - val_loss: 0.8971 - val_acc: 0.8183\n",
      "Epoch 125/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.9221\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5923 - acc: 0.9222 - val_loss: 0.8972 - val_acc: 0.8163\n",
      "Epoch 126/400\n",
      "12000/12000 [==============================] - 6s 525us/sample - loss: 0.5833 - acc: 0.9226 - val_loss: 0.8879 - val_acc: 0.8207\n",
      "Epoch 127/400\n",
      "12000/12000 [==============================] - 6s 489us/sample - loss: 0.5835 - acc: 0.9232 - val_loss: 0.8870 - val_acc: 0.8220\n",
      "Epoch 128/400\n",
      "12000/12000 [==============================] - 6s 497us/sample - loss: 0.5730 - acc: 0.9312 - val_loss: 0.8871 - val_acc: 0.8203\n",
      "Epoch 129/400\n",
      "12000/12000 [==============================] - 6s 531us/sample - loss: 0.5707 - acc: 0.9323 - val_loss: 0.8891 - val_acc: 0.8223\n",
      "Epoch 130/400\n",
      "12000/12000 [==============================] - 6s 514us/sample - loss: 0.5760 - acc: 0.9279 - val_loss: 0.8853 - val_acc: 0.8217\n",
      "Epoch 131/400\n",
      "12000/12000 [==============================] - 5s 458us/sample - loss: 0.5717 - acc: 0.9302 - val_loss: 0.8858 - val_acc: 0.8220\n",
      "Epoch 132/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.5740 - acc: 0.9287 - val_loss: 0.8863 - val_acc: 0.8250\n",
      "Epoch 133/400\n",
      "12000/12000 [==============================] - 4s 373us/sample - loss: 0.5723 - acc: 0.9305 - val_loss: 0.8851 - val_acc: 0.8197\n",
      "Epoch 134/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5759 - acc: 0.9268 - val_loss: 0.8867 - val_acc: 0.8207\n",
      "Epoch 135/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5657 - acc: 0.9340 - val_loss: 0.8868 - val_acc: 0.8223\n",
      "Epoch 136/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5668 - acc: 0.9312 - val_loss: 0.8771 - val_acc: 0.8237\n",
      "Epoch 137/400\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.5677 - acc: 0.9307 - val_loss: 0.8916 - val_acc: 0.8193\n",
      "Epoch 138/400\n",
      "12000/12000 [==============================] - 5s 430us/sample - loss: 0.5718 - acc: 0.9310 - val_loss: 0.8890 - val_acc: 0.8200\n",
      "Epoch 139/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5648 - acc: 0.9333 - val_loss: 0.8945 - val_acc: 0.8183\n",
      "Epoch 140/400\n",
      "12000/12000 [==============================] - 5s 439us/sample - loss: 0.5621 - acc: 0.9346 - val_loss: 0.8937 - val_acc: 0.8177\n",
      "Epoch 141/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.5709 - acc: 0.9303 - val_loss: 0.8911 - val_acc: 0.8180\n",
      "Epoch 142/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.5678 - acc: 0.9321 - val_loss: 0.8814 - val_acc: 0.8247\n",
      "Epoch 143/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.5660 - acc: 0.9326 - val_loss: 0.8900 - val_acc: 0.8193\n",
      "Epoch 144/400\n",
      "12000/12000 [==============================] - 5s 434us/sample - loss: 0.5708 - acc: 0.9296 - val_loss: 0.8879 - val_acc: 0.8200\n",
      "Epoch 145/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5645 - acc: 0.9336 - val_loss: 0.8929 - val_acc: 0.8187\n",
      "Epoch 146/400\n",
      "12000/12000 [==============================] - 5s 442us/sample - loss: 0.5670 - acc: 0.9315 - val_loss: 0.8885 - val_acc: 0.8200\n",
      "Epoch 147/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5624 - acc: 0.9338 - val_loss: 0.8843 - val_acc: 0.8250\n",
      "Epoch 148/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5690 - acc: 0.9322 - val_loss: 0.8980 - val_acc: 0.8163\n",
      "Epoch 149/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.5601 - acc: 0.9294 - val_loss: 0.8840 - val_acc: 0.8200\n",
      "Epoch 150/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.9344\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "12000/12000 [==============================] - 5s 432us/sample - loss: 0.5646 - acc: 0.9339 - val_loss: 0.8836 - val_acc: 0.8187\n",
      "Epoch 151/400\n",
      "12000/12000 [==============================] - 4s 368us/sample - loss: 0.5613 - acc: 0.9344 - val_loss: 0.8927 - val_acc: 0.8197\n",
      "Epoch 152/400\n",
      "12000/12000 [==============================] - 5s 451us/sample - loss: 0.5620 - acc: 0.9345 - val_loss: 0.8857 - val_acc: 0.8193\n",
      "Epoch 153/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5561 - acc: 0.9359 - val_loss: 0.8869 - val_acc: 0.8210\n",
      "Epoch 154/400\n",
      "12000/12000 [==============================] - 5s 453us/sample - loss: 0.5528 - acc: 0.9400 - val_loss: 0.8924 - val_acc: 0.8193\n",
      "Epoch 155/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5516 - acc: 0.9396 - val_loss: 0.8859 - val_acc: 0.8240\n",
      "Epoch 156/400\n",
      "12000/12000 [==============================] - 5s 427us/sample - loss: 0.5509 - acc: 0.9414 - val_loss: 0.8870 - val_acc: 0.8240\n",
      "Epoch 157/400\n",
      "12000/12000 [==============================] - 5s 381us/sample - loss: 0.5569 - acc: 0.9358 - val_loss: 0.8893 - val_acc: 0.8197\n",
      "Epoch 158/400\n",
      "12000/12000 [==============================] - 5s 450us/sample - loss: 0.5519 - acc: 0.9374 - val_loss: 0.8860 - val_acc: 0.8227\n",
      "Epoch 159/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5573 - acc: 0.9359 - val_loss: 0.8865 - val_acc: 0.8243\n",
      "Epoch 160/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5550 - acc: 0.9350 - val_loss: 0.8925 - val_acc: 0.8197\n",
      "Epoch 161/400\n",
      "12000/12000 [==============================] - 6s 459us/sample - loss: 0.5537 - acc: 0.9382 - val_loss: 0.8968 - val_acc: 0.8163\n",
      "Epoch 162/400\n",
      "12000/12000 [==============================] - 5s 425us/sample - loss: 0.5546 - acc: 0.9385 - val_loss: 0.8917 - val_acc: 0.8190\n",
      "Epoch 163/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5539 - acc: 0.9375 - val_loss: 0.8911 - val_acc: 0.8200\n",
      "Epoch 164/400\n",
      "12000/12000 [==============================] - 5s 455us/sample - loss: 0.5563 - acc: 0.9358 - val_loss: 0.8948 - val_acc: 0.8220\n",
      "Epoch 165/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5552 - acc: 0.9366 - val_loss: 0.8848 - val_acc: 0.8227\n",
      "Epoch 166/400\n",
      "12000/12000 [==============================] - 6s 480us/sample - loss: 0.5540 - acc: 0.9371 - val_loss: 0.8878 - val_acc: 0.8223\n",
      "Epoch 167/400\n",
      "12000/12000 [==============================] - 6s 462us/sample - loss: 0.5522 - acc: 0.9387 - val_loss: 0.8903 - val_acc: 0.8200\n",
      "Epoch 168/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.9353\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "12000/12000 [==============================] - 5s 420us/sample - loss: 0.5536 - acc: 0.9356 - val_loss: 0.8885 - val_acc: 0.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/400\n",
      "12000/12000 [==============================] - 5s 379us/sample - loss: 0.5505 - acc: 0.9388 - val_loss: 0.8877 - val_acc: 0.8227\n",
      "Epoch 170/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5495 - acc: 0.9405 - val_loss: 0.8830 - val_acc: 0.8233\n",
      "Epoch 171/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5465 - acc: 0.9430 - val_loss: 0.8842 - val_acc: 0.8227\n",
      "Epoch 172/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.5472 - acc: 0.9402 - val_loss: 0.8877 - val_acc: 0.8237\n",
      "Epoch 173/400\n",
      "12000/12000 [==============================] - 6s 495us/sample - loss: 0.5476 - acc: 0.9411 - val_loss: 0.8868 - val_acc: 0.8257\n",
      "Epoch 174/400\n",
      "12000/12000 [==============================] - 5s 393us/sample - loss: 0.5452 - acc: 0.9418 - val_loss: 0.8892 - val_acc: 0.8227\n",
      "Epoch 175/400\n",
      "12000/12000 [==============================] - 5s 384us/sample - loss: 0.5454 - acc: 0.9428 - val_loss: 0.8912 - val_acc: 0.8210\n",
      "Epoch 176/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5454 - acc: 0.9410 - val_loss: 0.8883 - val_acc: 0.8213\n",
      "Epoch 177/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5447 - acc: 0.9402 - val_loss: 0.8866 - val_acc: 0.8200\n",
      "Epoch 178/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5489 - acc: 0.9400 - val_loss: 0.8880 - val_acc: 0.8223\n",
      "Epoch 179/400\n",
      "12000/12000 [==============================] - 6s 479us/sample - loss: 0.5458 - acc: 0.9429 - val_loss: 0.8859 - val_acc: 0.8257\n",
      "Epoch 180/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5473 - acc: 0.9397 - val_loss: 0.8925 - val_acc: 0.8203\n",
      "Epoch 181/400\n",
      "12000/12000 [==============================] - 5s 386us/sample - loss: 0.5428 - acc: 0.9425 - val_loss: 0.8886 - val_acc: 0.8210\n",
      "Epoch 182/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5489 - acc: 0.9402 - val_loss: 0.8864 - val_acc: 0.8257\n",
      "Epoch 183/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5455 - acc: 0.9414 - val_loss: 0.8864 - val_acc: 0.8240\n",
      "Epoch 184/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5461 - acc: 0.9426 - val_loss: 0.8891 - val_acc: 0.8227\n",
      "Epoch 185/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5447 - acc: 0.9433 - val_loss: 0.8881 - val_acc: 0.8240\n",
      "Epoch 186/400\n",
      "12000/12000 [==============================] - 5s 378us/sample - loss: 0.5424 - acc: 0.9447 - val_loss: 0.8928 - val_acc: 0.8230\n",
      "Epoch 187/400\n",
      "12000/12000 [==============================] - 5s 391us/sample - loss: 0.5454 - acc: 0.9405 - val_loss: 0.8883 - val_acc: 0.8223\n",
      "Epoch 188/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5475 - acc: 0.9402 - val_loss: 0.8874 - val_acc: 0.8223\n",
      "Epoch 189/400\n",
      "12000/12000 [==============================] - 6s 473us/sample - loss: 0.5449 - acc: 0.9427 - val_loss: 0.8912 - val_acc: 0.8203\n",
      "Epoch 190/400\n",
      "12000/12000 [==============================] - 5s 456us/sample - loss: 0.5456 - acc: 0.9390 - val_loss: 0.8895 - val_acc: 0.8183\n",
      "Epoch 191/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.9403\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5454 - acc: 0.9402 - val_loss: 0.8886 - val_acc: 0.8223\n",
      "Epoch 192/400\n",
      "12000/12000 [==============================] - 5s 388us/sample - loss: 0.5428 - acc: 0.9432 - val_loss: 0.8881 - val_acc: 0.8240\n",
      "Epoch 193/400\n",
      "12000/12000 [==============================] - 5s 398us/sample - loss: 0.5454 - acc: 0.9417 - val_loss: 0.8884 - val_acc: 0.8213\n",
      "Epoch 194/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5419 - acc: 0.9458 - val_loss: 0.8879 - val_acc: 0.8197\n",
      "Epoch 195/400\n",
      "12000/12000 [==============================] - 6s 478us/sample - loss: 0.5412 - acc: 0.9440 - val_loss: 0.8891 - val_acc: 0.8200\n",
      "Epoch 196/400\n",
      "12000/12000 [==============================] - 6s 461us/sample - loss: 0.5448 - acc: 0.9405 - val_loss: 0.8900 - val_acc: 0.8217\n",
      "Epoch 197/400\n",
      "12000/12000 [==============================] - 6s 460us/sample - loss: 0.5435 - acc: 0.9443 - val_loss: 0.8909 - val_acc: 0.8223\n",
      "Epoch 198/400\n",
      "12000/12000 [==============================] - 5s 390us/sample - loss: 0.5459 - acc: 0.9410 - val_loss: 0.8902 - val_acc: 0.8183\n",
      "Epoch 199/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5431 - acc: 0.9433 - val_loss: 0.8887 - val_acc: 0.8213\n",
      "Epoch 200/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5454 - acc: 0.9417 - val_loss: 0.8902 - val_acc: 0.8200\n",
      "Epoch 201/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5414 - acc: 0.9427 - val_loss: 0.8898 - val_acc: 0.8213\n",
      "Epoch 202/400\n",
      "12000/12000 [==============================] - 6s 474us/sample - loss: 0.5382 - acc: 0.9437 - val_loss: 0.8890 - val_acc: 0.8217\n",
      "Epoch 203/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5411 - acc: 0.9427 - val_loss: 0.8904 - val_acc: 0.8203\n",
      "Epoch 204/400\n",
      "12000/12000 [==============================] - 5s 376us/sample - loss: 0.5399 - acc: 0.9446 - val_loss: 0.8914 - val_acc: 0.8210\n",
      "Epoch 205/400\n",
      "12000/12000 [==============================] - 5s 406us/sample - loss: 0.5458 - acc: 0.9417 - val_loss: 0.8901 - val_acc: 0.8200\n",
      "Epoch 206/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.5444 - acc: 0.9428 - val_loss: 0.8901 - val_acc: 0.8230\n",
      "Epoch 207/400\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.5439 - acc: 0.9419 - val_loss: 0.8887 - val_acc: 0.8230\n",
      "Epoch 208/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5416 - acc: 0.9427 - val_loss: 0.8902 - val_acc: 0.8237\n",
      "Epoch 209/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.9426\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "12000/12000 [==============================] - 6s 464us/sample - loss: 0.5433 - acc: 0.9428 - val_loss: 0.8902 - val_acc: 0.8197\n",
      "Epoch 210/400\n",
      "12000/12000 [==============================] - 5s 382us/sample - loss: 0.5382 - acc: 0.9447 - val_loss: 0.8894 - val_acc: 0.8207\n",
      "Epoch 211/400\n",
      "12000/12000 [==============================] - 5s 399us/sample - loss: 0.5417 - acc: 0.9419 - val_loss: 0.8883 - val_acc: 0.8220\n",
      "Epoch 212/400\n",
      "12000/12000 [==============================] - 6s 471us/sample - loss: 0.5445 - acc: 0.9423 - val_loss: 0.8897 - val_acc: 0.8220\n",
      "Epoch 213/400\n",
      "12000/12000 [==============================] - 6s 470us/sample - loss: 0.5401 - acc: 0.9428 - val_loss: 0.8897 - val_acc: 0.8220\n",
      "Epoch 214/400\n",
      "12000/12000 [==============================] - 6s 477us/sample - loss: 0.5378 - acc: 0.9435 - val_loss: 0.8901 - val_acc: 0.8220\n",
      "Epoch 215/400\n",
      "12000/12000 [==============================] - 6s 465us/sample - loss: 0.5417 - acc: 0.9462 - val_loss: 0.8894 - val_acc: 0.8213\n",
      "Epoch 216/400\n",
      "12000/12000 [==============================] - 4s 374us/sample - loss: 0.5364 - acc: 0.9459 - val_loss: 0.8894 - val_acc: 0.8210\n",
      "Epoch 217/400\n",
      "12000/12000 [==============================] - 5s 423us/sample - loss: 0.5368 - acc: 0.9470 - val_loss: 0.8905 - val_acc: 0.8233\n",
      "Epoch 218/400\n",
      "12000/12000 [==============================] - 6s 467us/sample - loss: 0.5426 - acc: 0.9417 - val_loss: 0.8909 - val_acc: 0.8230\n",
      "Epoch 219/400\n",
      "12000/12000 [==============================] - 6s 458us/sample - loss: 0.5415 - acc: 0.9435 - val_loss: 0.8908 - val_acc: 0.8220\n",
      "Epoch 220/400\n",
      "12000/12000 [==============================] - 6s 476us/sample - loss: 0.5459 - acc: 0.9413 - val_loss: 0.8898 - val_acc: 0.8220\n",
      "Epoch 221/400\n",
      "12000/12000 [==============================] - 6s 463us/sample - loss: 0.5392 - acc: 0.9426 - val_loss: 0.8890 - val_acc: 0.8233\n",
      "Epoch 222/400\n",
      "12000/12000 [==============================] - 4s 370us/sample - loss: 0.5416 - acc: 0.9442 - val_loss: 0.8897 - val_acc: 0.8233\n",
      "Epoch 223/400\n",
      "12000/12000 [==============================] - 5s 407us/sample - loss: 0.5397 - acc: 0.9452 - val_loss: 0.8896 - val_acc: 0.8227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.5404 - acc: 0.9450 - val_loss: 0.8896 - val_acc: 0.8230\n",
      "Epoch 225/400\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5416 - acc: 0.9405 - val_loss: 0.8893 - val_acc: 0.8230\n",
      "Epoch 226/400\n",
      "12000/12000 [==============================] - 6s 469us/sample - loss: 0.5391 - acc: 0.9447 - val_loss: 0.8891 - val_acc: 0.8233\n",
      "Epoch 227/400\n",
      "11776/12000 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.9419\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "12000/12000 [==============================] - 5s 448us/sample - loss: 0.5430 - acc: 0.9417 - val_loss: 0.8890 - val_acc: 0.8230\n",
      "Epoch 228/400\n",
      "12000/12000 [==============================] - 5s 383us/sample - loss: 0.5426 - acc: 0.9413 - val_loss: 0.8884 - val_acc: 0.8233\n",
      "Epoch 229/400\n",
      "12000/12000 [==============================] - 5s 412us/sample - loss: 0.5370 - acc: 0.9453 - val_loss: 0.8887 - val_acc: 0.8233\n",
      "Epoch 230/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5411 - acc: 0.9458 - val_loss: 0.8891 - val_acc: 0.8227\n",
      "Epoch 231/400\n",
      "12000/12000 [==============================] - 6s 468us/sample - loss: 0.5405 - acc: 0.9444 - val_loss: 0.8898 - val_acc: 0.8230\n",
      "Epoch 232/400\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.5416 - acc: 0.9427 - val_loss: 0.8887 - val_acc: 0.8240\n",
      "Epoch 233/400\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.5412 - acc: 0.9449 - val_loss: 0.8885 - val_acc: 0.8237\n",
      "Epoch 00233: early stopping\n",
      "0.826\n",
      "0.84435\n"
     ]
    }
   ],
   "source": [
    "# [:,:,:,[1]]\n",
    "train = x\n",
    "test = t\n",
    "\n",
    "fold_num=5\n",
    "kfold = StratifiedKFold(fold_num,random_state=42,shuffle=True)\n",
    "proba_t = np.zeros((16000, 20))\n",
    "proba_oof = np.zeros((15000, 20))\n",
    "\n",
    "oof_score = []\n",
    "oof_comm = []\n",
    "history = []\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05)\n",
    "\n",
    "# 两个输出    \n",
    "\n",
    "mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "\n",
    "# # 每一个大类输出 4\n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4)\n",
    "# # 每一个大类输出 \n",
    "# new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "# y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n",
    "# 每一个小类的输出 19\n",
    "\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "seeds = [42,39,17][:1]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(fold_num,random_state=seed,shuffle=True)\n",
    "    for fold, (xx, yy) in enumerate(kfold.split(train, y)):\n",
    "        print(train.shape)\n",
    "        mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "        new_mapping = {'A':0,'B':1,'C':2,'D':3}\n",
    "\n",
    "        model = LSTM_Model()\n",
    "        model.summary()\n",
    "#         optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n",
    "#         lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "#         wd_callback = WeightDecayScheduler(wd_schedule)\n",
    "        model.compile(loss=custom_loss,\n",
    "#                       ,loss_weights=[3,7,21],\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=[\"acc\"])#'',localscore\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    factor=0.5,\n",
    "                                    patience=18)\n",
    "        early_stopping = EarlyStopping(monitor=\"val_acc\",\n",
    "                                       verbose=1,\n",
    "                                       mode='max',\n",
    "                                       patience=60)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(f'LSTM-Copy1_new-Copy1{fold}.h5',\n",
    "                                     monitor=\"val_acc\",\n",
    "                                     verbose=0,\n",
    "                                     mode='max',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        train_res = model.fit(train[xx][:,:,:,0], y_binary[xx],\n",
    "                  epochs=400,\n",
    "                  batch_size=256,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(train[yy][:,:,:,0], y_binary[yy]),\n",
    "                  callbacks=[plateau, early_stopping, checkpoint],\n",
    "#                              class_weight=[classweights1,classweights2,classweights3]\n",
    "                             )\n",
    "        history.append(train_res)\n",
    "        \n",
    "        model.load_weights(f'LSTM-Copy1_new-Copy1{fold}.h5')\n",
    "        proba_t[:,:20] += model.predict(test[:,:,:,0], verbose=0, batch_size=1024) / fold_num /len(seeds)\n",
    "        proba_oof[yy,:20] += model.predict(train[yy][:,:,:,0],verbose=0,batch_size=1024)/len(seeds)\n",
    "\n",
    "\n",
    "        oof_y = np.argmax(proba_oof[yy,:20], axis=1)\n",
    "        acc = round(accuracy_score(y[yy], oof_y),3)\n",
    "        print(acc)\n",
    "        oof_score.append(acc)\n",
    "        scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n",
    "        oof_comm.append(scores)   \n",
    "        print(round(scores, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:28:38.528025Z",
     "start_time": "2020-08-21T16:28:38.479891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.827, 0.83, 0.814, 0.828, 0.826],\n",
       " 0.825,\n",
       " [0.8440793650793659,\n",
       "  0.8480634920634939,\n",
       "  0.8345238095238102,\n",
       "  0.8463015873015882,\n",
       "  0.8443492063492071],\n",
       " 0.8434634920634931)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_score,np.mean(oof_score),oof_comm,np.mean(oof_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:30:39.439679Z",
     "start_time": "2020-08-21T16:30:39.384223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8440793650793659 0.827\n",
      "1 0.8480634920634939 0.83\n",
      "2 0.8345238095238102 0.814\n",
      "3 0.8463015873015882 0.828\n",
      "4 0.8443492063492071 0.826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LSTM-Copy1_new-Copy1_LSTM_adddropout0.84346_dict.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(oof_comm):\n",
    "    print(index,i,oof_score[index])\n",
    "\n",
    "oof_dict = {\n",
    "    \"oof\":proba_oof,\n",
    "    \"test\":proba_t,\n",
    "    \"acc\":oof_comm,\n",
    "}\n",
    "import joblib \n",
    "joblib.dump(oof_dict,\"LSTM.pkl\"% np.mean(oof_comm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T16:30:41.049672Z",
     "start_time": "2020-08-21T16:30:40.397307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8248\n",
      "0.84346\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   fragment_id  16000 non-null  int64\n",
      " 1   behavior_id  16000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 250.1 KB\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {\n",
    "        0: 'A_1', 1: 'B_2', 2: 'A_3', 3: 'A_4', 4: 'B_3', 5: 'C_5', 6: 'C_2', 7: 'A_5', 8: 'B_1', \n",
    "        9: 'C_1', 10: 'A_2', 11: 'C_3',12: 'B_5', 13: 'B_4', 14: 'C_4', \n",
    "        15: 'D_6', 16: 'E_7', 17: 'F_8', 18: 'G_9', 19: 'H_0'\n",
    "              }\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "train_y = y\n",
    "labels = np.argmax(proba_t[:,:20], axis=1)\n",
    "oof_y = np.argmax(proba_oof[:,:20], axis=1)\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(scores, 5))\n",
    "data_path = '../data/'\n",
    "sub = pd.read_csv(data_path+'submit_example.csv')\n",
    "sub['behavior_id'] = labels\n",
    "\n",
    "vc = pd.Series(train_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = pd.Series(oof_y).value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "\n",
    "vc = sub['behavior_id'].value_counts().sort_index()\n",
    "sns.barplot(vc.index, vc.values)\n",
    "plt.show()\n",
    "sub.to_csv('LSTM-Copy1_new-Copy1_LSTM_adddropout%.5f.csv' % scores, index=False)\n",
    "sub.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}